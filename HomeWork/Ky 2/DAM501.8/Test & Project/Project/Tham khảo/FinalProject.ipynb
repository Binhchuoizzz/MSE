{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ae00ouMwT8Z"
   },
   "outputs": [],
   "source": [
    "#  ĐỀ BÀI:\n",
    "# Deadline nộp bài: 12h ngày 11/08/2025\n",
    "# Title email: MSE K23HN_DAM501_Fullname\n",
    "# Gửi link drive ở chế độ shared tới email: hungkbkhn@gmail.com\n",
    "\n",
    "# Nội dung trong Drive:\n",
    "# files dataset\n",
    "# file notebook\n",
    "# ảnh chụp màn hình minh chứng hoàn thành Khóa học Coursera\n",
    "\n",
    "# Trình bày (online):\n",
    "# vào buổi cuối 16/08 theo lịch\n",
    "\n",
    "# Yêu cầu thực hiện:\n",
    "# Mỗi bạn chọn 1 dataset tại: https://www.kaggle.com/discussions/general/210203\n",
    "# (Tự đưa ra giá trị min support, min confidence phù hợp)\n",
    "\n",
    "# Nhiệm vụ lập trình (bằng Python):\n",
    "# Khai thác tập phổ biến sử dụng giải thuật maxFP-growth\n",
    "# → Khai thác luật kết hợp\n",
    "# → Áp dụng cho ID sinh viên chẵn\n",
    "\n",
    "# Khai thác tập phổ biến sử dụng giải thuật Apriori\n",
    "# → Khai thác luật kết hợp\n",
    "# → Áp dụng cho ID sinh viên lẻ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1750834064094,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "02717804122024756558"
     },
     "user_tz": -420
    },
    "id": "Y6NzsjGTyyIC"
   },
   "outputs": [],
   "source": [
    "# Link dataset: https://www.kaggle.com/datasets/patricklford/mass-disasters-in-vietnam-from-1900-to-2024\n",
    "\n",
    "# Đề tài: Association Rule Mining of Natural Disaster Patterns and Damage Severity in Vietnam (1953–2024)\n",
    "\n",
    "# 4 phần phải làm\n",
    "# 1. Làm sạch & phân tích sơ bộ dữ liệu\n",
    "# 2. Phân loại damage level, gán mùa, phân theo vùng\n",
    "# 3. Chạy Apriori, sinh luật kết hợp\n",
    "# 4. Viết báo cáo, slide và hoàn thiện mô tả phân tích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "path = \"disaster_in_vietnam.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1750826336604,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "02717804122024756558"
     },
     "user_tz": -420
    },
    "id": "zHEF9WlgydmA",
    "outputId": "b0f9b5a7-01f2-42b5-8935-b370febc15bf"
   },
   "outputs": [],
   "source": [
    "# Read file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn các cột cần thiết\n",
    "df = df[\n",
    "    [\n",
    "        'Disaster Group', 'Disaster Subgroup', 'Disaster Type', 'Disaster Subtype',\n",
    "        'Location', 'Start Year', 'Start Month',\n",
    "        'Total Deaths', 'No. Injured', 'Total Affected', \"Total Damage ('000 US$)\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đổi tên cột cho ngắn gọn và dễ xử lý\n",
    "df.columns = ['group', 'subgroup', 'type', 'subtype', 'location', 'year', 'month',\n",
    "              'deaths', 'injured', 'affected', 'damage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển cột số thành kiểu numeric (tránh lỗi khi có chuỗi)\n",
    "df['deaths'] = pd.to_numeric(df['deaths'], errors='coerce')\n",
    "df['injured'] = pd.to_numeric(df['injured'], errors='coerce')\n",
    "df['affected'] = pd.to_numeric(df['affected'], errors='coerce')\n",
    "df['damage'] = pd.to_numeric(df['damage'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1750826336720,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "02717804122024756558"
     },
     "user_tz": -420
    },
    "id": "sxn93IPs0lHd",
    "outputId": "e7eec15a-45d6-4a6d-e89b-7c58e9ff9f56"
   },
   "outputs": [],
   "source": [
    "# Bỏ dòng không có thông tin về loại thiên tai hoặc thời gian\n",
    "df = df.dropna(subset=['type', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1750832901843,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "02717804122024756558"
     },
     "user_tz": -420
    },
    "id": "4Qqy99Zd8UMn",
    "outputId": "b204532f-009f-443a-b3e1-2db549451f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                         southern coast\n",
      "1                                                    nan\n",
      "2                                   saigon, mekong delta\n",
      "3                                        china sea coast\n",
      "4                                                    nan\n",
      "                             ...                        \n",
      "330    quang nam, quang tri, phu yen, phu loc distric...\n",
      "331                                  kien giang province\n",
      "332                     lai chau and dien bien provinces\n",
      "333                                                hanoi\n",
      "334     quang tri, thua thien hue and ha tinh provinces \n",
      "Name: location_clean, Length: 333, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Chuyển cột 'location' về chữ thường để dễ xử lý\n",
    "df['location_clean'] = df['location'].astype(str).str.lower()\n",
    "print(df['location_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.2.5)\n"
     ]
    }
   ],
   "source": [
    "# Cai dat cac module can thiet de download file, va chuan hoa ten tinh\n",
    "!pip install unidecode\n",
    "!pip install rapidfuzz\n",
    "!pip install openpyxl\n",
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             group                subgroup                  type  \\\n",
      "0          Natural          Meteorological                 Storm   \n",
      "1          Natural          Meteorological                 Storm   \n",
      "2          Natural              Biological              Epidemic   \n",
      "3          Natural          Meteorological                 Storm   \n",
      "4          Natural            Hydrological                 Flood   \n",
      "..             ...                     ...                   ...   \n",
      "330        Natural            Hydrological                 Flood   \n",
      "331        Natural          Meteorological                 Storm   \n",
      "332        Natural            Hydrological                 Flood   \n",
      "333  Technological  Miscellaneous accident  Fire (Miscellaneous)   \n",
      "334        Natural          Meteorological                 Storm   \n",
      "\n",
      "                  subtype                                           location  \\\n",
      "0        Tropical cyclone                                     Southern coast   \n",
      "1        Tropical cyclone                                                NaN   \n",
      "2       Bacterial disease                               Saigon, Mekong delta   \n",
      "3        Tropical cyclone                                    China sea coast   \n",
      "4         Flood (General)                                                NaN   \n",
      "..                    ...                                                ...   \n",
      "330       Flood (General)  Quang Nam, Quang Tri, Phu Yen, Phu Loc distric...   \n",
      "331      Tropical cyclone                                Kien Giang Province   \n",
      "332       Flood (General)                   Lai Chau and Dien Bien Provinces   \n",
      "333  Fire (Miscellaneous)                                              Hanoi   \n",
      "334        Severe weather   Quang Tri, Thua Thien Hue and Ha Tinh Provinces    \n",
      "\n",
      "     year  month  deaths  injured  affected   damage  \\\n",
      "0    1953    9.0  1000.0      NaN       NaN      NaN   \n",
      "1    1956   11.0    56.0      NaN       NaN      NaN   \n",
      "2    1964    1.0   598.0      NaN   10848.0      NaN   \n",
      "3    1964    9.0  7000.0      NaN  700000.0  50000.0   \n",
      "4    1964   12.0   400.0      NaN       NaN      NaN   \n",
      "..    ...    ...     ...      ...       ...      ...   \n",
      "330  2022   12.0     5.0      NaN   10495.0   1500.0   \n",
      "331  2023    7.0     1.0      3.0       3.0      NaN   \n",
      "332  2023    8.0     4.0      3.0     463.0      NaN   \n",
      "333  2023    9.0    56.0      NaN       NaN      NaN   \n",
      "334  2023   11.0     5.0      NaN   80000.0      NaN   \n",
      "\n",
      "                                        location_clean  \\\n",
      "0                                       southern coast   \n",
      "1                                                  nan   \n",
      "2                                 saigon, mekong delta   \n",
      "3                                      china sea coast   \n",
      "4                                                  nan   \n",
      "..                                                 ...   \n",
      "330  quang nam, quang tri, phu yen, phu loc distric...   \n",
      "331                                kien giang province   \n",
      "332                   lai chau and dien bien provinces   \n",
      "333                                              hanoi   \n",
      "334   quang tri, thua thien hue and ha tinh provinces    \n",
      "\n",
      "                                  location_replace  \n",
      "0                                             None  \n",
      "1                                             None  \n",
      "2                                             None  \n",
      "3                                             None  \n",
      "4                                             None  \n",
      "..                                             ...  \n",
      "330  phu yen, quang nam, quang tri, thua thien hue  \n",
      "331                                     kien giang  \n",
      "332                            dien bien, lai chau  \n",
      "333                                         ha noi  \n",
      "334             ha tinh, quang tri, thua thien hue  \n",
      "\n",
      "[333 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Chuẩn hoá tên tỉnh\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "from rapidfuzz import fuzz, process\n",
    "import pandas as pd\n",
    "\n",
    "province_list = [\n",
    "    'ha noi', 'ha giang', 'cao bang', 'bac kan', 'tuyen quang', 'lao cai', \n",
    "    'dien bien', 'lai chau', 'son la', 'yen bai', 'hoa binh', 'thai nguyen',\n",
    "    'lang son', 'quang ninh', 'bac giang', 'phu tho', 'vinh phuc', 'bac ninh',\n",
    "    'hai duong', 'hai phong', 'hung yen', 'thai binh', 'ha nam',\n",
    "    'nam dinh', 'ninh binh', 'thanh hoa', 'nghe an', 'ha tinh', 'quang binh',\n",
    "    'quang tri', 'thua thien hue', 'da nang', 'quang nam', 'quang ngai',\n",
    "    'binh dinh', 'phu yen', 'khanh hoa', 'ninh thuan', 'binh thuan',\n",
    "    'kon tum', 'gia lai', 'dak lak', 'dak nong', 'lam dong', 'ho chi minh',\n",
    "    'binh phuoc', 'tay ninh', 'binh duong', 'dong nai', 'ba ria vung tau',\n",
    "    'long an', 'tien giang', 'ben tre', 'tra vinh', 'vinh long', 'dong thap',\n",
    "    'an giang', 'kien giang', 'can tho', 'hau giang', 'soc trang', 'bac lieu',\n",
    "    'ca mau'\n",
    "]\n",
    "\n",
    "# Làm sạch văn bản\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unidecode.unidecode(text.lower())\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)       # loại bỏ ký tự đặc biệt\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # chuẩn hóa khoảng trắng\n",
    "    return text\n",
    "\n",
    "# Chia câu thành các cụm từ n-gram để tìm tỉnh\n",
    "def extract_all_provinces(text):\n",
    "    cleaned = clean_text(text)\n",
    "    words = cleaned.split()\n",
    "    found = set()\n",
    "\n",
    "    for n in range(1, 4):  # kiểm tra các cụm từ 1, 2, 3 từ\n",
    "        for i in range(len(words) - n + 1):\n",
    "            phrase = ' '.join(words[i:i+n])\n",
    "            match, score, _ = process.extractOne(phrase, province_list, scorer=fuzz.ratio)\n",
    "            if score >= 90:\n",
    "                found.add(match)\n",
    "\n",
    "    return ', '.join(sorted(found)) if found else None\n",
    "\n",
    "df['location_replace'] = df['location_clean'].apply(extract_all_provinces)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xuất file dia_phuong_khac.xlsx thành công!\n"
     ]
    }
   ],
   "source": [
    "# Xuất ra file Excel\n",
    "df[['location_replace']].to_excel('dia_phuong_khac.xlsx', index=False)\n",
    "print(\"Đã xuất file dia_phuong_khac.xlsx thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary nhữung item không trong danh sách\n",
    "location_mapping_simple = {\n",
    "    'southern coast': 'long an, tien giang, dong thap, vinh long, tra vinh, can tho, hau giang, soc trang, ben tre, an giang, kien giang, bac lieu, ca mau',\n",
    "    'saigon': 'ho chi minh',\n",
    "    'mekong delta': 'ho chi minh',\n",
    "    'china sea coast': 'da nang, quang nam, quang ngai, binh dinh, phu yen, khanh hoa, ninh thuan, binh thuan',\n",
    "    'near cambodia border': 'tay ninh, an giang, kien giang',\n",
    "    'north': 'ha noi, ha giang, cao bang, bac kan, tuyen quang, lao cai, dien bien, lai chau, son la, yen bai, hoa binh, thai nguyen, lang son, quang ninh, bac giang, phu tho, vinh phuc, bac ninh, hai duong, hai phong, hung yen, thai binh, ha nam, nam dinh, ninh binh, thanh hoa',\n",
    "    'near saigon': 'binh duong, dong nai, long an',\n",
    "    'central': 'nghe an, ha tinh, quang binh, quang tri, thua thien hue, da nang, quang nam, quang ngai, binh dinh, phu yen, khanh hoa, ninh thuan, binh thuan, kon tum, gia lai, dak lak, dak nong, lam dong',\n",
    "    'south': 'ho chi minh, binh phuoc, tay ninh, binh duong, dong nai, ba ria vung tau, long an, tien giang, ben tre, tra vinh, vinh long, dong thap, an giang, kien giang, can tho, hau giang, soc trang, bac lieu, ca mau',\n",
    "    'central - north regions': 'thanh hoa, nghe an, ha tinh, quang binh, quang tri, thua thien hue',\n",
    "    'hue': 'thua thien hue',\n",
    "    'nha trang': 'khanh hoa',\n",
    "    'halong bay': 'quang ninh',\n",
    "    'chuong my': 'ha noi',\n",
    "    'near hoi an': 'quang nam',\n",
    "    'near con dao island': 'ba ria vung tau',\n",
    "    'binh tri thien': 'thua thien hue, quang tri, quang binh',\n",
    "    'nghe tinh': 'nghe an, ha tinh',\n",
    "    'nghia binh': 'binh dinh, quang ngai',\n",
    "    'phu khahu': 'khanh hoa',\n",
    "    'cu long coast': 'long an, tien giang, dong thap, vinh long, tra vinh, can tho, hau giang, soc trang, ben tre, an giang, kien giang, bac lieu, ca mau',\n",
    "    'tonkin gulf': 'quang ninh, hai phong',\n",
    "    'thi vai river': 'ba ria vung tau',\n",
    "    'thinh dan village': 'thai nguyen',\n",
    "    'kien kang': 'kien giang',\n",
    "    'lang khe': 'nghe an',\n",
    "    'near mui ne': 'binh thuan',\n",
    "    'near ba ria-vung tau': 'ba ria vung tau',\n",
    "    'tan dan': 'hai duong',\n",
    "    'krong pak': 'dak lak',\n",
    "    'hanoã¯': 'ha noi',\n",
    "    'muong kuon': 'lao cai',\n",
    "    'bao xat': 'lao cai',\n",
    "    'bao thang': 'lao cai',\n",
    "    'sa pa': 'lao cai',\n",
    "    'thuan an': 'thua thien hue',\n",
    "    'chau binh': 'nghe an',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các tỉnh theo vùng\n",
    "north_provinces = [\n",
    "    'ha noi', 'ha giang', 'cao bang', 'bac kan', 'tuyen quang', 'lao cai',\n",
    "    'dien bien', 'lai chau', 'son la', 'yen bai', 'hoa binh', 'thai nguyen',\n",
    "    'lang son', 'quang ninh', 'bac giang', 'phu tho', 'vinh phuc', 'bac ninh',\n",
    "    'hai duong', 'hai phong', 'hung yen', 'thai binh', 'ha nam', 'nam dinh',\n",
    "    'ninh binh'\n",
    "]\n",
    "\n",
    "central_provinces = [\n",
    "    'thanh hoa', 'nghe an', 'ha tinh', 'quang binh', 'quang tri', 'thua thien hue',\n",
    "    'da nang', 'quang nam', 'quang ngai', 'binh dinh', 'phu yen', 'khanh hoa',\n",
    "    'ninh thuan', 'binh thuan', 'kon tum', 'gia lai', 'dak lak', 'dak nong', 'lam dong'\n",
    "]\n",
    "\n",
    "south_provinces = [\n",
    "    'ho chi minh', 'binh phuoc', 'tay ninh', 'binh duong', 'dong nai', 'ba ria vung tau',\n",
    "    'long an', 'tien giang', 'ben tre', 'tra vinh', 'vinh long', 'dong thap',\n",
    "    'an giang', 'kien giang', 'can tho', 'hau giang', 'soc trang', 'bac lieu', 'ca mau'\n",
    "]\n",
    "\n",
    "# Sinh tự động dictionary ánh xạ\n",
    "province_to_region = {}\n",
    "province_to_region.update({p: 'Bắc' for p in north_provinces})\n",
    "province_to_region.update({p: 'Trung' for p in central_provinces})\n",
    "province_to_region.update({p: 'Nam' for p in south_provinces})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_from_provinces(province_str):\n",
    "    if pd.isna(province_str):\n",
    "        return 'Không xác định'\n",
    "    provinces = [p.strip() for p in province_str.split(',')]\n",
    "    regions = set()\n",
    "    for province in provinces:\n",
    "        region = province_to_region.get(province)\n",
    "        if region:\n",
    "            regions.add(region)\n",
    "    if not regions:\n",
    "        return 'Không xác định'\n",
    "    if len(regions) == 1:\n",
    "        return regions.pop()\n",
    "    return ', '.join(sorted(regions))\n",
    "\n",
    "df['regions'] = df['location_replace'].apply(get_region_from_provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo cột regions từ location_replace\n",
    "\n",
    "# Filter theo region \"Không xác định\" và xóa records có location_replace null\n",
    "df = df[~((df['regions'] == 'Không xác định') & (df['location_replace'].isna()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xuất file dia_phuong.xlsx thành công!\n"
     ]
    }
   ],
   "source": [
    "# Xuất ra file Excel\n",
    "df.to_excel('dia_phuong.xlsx', index=False)\n",
    "print(\"Đã xuất file dia_phuong.xlsx thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đến thời điểm này thì đã xử lý xong cột location. Cột data chuẩn là location replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10748\\2640053727.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['damage_level'] = df['damage'].apply(classify_damage)\n"
     ]
    }
   ],
   "source": [
    "# Phân loại damage level\n",
    "def classify_damage(damage):\n",
    "    if pd.isna(damage):\n",
    "        return 'Không xác định'\n",
    "    if damage < 1000:\n",
    "        return 'Nhỏ'\n",
    "    elif damage < 10000:\n",
    "        return 'Trung bình'\n",
    "    elif damage < 100000:\n",
    "        return 'Lớn'\n",
    "    else:\n",
    "        return 'Rất lớn'\n",
    "\n",
    "df['damage_level'] = df['damage'].apply(classify_damage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xuất file dia_phuong.xlsx thành công!\n"
     ]
    }
   ],
   "source": [
    "# Xuất ra file Excel\n",
    "df['damage_level'].to_excel('dia_phuong.xlsx', index=False)\n",
    "print(\"Đã xuất file dia_phuong.xlsx thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10748\\818358701.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['season'] = df['month'].apply(get_season)\n"
     ]
    }
   ],
   "source": [
    "# Gán mùa theo tháng\n",
    "def get_season(month):\n",
    "    if pd.isna(month):\n",
    "        return 'Không xác định'\n",
    "    month = int(month)\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Đông'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Xuân'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Hạ'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Thu'\n",
    "    else:\n",
    "        return 'Không xác định'\n",
    "\n",
    "df['season'] = df['month'].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xuất file dia_phuong.xlsx thành công!\n"
     ]
    }
   ],
   "source": [
    "# Xuất ra file Excel\n",
    "df['season'].to_excel('dia_phuong.xlsx', index=False)\n",
    "print(\"Đã xuất file dia_phuong.xlsx thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bắt đầu chạy giải thuật Apriori, sinh luật kết hợp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlxtend) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlxtend) (2.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlxtend) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlxtend) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlxtend) (3.9.4)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.5.0)\n",
      "Downloading mlxtend-0.23.4-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 0.5/1.4 MB 114.8 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 119.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.0/1.4 MB 102.6 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 1.3/1.4 MB 111.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 113.9 kB/s eta 0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.4\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt thư viện cần thiết\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn bị dữ liệu cho Apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Chọn các cột phân loại phù hợp\n",
    "cols = ['group', 'type', 'damage_level', 'season', 'regions']\n",
    "df_apriori = df[cols].astype(str)\n",
    "\n",
    "# Chuyển mỗi dòng thành 1 list các thuộc tính\n",
    "transactions = df_apriori.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mã hoá dữ liệu\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          antecedents       consequents   support  confidence      lift\n",
      "2        (Bắc, Trung)         (Natural)  0.149813         1.0  1.208145\n",
      "97  (Lớn, Flood, Thu)         (Natural)  0.074906         1.0  1.208145\n",
      "4             (Flood)         (Natural)  0.355805         1.0  1.208145\n",
      "8              (Road)  (Không xác định)  0.063670         1.0  2.053846\n",
      "12              (Lớn)         (Natural)  0.205993         1.0  1.208145\n",
      "16            (Storm)         (Natural)  0.419476         1.0  1.208145\n",
      "15          (Rất lớn)         (Natural)  0.142322         1.0  1.208145\n",
      "31       (Bắc, Storm)         (Natural)  0.097378         1.0  1.208145\n",
      "21             (Road)   (Technological)  0.063670         1.0  5.804348\n",
      "25       (Flood, Bắc)         (Natural)  0.078652         1.0  1.208145\n"
     ]
    }
   ],
   "source": [
    "# Chạy Apriori để tìm tập phổ biến\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Sinh luật kết hợp\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "# Xem top 10 luật mạnh nhất\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values('confidence', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xuất luật kết hợp ra association_rules.xlsx\n"
     ]
    }
   ],
   "source": [
    "rules.to_excel('association_rules.xlsx', index=False)\n",
    "print(\"Đã xuất luật kết hợp ra association_rules.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNX83z//cMn0YLXRHDJzKxe",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
