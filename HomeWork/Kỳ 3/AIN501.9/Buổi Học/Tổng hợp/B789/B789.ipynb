{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d43100",
   "metadata": {},
   "source": [
    "# ML Advanced Topics — Playbook (Object Detection, Face, RNN, Embeddings, Seq2Seq, Transformer)\n",
    "This notebook contains compact, runnable code snippets demonstrating core concepts: IoU/NMS/anchors, triplet loss, LSTM, skip-gram NS, Transformer seq2seq and beam search. Run cell-by-cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ccd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - imports, seed, device\n",
    "import math, random, time, itertools\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from torchvision import transforms, datasets\n",
    "from pathlib import Path\n",
    "seed=42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d4037",
   "metadata": {},
   "source": [
    "## Object Detection — IoU, NMS, Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def iou_xyxy(a, b):\n",
    "    a = np.array(a); b = np.array(b)\n",
    "    if b.ndim==1: b = b.reshape(1,4)\n",
    "    x1 = np.maximum(a[0], b[:,0]); y1 = np.maximum(a[1], b[:,1])\n",
    "    x2 = np.minimum(a[2], b[:,2]); y2 = np.minimum(a[3], b[:,3])\n",
    "    inter = np.maximum(0, x2-x1) * np.maximum(0, y2-y1)\n",
    "    area_a = (a[2]-a[0])*(a[3]-a[1]); area_b = (b[:,2]-b[:,0])*(b[:,3]-b[:,1])\n",
    "    return inter / (area_a + area_b - inter + 1e-8)\n",
    "\n",
    "def nms(boxes, scores, iou_th=0.5):\n",
    "    idxs = np.argsort(scores)[::-1]; keep=[]\n",
    "    while idxs.size>0:\n",
    "        i = idxs[0]; keep.append(int(i))\n",
    "        if idxs.size==1: break\n",
    "        ious = iou_xyxy(boxes[i], boxes[idxs[1:]])\n",
    "        idxs = idxs[1:][ious <= iou_th]\n",
    "    return keep\n",
    "\n",
    "def generate_anchors(image_size=(224,224), grid=(7,7), scales=[0.5,1.0,2.0], ratios=[0.5,1.0,2.0]):\n",
    "    ih,iw = image_size; gh,gw = grid; anchors=[]\n",
    "    for i in range(gh):\n",
    "        for j in range(gw):\n",
    "            cx = (j+0.5)*iw/gw; cy = (i+0.5)*ih/gh\n",
    "            for s in scales:\n",
    "                for r in ratios:\n",
    "                    w = s*iw/gw*math.sqrt(r); h = s*ih/gh/math.sqrt(r)\n",
    "                    anchors.append([cx-w/2, cy-h/2, cx+w/2, cy+h/2])\n",
    "    return np.array(anchors)\n",
    "\n",
    "print('IoU demo:', iou_xyxy([30,30,120,160], [[28,25,118,158],[200,200,260,260]]))\n",
    "print('NMS demo keep:', nms(np.array([[28,25,118,158],[31,35,122,162],[200,200,260,260]]), np.array([0.9,0.7,0.6]), 0.4))\n",
    "print('Anchors count:', generate_anchors((224,224),(7,7)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee453b",
   "metadata": {},
   "source": [
    "## Face Recognition — Embedding & Triplet Loss (demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "class SmallEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Flatten(), nn.Linear(3*32*32,256), nn.ReLU(), nn.Linear(256,embed_dim))\n",
    "    def forward(self,x):\n",
    "        return F.normalize(self.net(x), p=2, dim=1)\n",
    "\n",
    "def triplet_loss(anc, pos, neg, margin=0.2):\n",
    "    pos_d = F.pairwise_distance(anc,pos); neg_d = F.pairwise_distance(anc,neg)\n",
    "    return F.relu(pos_d - neg_d + margin).mean()\n",
    "\n",
    "# demo forward\n",
    "embed = SmallEmbedder(32).to(device)\n",
    "anc = torch.randn(4,3,32,32).to(device); pos = torch.randn(4,3,32,32).to(device); neg = torch.randn(4,3,32,32).to(device)\n",
    "print('triplet demo loss:', triplet_loss(embed(anc), embed(pos), embed(neg)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf3932",
   "metadata": {},
   "source": [
    "## RNN / LSTM — Tiny sequence classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50466b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ToySeqDataset(Dataset):\n",
    "    def __init__(self, n=500, L=20, vocab=10):\n",
    "        self.X = torch.randint(0, vocab, (n,L))\n",
    "        self.y = (self.X.sum(dim=1) > (L*vocab//2)).long()\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab=10, emb=16, hidden=64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, emb)\n",
    "        self.lstm = nn.LSTM(emb, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x); out,_ = self.lstm(x); return self.fc(out[:,-1,:])\n",
    "\n",
    "ds = ToySeqDataset(600); tr, dev = random_split(ds, [480,120])\n",
    "tr_loader = DataLoader(tr, batch_size=64, shuffle=True); dv_loader = DataLoader(dev, batch_size=128)\n",
    "model = LSTMClassifier().to(device); opt = torch.optim.Adam(model.parameters(), lr=1e-3); loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for xb,yb in tr_loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        loss = loss_fn(model(xb), yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ys,ph=[] , []\n",
    "        for xb,yb in dv_loader:\n",
    "            xb,yb=xb.to(device), yb.to(device)\n",
    "            ph.append(model(xb).argmax(1).cpu())\n",
    "            ys.append(yb.cpu())\n",
    "        acc = (torch.cat(ys)==torch.cat(ph)).float().mean().item()\n",
    "    print('epoch', epoch+1, 'dev_acc', round(acc,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cd0e7",
   "metadata": {},
   "source": [
    "## Word Embeddings — Tiny Skip-Gram with Negative Sampling (toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c58f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = \"we like machine learning we like embeddings\".split()\n",
    "vocab = list(dict.fromkeys(corpus))\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "pairs=[]; window=2\n",
    "for i,w in enumerate(corpus):\n",
    "    for j in range(max(0,i-window), min(len(corpus), i+window+1)):\n",
    "        if i==j: continue\n",
    "        pairs.append((w2i[w], w2i[corpus[j]]))\n",
    "\n",
    "class SGNS(nn.Module):\n",
    "    def __init__(self, V, D=8):\n",
    "        super().__init__()\n",
    "        self.in_emb = nn.Embedding(V,D); self.out_emb = nn.Embedding(V,D)\n",
    "    def forward(self, c, o, negs):\n",
    "        v = self.in_emb(c); u = self.out_emb(o)\n",
    "        pos = (v*u).sum(dim=1); pos_loss = F.logsigmoid(pos)\n",
    "        neg = self.out_emb(negs)  # B,K,D\n",
    "        neg_score = torch.bmm(neg.neg(), v.unsqueeze(2)).squeeze()\n",
    "        neg_loss = F.logsigmoid(neg_score).sum(dim=1)\n",
    "        return -(pos_loss + neg_loss).mean()\n",
    "\n",
    "V = len(vocab)\n",
    "model = SGNS(V, D=8).to(device); opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "pairs = np.array(pairs)\n",
    "for epoch in range(3):\n",
    "    np.random.shuffle(pairs)\n",
    "    losses=[]\n",
    "    for i in range(0, len(pairs), 4):\n",
    "        batch = pairs[i:i+4]\n",
    "        c = torch.LongTensor(batch[:,0]).to(device); o = torch.LongTensor(batch[:,1]).to(device)\n",
    "        negs = torch.randint(0, V, (len(batch), 5)).to(device)\n",
    "        loss = model(c,o,negs); opt.zero_grad(); loss.backward(); opt.step(); losses.append(loss.item())\n",
    "    print('epoch', epoch+1, 'loss', np.mean(losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90410bb",
   "metadata": {},
   "source": [
    "## Seq2Seq & Attention — Transformer encoder-decoder demo + beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66399866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOK={'<pad>':0,'<s>':1,'</s>':2,'a':3,'b':4,'c':5}\n",
    "invTOK={v:k for k,v in TOK.items()}\n",
    "V=len(TOK)\n",
    "class TinyTrans(nn.Module):\n",
    "    def __init__(self,V,d_model=32,nhead=4,nlayers=1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(V,d_model)\n",
    "        self.trans = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=nlayers, num_decoder_layers=nlayers, batch_first=True)\n",
    "        self.out = nn.Linear(d_model, V)\n",
    "    def forward(self, src, tgt):\n",
    "        tgt_mask = self.trans.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        return self.out(self.trans(self.emb(src), self.emb(tgt), tgt_mask=tgt_mask))\n",
    "\n",
    "def beam_search_simple(model, src, start=1, end=2, beam=3, maxlen=8):\n",
    "    model.eval(); src = torch.LongTensor(src).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        memory = model.trans.encoder(model.emb(src))\n",
    "        beams=[(0.0,[start])]\n",
    "        for _ in range(maxlen):\n",
    "            new_beams=[]\n",
    "            for score, seq in beams:\n",
    "                if seq[-1]==end:\n",
    "                    new_beams.append((score, seq)); continue\n",
    "                tgt = torch.LongTensor(seq).unsqueeze(0).to(device)\n",
    "                tgt_mask = model.trans.generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "                dec = model.trans.decoder(model.emb(tgt), memory, tgt_mask=tgt_mask)\n",
    "                logits = model.out(dec[:, -1, :])\n",
    "                logp = F.log_softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
    "                topk = np.argsort(logp)[-beam:][::-1]\n",
    "                for k in topk:\n",
    "                    new_beams.append((score + float(logp[k]), seq+[int(k)]))\n",
    "            beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam]\n",
    "        best = max(beams, key=lambda x:x[0])[1]\n",
    "        return best\n",
    "\n",
    "model = TinyTrans(V).to(device)\n",
    "print('Beam demo tokens:', beam_search_simple(model, [3,4,3], beam=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fdea9",
   "metadata": {},
   "source": [
    "## Transformer — Self-attention forward demo\n",
    "Demonstrates positional encoding + nn.MultiheadAttention forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc95a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def positional_encoding(n,d):\n",
    "    pe = np.zeros((n,d))\n",
    "    pos = np.arange(n)[:,None]\n",
    "    div = np.exp(np.arange(0,d,2)*( - math.log(10000.0)/d))\n",
    "    pe[:,0::2] = np.sin(pos*div); pe[:,1::2] = np.cos(pos*div)\n",
    "    return torch.FloatTensor(pe).unsqueeze(0)  # (1,n,d)\n",
    "\n",
    "mha = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "x = torch.randn(2,6,32)\n",
    "x_pe = x + positional_encoding(6,32)\n",
    "out, _ = mha(x_pe, x_pe, x_pe)\n",
    "print('MHA output shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ea170",
   "metadata": {},
   "source": [
    "----\n",
    "Notebook saved to /mnt/data/ML_Advanced_Topics_Playbook.ipynb\n",
    "If you want additions (more detailed detection head, real dataset examples, extended training loops), tell me which part to expand."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}