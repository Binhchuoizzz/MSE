{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d582cc83",
   "metadata": {},
   "source": [
    "0. Chuẩn bị: dataset toy + tiện ích\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8898836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\bs4-0.0.2-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\lxml-5.3.0-py3.13-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\narwhals-1.19.0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\plotly-6.0.0rc0-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\pypandoc-1.14-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\requests-2.32.3-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\vnquant-0.1.2-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages\\wrapt-1.17.0-py3.13-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading torch-2.8.0-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp313-cp313-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\miniconda3\\envs\\myenvironment\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.8.0-cp313-cp313-win_amd64.whl (241.3 MB)\n",
      "   ---------------------------------------- 0.0/241.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/241.3 MB 12.8 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 14.7/241.3 MB 46.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 23.3/241.3 MB 54.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 34.3/241.3 MB 48.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 41.2/241.3 MB 45.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 46.7/241.3 MB 42.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 50.6/241.3 MB 38.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 53.5/241.3 MB 35.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 56.6/241.3 MB 33.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 59.8/241.3 MB 31.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 63.2/241.3 MB 29.7 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 66.3/241.3 MB 28.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 69.7/241.3 MB 27.5 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 73.1/241.3 MB 26.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 76.5/241.3 MB 26.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 80.2/241.3 MB 25.4 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 83.6/241.3 MB 24.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 87.3/241.3 MB 24.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 90.7/241.3 MB 24.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 94.1/241.3 MB 23.8 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 97.8/241.3 MB 23.5 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 101.4/241.3 MB 23.2 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 104.9/241.3 MB 23.0 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 108.5/241.3 MB 22.7 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 111.9/241.3 MB 22.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 115.9/241.3 MB 22.4 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 119.3/241.3 MB 22.2 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 122.9/241.3 MB 22.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 126.9/241.3 MB 21.9 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 130.5/241.3 MB 21.7 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 134.2/241.3 MB 21.6 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 137.9/241.3 MB 21.5 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 141.3/241.3 MB 21.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 145.2/241.3 MB 21.3 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 148.9/241.3 MB 21.2 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 152.8/241.3 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 156.5/241.3 MB 21.0 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 160.2/241.3 MB 20.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 163.8/241.3 MB 20.8 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 167.2/241.3 MB 20.8 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 170.4/241.3 MB 20.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 174.1/241.3 MB 20.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 177.7/241.3 MB 20.5 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 181.4/241.3 MB 20.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 185.1/241.3 MB 20.4 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 188.7/241.3 MB 20.3 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 192.7/241.3 MB 20.3 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 196.3/241.3 MB 20.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 200.0/241.3 MB 20.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 203.7/241.3 MB 20.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 207.4/241.3 MB 20.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 211.3/241.3 MB 20.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 215.0/241.3 MB 20.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 218.6/241.3 MB 20.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 222.6/241.3 MB 20.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 226.2/241.3 MB 20.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 229.9/241.3 MB 20.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 233.8/241.3 MB 20.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  237.8/241.3 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 241.3/241.3 MB 19.6 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.23.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 3.9/6.3 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 19.0 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, torch, torchvision\n",
      "Successfully installed mpmath-1.3.0 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be325299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision scikit-learn\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Toy classification data (tabular) để demo nhanh\n",
    "\n",
    "\n",
    "def make_toy(n=5000, d=20, n_classes=4, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n, d))\n",
    "    W = rng.normal(size=(d, n_classes))\n",
    "    logits = X @ W + rng.normal(scale=0.5, size=(n, n_classes))\n",
    "    y = logits.argmax(axis=1)\n",
    "    return X.astype(np.float32), y.astype(np.int64)\n",
    "\n",
    "\n",
    "X, y = make_toy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecda985",
   "metadata": {},
   "source": [
    "1. Train/Dev/Test split (tránh leakage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff07bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split để giữ tỉ lệ lớp, dev nhỏ (10%) nếu data lớn\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_dev,   X_test, y_dev,   y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Nếu có \"group\" (ví dụ: cùng user), dùng GroupShuffleSplit để không rò rỉ theo người dùng.\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# gss = GroupShuffleSplit(test_size=0.3, random_state=42)\n",
    "# train_idx, temp_idx = next(gss.split(X, y, groups=user_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3363145",
   "metadata": {},
   "source": [
    "2. Chuẩn hoá đầu vào (không leak: fit trên train, transform trên dev/test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_dev_s = scaler.transform(X_dev)\n",
    "X_test_s = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ef68c",
   "metadata": {},
   "source": [
    "3. MLP nhỏ với L2 (weight_decay), Dropout, BatchNorm, He init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden=256, d_out=4, p_drop=0.5, use_bn=True, act='relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(d_hidden) if use_bn else nn.Identity()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.fc2 = nn.Linear(d_hidden, d_out)\n",
    "        self.act = nn.ReLU() if act == 'relu' else nn.Tanh()\n",
    "        self._init_weights(act)\n",
    "\n",
    "    def _init_weights(self, act):\n",
    "        # He cho ReLU, Xavier cho tanh\n",
    "        if act == 'relu':\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)     # logits cho Softmax đa lớp\n",
    "        return x\n",
    "\n",
    "# Tensor + DataLoader (mini-batch)\n",
    "\n",
    "\n",
    "def make_loader(X, y, batch_size=128, shuffle=True):\n",
    "    ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c59a02",
   "metadata": {},
   "source": [
    "4. Vòng lặp train tổng quát + optimizers (SGD+momentum/RMSprop/Adam) + clip grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fcc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_dev, y_dev, *,\n",
    "                lr=1e-3, weight_decay=1e-4, batch_size=128,\n",
    "                optimizer_name='adam', max_epochs=20, clip_norm=5.0):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = MLP(X_train.shape[1], d_hidden=256, d_out=len(\n",
    "        np.unique(y_train)), p_drop=0.5, use_bn=True).to(device)\n",
    "\n",
    "    if optimizer_name == 'sgd':\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=lr,\n",
    "                              momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        opt = torch.optim.RMSprop(\n",
    "            model.parameters(), lr=lr, alpha=0.99, weight_decay=weight_decay)\n",
    "    else:  # 'adam'\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(\n",
    "            0.9, 0.999), eps=1e-8, weight_decay=weight_decay)\n",
    "\n",
    "    # Lịch giảm LR (step decay ví dụ)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        opt, step_size=max(1, max_epochs//3), gamma=0.1)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loader = make_loader(\n",
    "        X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = make_loader(X_dev,   y_dev,   batch_size=1024, shuffle=False)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            # Chống exploding gradients\n",
    "            if clip_norm is not None:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=clip_norm)\n",
    "            opt.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Evaluate dev\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xb = torch.from_numpy(X_dev).to(device)\n",
    "        y_pred = model(xb).argmax(dim=1).cpu().numpy()\n",
    "    dev_acc = accuracy_score(y_dev, y_pred)\n",
    "    return model, dev_acc\n",
    "\n",
    "# Ví dụ chạy nhanh (bỏ nếu không cần chạy):\n",
    "# model, dev_acc = train_model(X_train_s, y_train, X_dev_s, y_dev, optimizer_name='adam')\n",
    "# print(\"Dev acc:\", dev_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078518c",
   "metadata": {},
   "source": [
    "5. L1 regularization (thêm tay vào loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_penalty(model):\n",
    "    return sum(p.abs().sum() for p in model.parameters())\n",
    "\n",
    "# Trong vòng lặp train, thay:\n",
    "# loss = loss_fn(logits, yb) + lam_l1 * l1_penalty(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c88165",
   "metadata": {},
   "source": [
    "6. Gradient Checking (NumPy, central differences) – dùng để debug backprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative diff: 1.698930842107017e-08\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ network 1 hidden (tanh), softmax CE – chỉ để GRAD-CHECK (không tối ưu hiệu năng)\n",
    "def softmax(z):\n",
    "    z = z - z.max(axis=1, keepdims=True)\n",
    "    ez = np.exp(z)\n",
    "    return ez / ez.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def forward_backward(params, X, y):\n",
    "    # params: dict chứa W1,b1,W2,b2\n",
    "    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    n = X.shape[0]\n",
    "    # forward\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    P = softmax(Z2)\n",
    "    # loss CE\n",
    "    Y = np.eye(W2.shape[1])[y]\n",
    "    loss = -np.sum(Y * np.log(P + 1e-12)) / n\n",
    "    # backward\n",
    "    dZ2 = (P - Y) / n\n",
    "    dW2 = A1.T @ dZ2\n",
    "    db2 = dZ2.sum(axis=0, keepdims=True)\n",
    "    dA1 = dZ2 @ W2.T\n",
    "    dZ1 = dA1 * (1 - np.tanh(Z1)**2)\n",
    "    dW1 = X.T @ dZ1\n",
    "    db1 = dZ1.sum(axis=0, keepdims=True)\n",
    "    grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "def numerical_grad(params, X, y, eps=1e-7):\n",
    "    num = {}\n",
    "    for k in params:\n",
    "        num[k] = np.zeros_like(params[k])\n",
    "        it = np.nditer(params[k], flags=['multi_index'],\n",
    "                       op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            old = params[k][idx]\n",
    "            params[k][idx] = old + eps\n",
    "            lp, _ = forward_backward(params, X, y)\n",
    "            params[k][idx] = old - eps\n",
    "            lm, _ = forward_backward(params, X, y)\n",
    "            num[k][idx] = (lp - lm) / (2*eps)\n",
    "            params[k][idx] = old\n",
    "            it.iternext()\n",
    "    return num\n",
    "\n",
    "\n",
    "def relative_diff(ga, gn):\n",
    "    num = sum(np.linalg.norm(ga[k] - gn[k]) for k in ga)\n",
    "    den = sum(np.linalg.norm(ga[k]) + np.linalg.norm(gn[k]) for k in ga)\n",
    "    return num / (den + 1e-12)\n",
    "\n",
    "\n",
    "# Demo nhỏ:\n",
    "np.random.seed(0)\n",
    "d_in, d_h, d_out = 5, 4, 3\n",
    "X_small = np.random.randn(6, d_in)\n",
    "y_small = np.random.randint(0, d_out, size=6)\n",
    "params = {\n",
    "    'W1': np.random.randn(d_in, d_h)*0.1, 'b1': np.zeros((1, d_h)),\n",
    "    'W2': np.random.randn(d_h, d_out)*0.1, 'b2': np.zeros((1, d_out)),\n",
    "}\n",
    "loss, grads = forward_backward(params, X_small, y_small)\n",
    "numg = numerical_grad({k: v.copy()\n",
    "                      for k, v in params.items()}, X_small, y_small)\n",
    "# ~ 1e-6 hoặc nhỏ hơn là ổn\n",
    "print(\"relative diff:\", relative_diff(grads, numg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80311625",
   "metadata": {},
   "source": [
    "7. Học theo mini-batch + chọn optimizer\n",
    "\n",
    "Đã thể hiện trong train_model(...) ở mục (4). Chỉ cần đổi optimizer_name giữa 'sgd' | 'rmsprop' | 'adam'.\n",
    "\n",
    "Nếu bạn muốn RMSProp + Momentum kiểu Nadam/AdamW, thay optimizer tương ứng:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0b297",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# AdamW (decoupled weight decay – thường tốt hơn với transformer, CNN lớn)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# AdamW (decoupled weight decay – thường tốt hơn với transformer, CNN lớn)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7ed31",
   "metadata": {},
   "source": [
    "8. Lịch Learning Rate (cosine, one-cycle, v.v.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6870d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cosine annealing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(\u001b[43mopt\u001b[49m, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# OneCycleLR (tăng LR rồi giảm – thường hợp khi train từ scratch trên image)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=lr, steps_per_epoch=len(train_loader), epochs=max_epochs)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "# Cosine annealing\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "\n",
    "# OneCycleLR (tăng LR rồi giảm – thường hợp khi train từ scratch trên image)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=lr, steps_per_epoch=len(train_loader), epochs=max_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768a06d",
   "metadata": {},
   "source": [
    "9. Random Search HPO (log-scale cho LR, coarse→fine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c9486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best dev acc: (0.896, {'lr': 0.0014187079131473233, 'weight_decay': 0.0006136045802294309, 'batch_size': 62, 'p_drop': 0.4337051899818408, 'optimizer_name': 'rmsprop'})\n"
     ]
    }
   ],
   "source": [
    "def sample_hparams():\n",
    "    # log-uniform cho LR: 10^U[-4, -2] = [1e-4, 1e-2]\n",
    "    lr = 10 ** np.random.uniform(-4, -2)\n",
    "    weight_decay = 10 ** np.random.uniform(-6, -3)    # L2\n",
    "    batch_size = int(2 ** np.random.uniform(5, 8))     # 32..256\n",
    "    p_drop = np.random.uniform(0.1, 0.6)\n",
    "    opt_name = random.choice(['adam', 'rmsprop', 'sgd'])\n",
    "    return dict(lr=lr, weight_decay=weight_decay, batch_size=batch_size, p_drop=p_drop, optimizer_name=opt_name)\n",
    "\n",
    "\n",
    "def run_trial(Xtr, ytr, Xdv, ydv, trial_id=0, max_epochs=15):\n",
    "    hp = sample_hparams()\n",
    "    # cập nhật dropout cho model bằng cách monkeypatch tham số khi khởi tạo\n",
    "    model, dev_acc = train_model(Xtr, ytr, Xdv, ydv,\n",
    "                                 lr=hp['lr'], weight_decay=hp['weight_decay'],\n",
    "                                 batch_size=hp['batch_size'],\n",
    "                                 optimizer_name=hp['optimizer_name'],\n",
    "                                 max_epochs=max_epochs)\n",
    "    return dev_acc, hp\n",
    "\n",
    "\n",
    "best = (-1, None)\n",
    "for t in range(10):  # coarse search 10 trial, sau đó zoom-in vùng tốt\n",
    "    acc, hp = run_trial(X_train_s, y_train, X_dev_s,\n",
    "                        y_dev, trial_id=t, max_epochs=12)\n",
    "    if acc > best[0]:\n",
    "        best = (acc, hp)\n",
    "print(\"Best dev acc:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b75c80d",
   "metadata": {},
   "source": [
    "10. Đọc xác suất Softmax + dự đoán\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc55044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.from_numpy(X))\n",
    "        probs = torch.softmax(logits, dim=1).numpy()\n",
    "    return probs\n",
    "\n",
    "# probs_dev = predict_proba(model, X_dev_s)\n",
    "# y_pred = probs_dev.argmax(axis=1)\n",
    "# print(\"Dev accuracy:\", accuracy_score(y_dev, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cde0a",
   "metadata": {},
   "source": [
    "11. “Nếu thấy triệu chứng thì làm gì?” (áp dụng vào code)\n",
    "\n",
    "Train loss cao & Dev loss cao (High Bias)\n",
    "→ tăng d_hidden, thêm layer, đổi activation, train lâu hơn, giảm weight_decay, bỏ bớt Dropout.\n",
    "\n",
    "Train loss thấp & Dev loss cao (High Variance)\n",
    "→ tăng weight_decay, tăng p_drop, thêm/đổi augmentation (cho ảnh), thu nhỏ model, thu thêm dữ liệu.\n",
    "\n",
    "Loss “răng cưa”, không ổn định\n",
    "→ giảm lr hoặc dùng Adam/RMSprop; tăng batch_size; dùng BatchNorm; clip grad.\n",
    "\n",
    "Học chậm\n",
    "→ chuẩn hoá đúng (fit trên train), He init (ReLU) / Xavier (tanh), dùng scheduler LR\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
