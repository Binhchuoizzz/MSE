{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP501 - NATURAL LANGUAGE PROCESSING\n",
        "# LAB 01\n",
        "## Text Preprocessing & Feature Extraction\n",
        "## Sentiment Analysis with Logistic Regression\n",
        "\n",
        "- **Tools:** Python 3.8+, Jupyter Notebook, NLTK, Scikit-learn\n",
        "- **Dataset:** NLTK Twitter Samples / Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chuẩn Bị Môi Trường\n",
        "\n",
        "### 2.1 Cài đặt thư viện\n",
        "\n",
        "Mở terminal và chạy các lệnh sau:\n",
        "\n",
        "```bash\n",
        "# Tạo virtual environment \n",
        "python -m venv nlp_env\n",
        "source nlp_env/bin/activate  # Linux/Mac\n",
        "nlp_env\\Scripts\\activate   # Windows\n",
        "\n",
        "# Cài đặt các thư viện cần thiết\n",
        "pip install nltk numpy pandas scikit-learn matplotlib seaborn jupyter\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Download NLTK Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('twitter_samples')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Kiểm tra cài đặt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Test\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "print(f'Loaded {len(positive_tweets)} positive tweets')\n",
        "print(f'Sample: {positive_tweets[0][:50]}...')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Phần 1: Text Preprocessing\n",
        "\n",
        "### 3.1 Load và khám phá dữ liệu\n",
        "\n",
        "**Task 1:** Load dữ liệu Twitter và quan sát cấu trúc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "import random\n",
        "\n",
        "# Load positive và negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "print(f'Number of positive tweets: {len(positive_tweets)}')\n",
        "print(f'Number of negative tweets: {len(negative_tweets)}')\n",
        "\n",
        "print('\\n POSITIVE TWEETS ')\n",
        "for tweet in positive_tweets[:3]:\n",
        "    print(f'- {tweet[:80]}...')\n",
        "\n",
        "print('\\n NEGATIVE TWEETS')\n",
        "for tweet in negative_tweets[:3]:\n",
        "    print(f'- {tweet[:80]}...')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Xây dựng Preprocessing Pipeline\n",
        "\n",
        "**Task 2:** Implement các hàm tiền xử lý văn bản\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# NOTE: keep some stopwords for sentiment\n",
        "stop_words.discard('not')\n",
        "stop_words.discard('no')\n",
        "stop_words.discard('nor')\n",
        "stop_words.discard(\"won't\")\n",
        "stop_words.discard(\"wouldn't\")\n",
        "stop_words.discard(\"couldn't\")\n",
        "stop_words.discard(\"shouldn't\")\n",
        "\n",
        "def preprocess_tweet(tweet, use_stemming=False):\n",
        "    '''\n",
        "    tweet preprocessing.\n",
        "    \n",
        "    Args:\n",
        "        tweet: string - raw tweet text\n",
        "        use_stemming: bool - True for stemming, False for lemmatization\n",
        "    \n",
        "    Returns:\n",
        "        list of tokens after preprocessing\n",
        "    '''\n",
        "    # 1. Lowercase\n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    # 2. Remove URLs\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
        "    \n",
        "    # 3. Remove @mentions\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)\n",
        "    \n",
        "    # 4. Remove hashtag symbol\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    \n",
        "    # 5. Remove punctuation và special characters\n",
        "    tweet = re.sub(r'[^a-zA-Z\\s]', '', tweet)\n",
        "    \n",
        "    # 6. Tokenize\n",
        "    tokens = word_tokenize(tweet)\n",
        "    \n",
        "    # 7. Remove stopwords\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
        "    \n",
        "    # 8. Stemming or Lemmatization\n",
        "    if use_stemming:\n",
        "        tokens = [stemmer.stem(t) for t in tokens]\n",
        "    else:\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Test preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_tweet = \"@user I LOVE this movie!!! It's SO good :) #amazing #best http://example.com\"\n",
        "\n",
        "print('Original:', sample_tweet)\n",
        "print('Processed:', preprocess_tweet(sample_tweet))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Comparing Stemming vs Lemmatization\n",
        "\n",
        "**Task 3:** So sánh kết quả của hai phương pháp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_words = ['running', 'better', 'studies', 'flying', 'happily', 'wolves']\n",
        "\n",
        "print(f'{\"Word\":<12} {\"Stemmed\":<12} {\"Lemmatized\":<12}')\n",
        "print('-' * 36)\n",
        "for word in test_words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    lemmatized = lemmatizer.lemmatize(word)\n",
        "    print(f'{word:<12} {stemmed:<12} {lemmatized:<12}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Extraction\n",
        "\n",
        "### 4.1 Build Vocabulary và Frequency Dictionary\n",
        "\n",
        "**Task 4:** Xây dựng từ điển tần số từ corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_frequency_dict(tweets, labels):\n",
        "    freq_dict = defaultdict(int)\n",
        "    \n",
        "    for tweet, label in zip(tweets, labels):\n",
        "        tokens = preprocess_tweet(tweet)\n",
        "        for token in tokens:\n",
        "            freq_dict[(token, label)] += 1\n",
        "    \n",
        "    return freq_dict\n",
        "\n",
        "all_tweets = positive_tweets + negative_tweets\n",
        "labels = [1] * len(positive_tweets) + [0] * len(negative_tweets)\n",
        "\n",
        "freq_dict = build_frequency_dict(all_tweets, labels)\n",
        "\n",
        "print(' Top words in POSITIVE tweets ')\n",
        "pos_words = [(k[0], v) for k, v in freq_dict.items() if k[1] == 1]\n",
        "pos_words.sort(key=lambda x: -x[1])\n",
        "for word, count in pos_words[:10]:\n",
        "    print(f'{word}: {count}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Feature Extraction: Simple Frequency Features\n",
        "\n",
        "**Task 5:** Implement feature extraction đơn giản"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def extract_features(tweet, freq_dict):\n",
        "    '''\n",
        "    Features:\n",
        "        - pos_freq: tổng tần số positive của các từ trong tweet\n",
        "        - neg_freq: tổng tần số negative của các từ trong tweet\n",
        "    \n",
        "    Returns:\n",
        "        numpy array [pos_freq, neg_freq]\n",
        "    '''\n",
        "    \n",
        "    tokens = preprocess_tweet(tweet)\n",
        "    \n",
        "    pos_freq = 0\n",
        "    neg_freq = 0\n",
        "    \n",
        "    for token in tokens:\n",
        "        pos_freq += freq_dict.get((token, 1), 0)\n",
        "        neg_freq += freq_dict.get((token, 0), 0)\n",
        "    \n",
        "    return np.array([pos_freq, neg_freq])\n",
        "\n",
        "# Test\n",
        "test_tweet = \"I love this beautiful day\"\n",
        "features = extract_features(test_tweet, freq_dict)\n",
        "print(f'Tweet: {test_tweet}')\n",
        "print(f'Features [pos_freq, neg_freq]: {features}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Implement TF-IDF\n",
        "\n",
        "**Task 6:** Implement TF-IDF vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class SimpleTFIDF:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.idf = {}\n",
        "        self.num_docs = 0\n",
        "    \n",
        "    def fit(self, documents):\n",
        "        self.num_docs = len(documents)\n",
        "        doc_freq = Counter()\n",
        "        \n",
        "        for doc in documents:\n",
        "            tokens = set(preprocess_tweet(doc))\n",
        "            for token in tokens:\n",
        "                doc_freq[token] += 1\n",
        "        \n",
        "        for idx, (word, df) in enumerate(doc_freq.items()):\n",
        "            self.vocab[word] = idx\n",
        "            self.idf[word] = math.log(self.num_docs / df)\n",
        "        \n",
        "        print(f'Vocabulary size: {len(self.vocab)}')\n",
        "        return self\n",
        "    \n",
        "    def transform(self, documents):\n",
        "        vectors = []\n",
        "        for doc in documents:\n",
        "            tokens = preprocess_tweet(doc)\n",
        "            tf = Counter(tokens)\n",
        "            total_terms = len(tokens) if tokens else 1\n",
        "            \n",
        "            vector = np.zeros(len(self.vocab))\n",
        "            for token, count in tf.items():\n",
        "                if token in self.vocab:\n",
        "                    tf_val = count / total_terms\n",
        "                    tfidf_val = tf_val * self.idf[token]\n",
        "                    vector[self.vocab[token]] = tfidf_val\n",
        "            \n",
        "            vectors.append(vector)\n",
        "        \n",
        "        return np.array(vectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test với small corpus\n",
        "test_corpus = [\n",
        "    'I love this movie',\n",
        "    'This movie is terrible',\n",
        "    'Great movie, I love it'\n",
        "]\n",
        "\n",
        "tfidf = SimpleTFIDF()\n",
        "tfidf.fit(test_corpus)\n",
        "vectors = tfidf.transform(test_corpus)\n",
        "\n",
        "print('Vocabulary:', list(tfidf.vocab.keys()))\n",
        "print('\\nTF-IDF vectors shape:', vectors.shape)\n",
        "print('\\nFirst document vector:')\n",
        "for word, idx in tfidf.vocab.items():\n",
        "    if vectors[0][idx] > 0:\n",
        "        print(f'  {word}: {vectors[0][idx]:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Phần 3: Train Logistic Regression\n",
        "\n",
        "### 5.1 Chuẩn bị dữ liệu\n",
        "\n",
        "**Task 7:** Chia train/test và tạo feature matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tạo dataset\n",
        "all_tweets = positive_tweets + negative_tweets\n",
        "labels = np.array([1] * len(positive_tweets) + [0] * len(negative_tweets))\n",
        "\n",
        "# Shuffle dữ liệu\n",
        "indices = np.random.permutation(len(all_tweets))\n",
        "all_tweets = [all_tweets[i] for i in indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Train/Test split (80/20)\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    all_tweets, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Training set: {len(X_train_raw)} samples')\n",
        "print(f'Test set: {len(X_test_raw)} samples')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Phương pháp 1: Simple Frequency Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build frequency dict từ training data only\n",
        "freq_dict = build_frequency_dict(X_train_raw, y_train)\n",
        "\n",
        "# Tạo feature matrix\n",
        "X_train = np.array([extract_features(t, freq_dict) for t in X_train_raw])\n",
        "X_test = np.array([extract_features(t, freq_dict) for t in X_test_raw])\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Train Logistic Regression\n",
        "\n",
        "**Task 8:** Huấn luyện và đánh giá mô hình\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Training Accuracy: {accuracy_score(y_train, y_pred_train):.4f}')\n",
        "print(f'Test Accuracy: {accuracy_score(y_test, y_pred_test):.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Detailed Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification Report\n",
        "print('\\n CLASSIFICATION REPORT ')\n",
        "print(classification_report(y_test, y_pred_test, target_names=['Negative', 'Positive']))\n",
        "\n",
        "# Confusion Matrix\n",
        "print('\\n CONFUSION MATRIX ')\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "print(f'              Predicted')\n",
        "print(f'              Neg    Pos')\n",
        "print(f'Actual Neg   {cm[0][0]:4d}   {cm[0][1]:4d}')\n",
        "print(f'Actual Pos   {cm[1][0]:4d}   {cm[1][1]:4d}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Visualize Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Sentiment Analysis')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 Phân tích Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Model weights:')\n",
        "print(f'  w_pos_freq = {model.coef_[0][0]:.6f}')\n",
        "print(f'  w_neg_freq = {model.coef_[0][1]:.6f}')\n",
        "print(f'  bias = {model.intercept_[0]:.6f}')\n",
        "\n",
        "if model.coef_[0][0] > 0:\n",
        "    print('- Positive frequency có trọng số DƯƠNG -> tăng pos_freq -> tăng P(positive)')\n",
        "if model.coef_[0][1] < 0:\n",
        "    print('- Negative frequency có trọng số ÂM -> tăng neg_freq -> giảm P(positive)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Phần 4: Error Analysis\n",
        "\n",
        "### 6.1 Tìm các trường hợp dự đoán sai\n",
        "\n",
        "**Task 9:** Phân tích các lỗi của mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tìm các mẫu dự đoán sai\n",
        "errors = []\n",
        "for i in range(len(X_test_raw)):\n",
        "    if y_pred_test[i] != y_test[i]:\n",
        "        errors.append({\n",
        "            'tweet': X_test_raw[i],\n",
        "            'actual': 'Positive' if y_test[i] == 1 else 'Negative',\n",
        "            'predicted': 'Positive' if y_pred_test[i] == 1 else 'Negative',\n",
        "            'features': X_test[i]\n",
        "        })\n",
        "\n",
        "print(f'Total errors: {len(errors)} / {len(X_test_raw)} ({100*len(errors)/len(X_test_raw):.1f}%)')\n",
        "\n",
        "print('\\n SAMPLE ERRORS ')\n",
        "for i, err in enumerate(errors[:10]):\n",
        "    print(f\"\\n[{i+1}] Actual: {err['actual']}, Predicted: {err['predicted']}\")\n",
        "    print(f\"    Features: pos_freq={err['features'][0]:.0f}, neg_freq={err['features'][1]:.0f}\")\n",
        "    print(f\"    Tweet: {err['tweet'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Phân loại các loại lỗi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorize errors\n",
        "false_positives = [e for e in errors if e['predicted'] == 'Positive']\n",
        "false_negatives = [e for e in errors if e['predicted'] == 'Negative']\n",
        "\n",
        "print(f'False Positives (predicted + but actual -): {len(false_positives)}')\n",
        "print(f'False Negatives (predicted - but actual +): {len(false_negatives)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Test với các câu mới\n",
        "\n",
        "**Task 10:** Test mô hình với dữ liệu mới"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(tweet, model, freq_dict):\n",
        "    '''Dự đoán sentiment cho một tweet mới'''\n",
        "    features = extract_features(tweet, freq_dict).reshape(1, -1)\n",
        "    pred = model.predict(features)[0]\n",
        "    prob = model.predict_proba(features)[0]\n",
        "    \n",
        "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
        "    confidence = prob[pred]\n",
        "    \n",
        "    return sentiment, confidence\n",
        "\n",
        "test_sentences = [\n",
        "    'I absolutely love this product! Best purchase ever!',\n",
        "    'This is the worst experience I have ever had.',\n",
        "    'The movie was okay, nothing special.',\n",
        "    'I am not happy with this service.',\n",
        "    'Great job, really disappointed...',  # sarcasm\n",
        "]\n",
        "\n",
        "print(' PREDICTIONS ')\n",
        "for sentence in test_sentences:\n",
        "    sentiment, conf = predict_sentiment(sentence, model, freq_dict)\n",
        "    print(f'\\n\"{sentence}\"')\n",
        "    print(f'  -> {sentiment} (confidence: {conf:.2%})')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
