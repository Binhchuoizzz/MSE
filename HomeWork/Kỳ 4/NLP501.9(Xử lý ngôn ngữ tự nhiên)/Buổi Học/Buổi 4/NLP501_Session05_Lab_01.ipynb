{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrect System & HMM POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: AUTOCORRECT SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Xây dựng Vocabulary từ Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus():\n",
    "    \"\"\"\n",
    "    Load and preprocess text corpus from NLTK Brown corpus.\n",
    "    Returns:\n",
    "        words: list of all words in corpus (lowercased)\n",
    "    \"\"\"\n",
    "    words = brown.words()\n",
    "    \n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def build_vocab(words):\n",
    "    \"\"\"\n",
    "    Build vocabulary\n",
    "    \"\"\"\n",
    "    return set(words)\n",
    "\n",
    "def count_word_freq(words):\n",
    "    \"\"\"\n",
    "    Count word frequency\n",
    "    Returns:\n",
    "        word_freq: Counter object với {word: count}\n",
    "    \"\"\"\n",
    "    return Counter(words)\n",
    "\n",
    "corpus_words = load_corpus()\n",
    "vocab = build_vocab(corpus_words)\n",
    "word_freq = count_word_freq(corpus_words)\n",
    "\n",
    "print(f\"Total words in corpus: {len(corpus_words):,}\")\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"\\nTop 10 most frequent words:\")\n",
    "for word, count in word_freq.most_common(10):\n",
    "    print(f\"  {word}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target):\n",
    "    \"\"\"\n",
    "    Calculate Minimum Edit Distance between source and target string.\n",
    "    \n",
    "    Args:\n",
    "        source: source string\n",
    "        target: target string\n",
    "    \n",
    "    Returns:\n",
    "        distance: minimum edit distance\n",
    "    \"\"\"\n",
    "    m, n = len(source), len(target)\n",
    "    \n",
    "    # Initialize DP matrix\n",
    "    # D[i][j] = edit distance between source[:i] and target[:j]\n",
    "    D = np.zeros((m + 1, n + 1), dtype=int)\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        D[i, 0] = i  # Delete i characters to become empty string\n",
    "    \n",
    "    for j in range(n + 1):\n",
    "        D[0, j] = j  # Insert j characters from empty string\n",
    "    \n",
    "    # Fill the DP table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            # Hint: Compare source[i-1] with target[j-1]\n",
    "            \n",
    "            # Cost of substitution (0 if match, 1 if not)\n",
    "            if source[i-1] == target[j-1]:\n",
    "                sub_cost = 0\n",
    "            else:\n",
    "                sub_cost = 1\n",
    "            \n",
    "            # Calculate minimum of 3 operations\n",
    "            D[i, j] = min(\n",
    "                D[i-1, j] + 1,      # Delete\n",
    "                D[i, j-1] + 1,      # Insert\n",
    "                D[i-1, j-1] + sub_cost  # Substitute/Match\n",
    "            )\n",
    "    \n",
    "    return D[m, n]\n",
    "\n",
    "test_cases = [\n",
    "    (\"cat\", \"cut\"),     \n",
    "    (\"intention\", \"execution\"),  \n",
    "    (\"kitten\", \"sitting\"),       \n",
    "    (\"sunday\", \"saturday\"),      \n",
    "]\n",
    "\n",
    "print(\"Testing Edit Distance:\")\n",
    "for source, target in test_cases:\n",
    "    dist = min_edit_distance(source, target)\n",
    "    print(f\"  '{source}' -> '{target}': {dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Backtrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance_with_backtrace(source, target):\n",
    "    \"\"\"\n",
    "    Calculate Edit Distance and return sequence of operations.\n",
    "    \n",
    "    Returns:\n",
    "        distance: edit distance\n",
    "        operations: list of (operation, position, char) tuples\n",
    "    \"\"\"\n",
    "    m, n = len(source), len(target)\n",
    "    \n",
    "    D = np.zeros((m + 1, n + 1), dtype=int)\n",
    "    \n",
    "    # Backpointer table: 0: match, 1: substitute, 2: insert, 3: delete\n",
    "    backpointer = np.zeros((m + 1, n + 1), dtype=int)\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        D[i, 0] = i\n",
    "        backpointer[i, 0] = 3  # delete\n",
    "    \n",
    "    for j in range(n + 1):\n",
    "        D[0, j] = j\n",
    "        backpointer[0, j] = 2  # insert\n",
    "    \n",
    "    # Fill tables\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if source[i-1] == target[j-1]:\n",
    "                sub_cost = 0\n",
    "            else:\n",
    "                sub_cost = 1\n",
    "            \n",
    "            costs = [\n",
    "                D[i-1, j-1] + sub_cost,  # substitute/match\n",
    "                D[i, j-1] + 1,            # insert\n",
    "                D[i-1, j] + 1             # delete\n",
    "            ]\n",
    "            \n",
    "            min_cost = min(costs)\n",
    "            D[i, j] = min_cost\n",
    "            \n",
    "            # Store backpointer\n",
    "            if min_cost == costs[0]:\n",
    "                if sub_cost == 0:\n",
    "                    backpointer[i, j] = 0  # match\n",
    "                else:\n",
    "                    backpointer[i, j] = 1  # substitute\n",
    "            elif min_cost == costs[1]:\n",
    "                backpointer[i, j] = 2  # insert\n",
    "            else:\n",
    "                backpointer[i, j] = 3  # delete\n",
    "    \n",
    "    # Backtrace\n",
    "    operations = []\n",
    "    i, j = m, n\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        if i == 0:\n",
    "            operations.append(('INSERT', target[j-1]))\n",
    "            j -= 1\n",
    "        elif j == 0:\n",
    "            operations.append(('DELETE', source[i-1]))\n",
    "            i -= 1\n",
    "        else:\n",
    "            bp = backpointer[i, j]\n",
    "            if bp == 0:  # match\n",
    "                operations.append(('MATCH', source[i-1]))\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif bp == 1:  # substitute\n",
    "                operations.append(('SUBSTITUTE', f\"{source[i-1]}->{target[j-1]}\"))\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif bp == 2:  # insert\n",
    "                operations.append(('INSERT', target[j-1]))\n",
    "                j -= 1\n",
    "            else:  # delete\n",
    "                operations.append(('DELETE', source[i-1]))\n",
    "                i -= 1\n",
    "    \n",
    "    operations.reverse()\n",
    "    return D[m, n], operations\n",
    "\n",
    "source, target = \"intention\", \"execution\"\n",
    "dist, ops = min_edit_distance_with_backtrace(source, target)\n",
    "print(f\"\\nEdit Distance: '{source}' -> '{target}' = {dist}\")\n",
    "print(\"Operations:\")\n",
    "for op, char in ops:\n",
    "    if op != 'MATCH':\n",
    "        print(f\"  {op}: {char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits_one(word):\n",
    "    \"\"\"\n",
    "    Generate all words with edit distance = 1 with word.\n",
    "    Includes: deletes, transposes, replaces, inserts\n",
    "    \"\"\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    # Create all possible splits of word\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    # 1. Deletes: delete 1 character\n",
    "    # \"abc\" → \"bc\", \"ac\", \"ab\"\n",
    "    deletes = [left + right[1:] for left, right in splits if right]\n",
    "    \n",
    "    # 2. Transposes: swap 2 adjacent characters\n",
    "    # \"abc\" → \"bac\", \"acb\"\n",
    "    transposes = [left + right[1] + right[0] + right[2:] \n",
    "                  for left, right in splits if len(right) > 1]\n",
    "    \n",
    "    # 3. Replaces: replace 1 character\n",
    "    # \"abc\" → \"xbc\", \"axc\", \"abx\" (for each letter)\n",
    "    replaces = [left + c + right[1:] \n",
    "                for left, right in splits if right \n",
    "                for c in letters]\n",
    "    \n",
    "    # 4. Inserts: insert 1 character\n",
    "    # \"abc\" → \"xabc\", \"axbc\", \"abxc\", \"abcx\" (for each letter)\n",
    "    inserts = [left + c + right \n",
    "               for left, right in splits \n",
    "               for c in letters]\n",
    "    \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits_two(word):\n",
    "    \"\"\"\n",
    "    Generate all words with edit distance = 2 with word.\n",
    "    By applying edits_one 2 times.\n",
    "    \"\"\"\n",
    "    return set(e2 for e1 in edits_one(word) for e2 in edits_one(e1))\n",
    "\n",
    "test_word = \"cat\"\n",
    "e1 = edits_one(test_word)\n",
    "e2 = edits_two(test_word)\n",
    "\n",
    "print(f\"Word: '{test_word}'\")\n",
    "print(f\"Edits distance 1: {len(e1)} words\")\n",
    "print(f\"Edits distance 2: {len(e2)} words\")\n",
    "print(f\"\\nSample edits_one: {list(e1)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Autocorrect Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autocorrect:\n",
    "    \"\"\"\n",
    "    Autocorrect system using Edit Distance and Noisy Channel Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, word_freq):\n",
    "        \"\"\"\n",
    "        Initialize with vocabulary and word frequencies.\n",
    "        \n",
    "        Args:\n",
    "            vocab: set of valid words\n",
    "            word_freq: Counter object với word frequencies\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.word_freq = word_freq\n",
    "        self.total_words = sum(word_freq.values())\n",
    "    \n",
    "    def _edits_one(self, word):\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        \n",
    "        deletes = [left + right[1:] for left, right in splits if right]\n",
    "        transposes = [left + right[1] + right[0] + right[2:] \n",
    "                      for left, right in splits if len(right) > 1]\n",
    "        replaces = [left + c + right[1:] \n",
    "                    for left, right in splits if right \n",
    "                    for c in letters]\n",
    "        inserts = [left + c + right \n",
    "                   for left, right in splits \n",
    "                   for c in letters]\n",
    "        \n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    def _edits_two(self, word):\n",
    "        return set(e2 for e1 in self._edits_one(word) for e2 in self._edits_one(e1))\n",
    "    \n",
    "    def _known(self, words):\n",
    "        \"\"\"Filter words that are in vocabulary.\"\"\"\n",
    "        return set(w for w in words if w in self.vocab)\n",
    "    \n",
    "    def get_candidates(self, word):\n",
    "        \"\"\"\n",
    "        Generate list of candidates in order of priority:\n",
    "        \"\"\"\n",
    "        if word in self.vocab:\n",
    "            return {word}\n",
    "        \n",
    "        candidates = self._known(self._edits_one(word))\n",
    "        if candidates:\n",
    "            return candidates\n",
    "        \n",
    "        candidates = self._known(self._edits_two(word))\n",
    "        if candidates:\n",
    "            return candidates\n",
    "        \n",
    "        return {word}\n",
    "    \n",
    "    def word_probability(self, word):\n",
    "        \"\"\"\n",
    "        Calculate P(word) = Language Model probability.\n",
    "        Use unigram model with Laplace smoothing.\n",
    "        \"\"\"\n",
    "        # P(word) = (count(word) + 1) / (total_words + vocab_size)\n",
    "        return (self.word_freq.get(word, 0) + 1) / (self.total_words + len(self.vocab))\n",
    "    \n",
    "    def correct(self, word):\n",
    "        \"\"\"\n",
    "        Return the word with the highest probability.\n",
    "        Use Noisy Channel Model: argmax P(c) * P(w|c)\n",
    "        But we just use P(c)\n",
    "        \"\"\"\n",
    "        candidates = self.get_candidates(word.lower())\n",
    "        \n",
    "        # Select candidate with the highest probability\n",
    "        return max(candidates, key=self.word_probability)\n",
    "    \n",
    "    def correct_with_scores(self, word):\n",
    "        \"\"\"\n",
    "        Return top candidates with scores.\n",
    "        \"\"\"\n",
    "        candidates = self.get_candidates(word.lower())\n",
    "        \n",
    "        scored = [(c, self.word_probability(c)) for c in candidates]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return scored[:5]  # Top 5\n",
    "\n",
    "autocorrector = Autocorrect(vocab, word_freq)\n",
    "\n",
    "test_words = [\n",
    "    \"speling\",     # spelling\n",
    "    \"correc\",      # correct\n",
    "    \"naturla\",     # natural\n",
    "    \"languge\",     # language\n",
    "    \"computr\",     # computer\n",
    "    \"helo\",        # hello\n",
    "    \"teh\",         # the\n",
    "]\n",
    "\n",
    "print(\"Autocorrect Results:\")\n",
    "for word in test_words:\n",
    "    correction = autocorrector.correct(word)\n",
    "    print(f\"  '{word}' -> '{correction}'\")\n",
    "\n",
    "print(\"Detailed scores for 'speling':\")\n",
    "for word, score in autocorrector.correct_with_scores(\"speling\"):\n",
    "    print(f\"  {word}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    (\"teh\", \"the\"),\n",
    "    (\"woudl\", \"would\"),\n",
    "    (\"peolpe\", \"people\"),\n",
    "    (\"becuase\", \"because\"),\n",
    "    (\"recieve\", \"receive\"),\n",
    "    (\"occured\", \"occurred\"),\n",
    "    (\"seperate\", \"separate\"),\n",
    "    (\"definately\", \"definitely\"),\n",
    "    (\"goverment\", \"government\"),\n",
    "    (\"enviroment\", \"environment\"),\n",
    "]\n",
    "\n",
    "correct_count = 0\n",
    "print(\"Evaluation Results:\")\n",
    "\n",
    "for misspelled, correct in test_set:\n",
    "    prediction = autocorrector.correct(misspelled)\n",
    "    is_correct = prediction == correct\n",
    "    correct_count += is_correct\n",
    "    \n",
    "    status = \"CORRECT\" if is_correct else \"INCORRECT\"\n",
    "    print(f\"  {status} '{misspelled}' -> '{prediction}' (expected: '{correct}')\")\n",
    "\n",
    "accuracy = correct_count / len(test_set) * 100\n",
    "print(f\"Accuracy: {correct_count}/{len(test_set)} = {accuracy:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
