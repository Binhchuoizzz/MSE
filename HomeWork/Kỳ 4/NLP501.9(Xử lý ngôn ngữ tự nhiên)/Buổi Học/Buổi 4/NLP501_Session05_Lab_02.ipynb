{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrect System & HMM POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/dongnd/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/dongnd/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/dongnd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: HMM POS TAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load và Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Brown corpus...\n",
      "Total sentences: 57,340\n",
      "Training sentences: 45,872\n",
      "Test sentences: 11,468\n",
      "\n",
      "Sample sentence:\n",
      "  the             → DET\n",
      "  fulton          → NOUN\n",
      "  county          → NOUN\n",
      "  grand           → ADJ\n",
      "  jury            → NOUN\n",
      "  said            → VERB\n",
      "  friday          → NOUN\n",
      "  an              → DET\n",
      "  investigation   → NOUN\n",
      "  of              → ADP\n"
     ]
    }
   ],
   "source": [
    "def load_brown_corpus(tagset='universal'):\n",
    "    \"\"\"\n",
    "    Load Brown corpus with POS tags.\n",
    "    \n",
    "    Args:\n",
    "        tagset: 'universal' (17 tags) or None (Penn Treebank)\n",
    "    \n",
    "    Returns:\n",
    "        tagged_sents: list of sentences, each sentence is a list of (word, tag) tuples\n",
    "    \"\"\"\n",
    "    tagged_sents = brown.tagged_sents(tagset=tagset)\n",
    "    \n",
    "    processed = []\n",
    "    for sent in tagged_sents:\n",
    "        processed_sent = [(word.lower(), tag) for word, tag in sent]\n",
    "        processed.append(processed_sent)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def split_data(data, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets.\n",
    "    \"\"\"\n",
    "    split_idx = int(len(data) * train_ratio)\n",
    "    return data[:split_idx], data[split_idx:]\n",
    "\n",
    "print(\"Loading Brown corpus...\")\n",
    "tagged_sentences = load_brown_corpus(tagset='universal')\n",
    "train_data, test_data = split_data(tagged_sentences)\n",
    "\n",
    "print(f\"Total sentences: {len(tagged_sentences):,}\")\n",
    "print(f\"Training sentences: {len(train_data):,}\")\n",
    "print(f\"Test sentences: {len(test_data):,}\")\n",
    "\n",
    "print(\"\\nSample sentence:\")\n",
    "sample = train_data[0][:10]\n",
    "for word, tag in sample:\n",
    "    print(f\"  {word:15} → {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Extract Tags and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45,755\n",
      "Number of tags: 12\n",
      "\n",
      "Tags: ['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "def extract_vocab_and_tags(tagged_sents):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and set of tags from tagged corpus.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    tags = set()\n",
    "    \n",
    "    for sent in tagged_sents:\n",
    "        for word, tag in sent:\n",
    "            words.add(word)\n",
    "            tags.add(tag)\n",
    "    \n",
    "    return words, tags\n",
    "\n",
    "word_vocab, tag_set = extract_vocab_and_tags(train_data)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_vocab):,}\")\n",
    "print(f\"Number of tags: {len(tag_set)}\")\n",
    "print(f\"\\nTags: {sorted(tag_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. HMM Tagger Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMTagger:\n",
    "    \"\"\"\n",
    "    Hidden Markov Model POS Tagger.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing_k=1.0):\n",
    "        \"\"\"\n",
    "        Initialize HMM Tagger.\n",
    "        \n",
    "        Args:\n",
    "            smoothing_k: smoothing coefficient\n",
    "        \"\"\"\n",
    "        self.smoothing_k = smoothing_k\n",
    "        \n",
    "        self.tag_counts = Counter()           # C(tag)\n",
    "        self.transition_counts = defaultdict(Counter)  # C(tag_i, tag_j)\n",
    "        self.emission_counts = defaultdict(Counter)    # C(tag, word)\n",
    "        self.initial_counts = Counter()       # C(tag tại vị trí đầu)\n",
    "        \n",
    "        # Vocabulary and tag set\n",
    "        self.vocab = set()\n",
    "        self.tags = set()\n",
    "        \n",
    "        # Probabilities (will be calculated after training)\n",
    "        self.transition_probs = {}  # A matrix\n",
    "        self.emission_probs = {}    # B matrix\n",
    "        self.initial_probs = {}     # π vector\n",
    "    \n",
    "    def train(self, tagged_sents):\n",
    "        \"\"\"\n",
    "        Train HMM from tagged corpus.\n",
    "        \n",
    "        Args:\n",
    "            tagged_sents: list of sentences, mỗi sentence là list of (word, tag)\n",
    "        \"\"\"\n",
    "        # Step 1: Count events\n",
    "        for sent in tagged_sents:\n",
    "            prev_tag = '<START>'  # Special start tag\n",
    "            \n",
    "            for i, (word, tag) in enumerate(sent):\n",
    "                # Update tag count\n",
    "                self.tag_counts[tag] += 1\n",
    "                self.tags.add(tag)\n",
    "                self.vocab.add(word)\n",
    "                \n",
    "                # Update emission count: C(tag, word)\n",
    "                self.emission_counts[tag][word] += 1\n",
    "                \n",
    "                # Update transition count: C(prev_tag, tag)\n",
    "                self.transition_counts[prev_tag][tag] += 1\n",
    "                \n",
    "                # Update initial count (only for the first tag)\n",
    "                if i == 0:\n",
    "                    self.initial_counts[tag] += 1\n",
    "                \n",
    "                prev_tag = tag\n",
    "        \n",
    "        # Add <START> to tags for transition\n",
    "        self.tags.add('<START>')\n",
    "        \n",
    "        # Step 2: Calculate probabilities with smoothing\n",
    "        self._compute_probabilities()\n",
    "        \n",
    "        print(f\"Training completed!\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocab):,}\")\n",
    "        print(f\"  Number of tags: {len(self.tags) - 1}\")  # Exclude <START>\n",
    "    \n",
    "    def _compute_probabilities(self):\n",
    "        \"\"\"\n",
    "        Calculate transition, emission, and initial probabilities with Laplace smoothing.\n",
    "        \"\"\"\n",
    "        k = self.smoothing_k\n",
    "        num_tags = len(self.tags)\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Transition probabilities: P(tag_j | tag_i)\n",
    "        # P(j|i) = (C(i,j) + k) / (C(i) + k * num_tags)\n",
    "        for tag_i in self.tags:\n",
    "            total_i = sum(self.transition_counts[tag_i].values())\n",
    "            self.transition_probs[tag_i] = {}\n",
    "            \n",
    "            for tag_j in self.tags:\n",
    "                if tag_j == '<START>':\n",
    "                    continue\n",
    "                count = self.transition_counts[tag_i].get(tag_j, 0)\n",
    "                self.transition_probs[tag_i][tag_j] = (count + k) / (total_i + k * num_tags)\n",
    "        \n",
    "        # Emission probabilities: P(word | tag)\n",
    "        # P(w|t) = (C(t,w) + k) / (C(t) + k * vocab_size)\n",
    "        for tag in self.tags:\n",
    "            if tag == '<START>':\n",
    "                continue\n",
    "            total_tag = self.tag_counts[tag]\n",
    "            self.emission_probs[tag] = {}\n",
    "            \n",
    "            # Only save words with count > 0 to save memory\n",
    "            for word in self.emission_counts[tag]:\n",
    "                count = self.emission_counts[tag][word]\n",
    "                self.emission_probs[tag][word] = (count + k) / (total_tag + k * vocab_size)\n",
    "            \n",
    "            # Save default probability for unknown words\n",
    "            self.emission_probs[tag]['<UNK>'] = k / (total_tag + k * vocab_size)\n",
    "        \n",
    "        # Initial probabilities: P(tag at the first position)\n",
    "        total_sents = sum(self.initial_counts.values())\n",
    "        for tag in self.tags:\n",
    "            if tag == '<START>':\n",
    "                continue\n",
    "            count = self.initial_counts.get(tag, 0)\n",
    "            self.initial_probs[tag] = (count + k) / (total_sents + k * num_tags)\n",
    "    \n",
    "    def get_transition_prob(self, tag_i, tag_j):\n",
    "        \"\"\"Get P(tag_j | tag_i).\"\"\"\n",
    "        if tag_i in self.transition_probs and tag_j in self.transition_probs[tag_i]:\n",
    "            return self.transition_probs[tag_i][tag_j]\n",
    "        # Smoothed probability for unknown transition\n",
    "        return self.smoothing_k / (self.tag_counts.get(tag_i, 0) + self.smoothing_k * len(self.tags))\n",
    "    \n",
    "    def get_emission_prob(self, tag, word):\n",
    "        \"\"\"Get P(word | tag).\"\"\"\n",
    "        if tag in self.emission_probs:\n",
    "            if word in self.emission_probs[tag]:\n",
    "                return self.emission_probs[tag][word]\n",
    "            return self.emission_probs[tag]['<UNK>']\n",
    "        return self.smoothing_k / (self.tag_counts.get(tag, 0) + self.smoothing_k * len(self.vocab))\n",
    "    \n",
    "    def get_initial_prob(self, tag):\n",
    "        \"\"\"Get P(tag tại vị trí đầu).\"\"\"\n",
    "        return self.initial_probs.get(tag, self.smoothing_k / sum(self.initial_counts.values()))\n",
    "    \n",
    "    def viterbi(self, sentence):\n",
    "        \"\"\"\n",
    "        Viterbi algorithm to find the optimal sequence of tags.\n",
    "        \n",
    "        Args:\n",
    "            sentence: list of words\n",
    "        \n",
    "        Returns:\n",
    "            best_tags: list of predicted tags\n",
    "        \"\"\"\n",
    "        if not sentence:\n",
    "            return []\n",
    "        \n",
    "        # Tags to consider (exclude <START>)\n",
    "        tags = [t for t in self.tags if t != '<START>']\n",
    "        n_tags = len(tags)\n",
    "        n_words = len(sentence)\n",
    "        \n",
    "        # Viterbi matrix: v[t][j] = best log prob to reach tag j at time t\n",
    "        # Use log probabilities to avoid underflow\n",
    "        v = np.full((n_words, n_tags), -np.inf)\n",
    "        \n",
    "        # Backpointer matrix\n",
    "        backpointer = np.zeros((n_words, n_tags), dtype=int)\n",
    "        \n",
    "        # Tag to index mapping\n",
    "        tag2idx = {tag: i for i, tag in enumerate(tags)}\n",
    "        idx2tag = {i: tag for i, tag in enumerate(tags)}\n",
    "        \n",
    "        # Step 1: Initialization (t = 0)\n",
    "        word = sentence[0].lower()\n",
    "        for j, tag in enumerate(tags):\n",
    "            # v[0][j] = log(π[j]) + log(b[j][word])\n",
    "            init_prob = self.get_initial_prob(tag)\n",
    "            emit_prob = self.get_emission_prob(tag, word)\n",
    "            \n",
    "            v[0, j] = np.log(init_prob + 1e-10) + np.log(emit_prob + 1e-10)\n",
    "        \n",
    "        # Step 2: Recursion (t = 1...T-1)\n",
    "        for t in range(1, n_words):\n",
    "            word = sentence[t].lower()\n",
    "            \n",
    "            for j, tag_j in enumerate(tags):\n",
    "                emit_prob = self.get_emission_prob(tag_j, word)\n",
    "                log_emit = np.log(emit_prob + 1e-10)\n",
    "                \n",
    "                # Find the best state\n",
    "                best_score = -np.inf\n",
    "                best_prev = 0\n",
    "                \n",
    "                for i, tag_i in enumerate(tags):\n",
    "                    trans_prob = self.get_transition_prob(tag_i, tag_j)\n",
    "                    score = v[t-1, i] + np.log(trans_prob + 1e-10)\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev = i\n",
    "                \n",
    "                v[t, j] = best_score + log_emit\n",
    "                backpointer[t, j] = best_prev\n",
    "        \n",
    "        # Step 3: Termination\n",
    "        best_last_tag_idx = np.argmax(v[n_words - 1])\n",
    "        \n",
    "        # Step 4: Backtrace\n",
    "        best_tags = [idx2tag[best_last_tag_idx]]\n",
    "        \n",
    "        for t in range(n_words - 1, 0, -1):\n",
    "            best_last_tag_idx = backpointer[t, best_last_tag_idx]\n",
    "            best_tags.append(idx2tag[best_last_tag_idx])\n",
    "        \n",
    "        best_tags.reverse()\n",
    "        return best_tags\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        \"\"\"\n",
    "        Tag a sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: list of words hoặc string\n",
    "        \n",
    "        Returns:\n",
    "            list of (word, tag) tuples\n",
    "        \"\"\"\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "        \n",
    "        tags = self.viterbi(sentence)\n",
    "        return list(zip(sentence, tags))\n",
    "    \n",
    "    def evaluate(self, test_sents):\n",
    "        \"\"\"\n",
    "        Evaluate accuracy on test set.\n",
    "        \n",
    "        Args:\n",
    "            test_sents: list of tagged sentences\n",
    "        \n",
    "        Returns:\n",
    "            accuracy: float\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for sent in test_sents:\n",
    "            words = [w for w, t in sent]\n",
    "            gold_tags = [t for w, t in sent]\n",
    "            pred_tags = self.viterbi(words)\n",
    "            \n",
    "            for gold, pred in zip(gold_tags, pred_tags):\n",
    "                if gold == pred:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Train và Evaluate HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "  Vocabulary size: 45,755\n",
      "  Number of tags: 12\n"
     ]
    }
   ],
   "source": [
    "hmm_tagger = HMMTagger(smoothing_k=1.0)\n",
    "hmm_tagger.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tagging Results:\n",
      "\n",
      "Sentence: The cat sat on the mat\n",
      "Tags:\n",
      "  The             -> DET\n",
      "  cat             -> NOUN\n",
      "  sat             -> VERB\n",
      "  on              -> ADP\n",
      "  the             -> DET\n",
      "  mat             -> NOUN\n",
      "\n",
      "Sentence: I want to learn natural language processing\n",
      "Tags:\n",
      "  I               -> PRON\n",
      "  want            -> VERB\n",
      "  to              -> PRT\n",
      "  learn           -> VERB\n",
      "  natural         -> ADJ\n",
      "  language        -> NOUN\n",
      "  processing      -> VERB\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog\n",
      "Tags:\n",
      "  The             -> DET\n",
      "  quick           -> ADJ\n",
      "  brown           -> NOUN\n",
      "  fox             -> NOUN\n",
      "  jumps           -> VERB\n",
      "  over            -> ADP\n",
      "  the             -> DET\n",
      "  lazy            -> ADJ\n",
      "  dog             -> NOUN\n"
     ]
    }
   ],
   "source": [
    "# Test trên một vài câu\n",
    "test_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"I want to learn natural language processing\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "]\n",
    "\n",
    "print(\"Sample Tagging Results:\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    tagged = hmm_tagger.tag(sent)\n",
    "    print(f\"\\nSentence: {sent}\")\n",
    "    print(\"Tags:\")\n",
    "    for word, tag in tagged:\n",
    "        print(f\"  {word:15} -> {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "\n",
      "Accuracy on test subset (1000 sentences): 93.17%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "test_subset = test_data[:1000]\n",
    "accuracy = hmm_tagger.evaluate(test_subset)\n",
    "\n",
    "print(f\"\\nAccuracy on test subset ({len(test_subset)} sentences): {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Analyzing Transition và Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  P(NOUN | DET) = 0.6190\n",
      "  P(ADJ | DET) = 0.2453\n",
      "  P(NOUN | ADJ) = 0.6644\n",
      "  P(VERB | NOUN) = 0.1561\n",
      "  P(DET | VERB) = 0.1636\n",
      "  P(VERB | PRON) = 0.7170\n"
     ]
    }
   ],
   "source": [
    "interesting_transitions = [\n",
    "    ('DET', 'NOUN'),    # the cat\n",
    "    ('DET', 'ADJ'),     # the big\n",
    "    ('ADJ', 'NOUN'),    # big cat\n",
    "    ('NOUN', 'VERB'),   # cat runs\n",
    "    ('VERB', 'DET'),    # runs the\n",
    "    ('PRON', 'VERB'),   # I run\n",
    "]\n",
    "\n",
    "for tag_i, tag_j in interesting_transitions:\n",
    "    prob = hmm_tagger.get_transition_prob(tag_i, tag_j)\n",
    "    print(f\"  P({tag_j} | {tag_i}) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 words for each tag:\n",
      "  .     : ,(48491), .(39534), ``(6160), ''(6119), ;(4884)\n",
      "  ADJ   : new(1528), other(1507), first(923), many(921), such(913)\n",
      "  ADP   : of(32940), in(18607), to(9686), for(8380), with(6153)\n",
      "  ADV   : not(3933), when(1879), so(1222), only(1154), more(1077)\n",
      "  CONJ  : and(24278), or(3798), but(3311), either(175), nor(169)\n",
      "  DET   : the(61188), a(19375), his(5215), this(4567), an(3285)\n",
      "  NOUN  : time(1286), af(994), man(879), years(843), state(772)\n",
      "  NUM   : one(2374), two(1224), three(534), 1(525), 2(443)\n",
      "  PRON  : it(6949), he(6685), i(3107), they(2899), we(2291)\n",
      "  PRT   : to(12732), all(2277), there(1827), up(1224), out(1189)\n",
      "  VERB  : is(9670), was(7334), be(5724), are(4188), had(3511)\n",
      "  X     : de(63), la(34), et(25), comedie(14), quo(11)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 5 words for each tag:\")\n",
    "\n",
    "for tag in sorted(hmm_tagger.tags):\n",
    "    if tag == '<START>':\n",
    "        continue\n",
    "    \n",
    "    # Get top words\n",
    "    tag_words = hmm_tagger.emission_counts[tag].most_common(5)\n",
    "    words_str = \", \".join([f\"{w}({c})\" for w, c in tag_words])\n",
    "    print(f\"  {tag:6}: {words_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors found: 499\n",
      "\n",
      "Sample Errors:\n",
      "  Word: 'because'\n",
      "  Gold: ADP | Predicted: ADV\n",
      "  Context: ...important , because of the...\n",
      "\n",
      "  Word: 'beech'\n",
      "  Gold: NOUN | Predicted: ADJ\n",
      "  Context: ...of the beech pasture ....\n",
      "\n",
      "  Word: 'that'\n",
      "  Gold: DET | Predicted: ADP\n",
      "  Context: ...what's that ? ?...\n",
      "\n",
      "  Word: 'as-it-were'\n",
      "  Gold: ADV | Predicted: VERB\n",
      "  Context: ...gave me as-it-were the spirit...\n",
      "\n",
      "  Word: 'demoniac'\n",
      "  Gold: ADJ | Predicted: NOUN\n",
      "  Context: ..., the demoniac , evil...\n",
      "\n",
      "  Word: 'whole'\n",
      "  Gold: NOUN | Predicted: ADJ\n",
      "  Context: ...of this whole affair ....\n",
      "\n",
      "  Word: 'registrar'\n",
      "  Gold: NOUN | Predicted: ADV\n",
      "  Context: ...am also registrar ....\n",
      "\n",
      "  Word: 'blows'\n",
      "  Gold: VERB | Predicted: NOUN\n",
      "  Context: ...live wind blows , and...\n",
      "\n",
      "  Word: 'water-line'\n",
      "  Gold: NOUN | Predicted: ADJ\n",
      "  Context: ...below the water-line interests me...\n",
      "\n",
      "  Word: 'interests'\n",
      "  Gold: VERB | Predicted: NOUN\n",
      "  Context: ...the water-line interests me also...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_errors(tagger, test_sents, n_samples=5):\n",
    "    \"\"\"\n",
    "    Analyze tagging errors.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for sent in test_sents[:500]:  # Check first 500 sentences\n",
    "        words = [w for w, t in sent]\n",
    "        gold_tags = [t for w, t in sent]\n",
    "        pred_tags = tagger.viterbi(words)\n",
    "        \n",
    "        for i, (word, gold, pred) in enumerate(zip(words, gold_tags, pred_tags)):\n",
    "            if gold != pred:\n",
    "                # Get context\n",
    "                context = \" \".join(words[max(0,i-2):i+3])\n",
    "                errors.append({\n",
    "                    'word': word,\n",
    "                    'gold': gold,\n",
    "                    'pred': pred,\n",
    "                    'context': context\n",
    "                })\n",
    "    \n",
    "    return errors\n",
    "\n",
    "errors = analyze_errors(hmm_tagger, test_data)\n",
    "\n",
    "print(f\"Total errors found: {len(errors)}\")\n",
    "print(\"\\nSample Errors:\")\n",
    "\n",
    "for err in errors[:10]:\n",
    "    print(f\"  Word: '{err['word']}'\")\n",
    "    print(f\"  Gold: {err['gold']} | Predicted: {err['pred']}\")\n",
    "    print(f\"  Context: ...{err['context']}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common confusions:\n",
      "  VERB -> NOUN: 42 times\n",
      "  PRT -> ADP: 38 times\n",
      "  NOUN -> VERB: 32 times\n",
      "  NOUN -> PRON: 31 times\n",
      "  ADJ -> ADV: 30 times\n",
      "  NOUN -> ADJ: 29 times\n",
      "  ADJ -> NOUN: 22 times\n",
      "  NOUN -> DET: 20 times\n",
      "  ADV -> ADJ: 19 times\n",
      "  ADP -> ADV: 17 times\n"
     ]
    }
   ],
   "source": [
    "# Confusion analysis\n",
    "from collections import defaultdict\n",
    "\n",
    "confusion = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for err in errors:\n",
    "    confusion[err['gold']][err['pred']] += 1\n",
    "\n",
    "print(\"Most common confusions:\")\n",
    "\n",
    "confusion_list = []\n",
    "for gold in confusion:\n",
    "    for pred, count in confusion[gold].items():\n",
    "        confusion_list.append((gold, pred, count))\n",
    "\n",
    "confusion_list.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for gold, pred, count in confusion_list[:10]:\n",
    "    print(f\"  {gold} -> {pred}: {count} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
