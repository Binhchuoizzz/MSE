{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 05-a: POS Tagging with NLTK and spaCy\n",
        "\n",
        "- **NLTK**: Taggers: PerceptronTagger, Brill Tagger, etc.\n",
        "- **spaCy**: Industrial-strength NLP framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup và Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import PerceptronTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm # for small model\n",
        "# Run: python -m spacy download en_core_web_md # for medium model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. NLTK POS Taggers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Default POS Tagger (Perceptron-based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NLTK's default tagger (PerceptronTagger)\n",
        "test_sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"I want to learn natural language processing\",\n",
        "    \"Python is a popular programming language\",\n",
        "    \"She sells seashells by the seashore\",\n",
        "    \"The cat sat on the mat\"\n",
        "]\n",
        "\n",
        "print(\"NLTK Default POS Tagger (Penn Treebank Tags)\")\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    print(f\"\\n'{sentence}'\")\n",
        "    print(\"  Tags:\", end=\" \")\n",
        "    for word, tag in pos_tags:\n",
        "        print(f\"{word}/{tag}\", end=\" \")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Universal tagset (simplified)\n",
        "print(\"NLTK POS Tagger (Universal Tagset)\")\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens, tagset='universal')\n",
        "    \n",
        "    print(f\"\\n'{sentence}'\")\n",
        "    print(\"  Tags:\", end=\" \")\n",
        "    for word, tag in pos_tags:\n",
        "        print(f\"{word}/{tag}\", end=\" \")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Penn Treebank vs Universal Tagset\n",
        "\n",
        "**Penn Treebank**: 36 tags (very detail)\n",
        "- NN, NNS, NNP, NNPS (noun variations)\n",
        "- VB, VBD, VBG, VBN, VBP, VBZ (verb variations)\n",
        "\n",
        "**Universal**: 12 tags (much more simplified)\n",
        "- NOUN, VERB, ADJ, ADV, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tag comparison\n",
        "sample_text = \"The cats are running quickly through the beautiful garden\"\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "penn_tags = pos_tag(tokens)\n",
        "universal_tags = pos_tag(tokens, tagset='universal')\n",
        "\n",
        "print(\"\\nTagset Comparison:\")\n",
        "print(f\"{'Word':<15} {'Penn Treebank':<15} {'Universal':<15}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for (w1, pt), (w2, ut) in zip(penn_tags, universal_tags):\n",
        "    print(f\"{w1:<15} {pt:<15} {ut:<15}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Training Custom NLTK Taggers\n",
        "\n",
        "NLTK allows to train custom taggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Brown corpus for training\n",
        "print(\"Loading Brown corpus...\")\n",
        "brown_tagged = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "# Split data\n",
        "split_idx = int(len(brown_tagged) * 0.8)\n",
        "train_sents = brown_tagged[:split_idx]\n",
        "test_sents = brown_tagged[split_idx:split_idx + 1000]  # Use 1000 for testing\n",
        "\n",
        "print(f\"Training sentences: {len(train_sents):,}\")\n",
        "print(f\"Test sentences: {len(test_sents):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Unigram Tagger\n",
        "print(\"\\n[1/3] Training Unigram Tagger...\")\n",
        "unigram_tagger = UnigramTagger(train_sents)\n",
        "unigram_accuracy = unigram_tagger.accuracy(test_sents)\n",
        "print(f\"  Accuracy: {unigram_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Bigram Tagger with Unigram backoff\n",
        "print(\"\\n[2/3] Training Bigram Tagger (with Unigram backoff)...\")\n",
        "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
        "bigram_accuracy = bigram_tagger.accuracy(test_sents)\n",
        "print(f\"  Accuracy: {bigram_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Trigram Tagger with backoff chain\n",
        "print(\"\\n[3/3] Training Trigram Tagger (with Bigram & Unigram backoff)...\")\n",
        "trigram_tagger = TrigramTagger(train_sents, backoff=bigram_tagger)\n",
        "trigram_accuracy = trigram_tagger.accuracy(test_sents)\n",
        "print(f\"  Accuracy: {trigram_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare taggers on sample sentence\n",
        "sample = \"The quick brown fox jumps over the lazy dog\".split()\n",
        "\n",
        "print(\"Comparison on sample sentence:\")\n",
        "print(f\"Sentence: {' '.join(sample)}\")\n",
        "\n",
        "print(\"Unigram:\", unigram_tagger.tag(sample))\n",
        "print(\"Bigram: \", bigram_tagger.tag(sample))\n",
        "print(\"Trigram:\", trigram_tagger.tag(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. spaCy POS Tagging\n",
        "\n",
        "spaCy is an industrial-strength NLP framework:\n",
        "- Pre-trained models\n",
        "- NER, dependency parsing, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Load spaCy Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load spaCy model\n",
        "# Small model (11MB)\n",
        "print(\"Loading spaCy model (en_core_web_sm)...\")\n",
        "try:\n",
        "    nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"  Small model loaded\")\n",
        "except:\n",
        "    print(\"  Small model not found. Run: python -m spacy download en_core_web_sm\")\n",
        "    nlp_sm = None\n",
        "\n",
        "# Medium model (40MB) - better accuracy\n",
        "try:\n",
        "    nlp_md = spacy.load(\"en_core_web_md\")\n",
        "    print(\"  Medium model loaded\")\n",
        "except:\n",
        "    print(\"  Medium model not found. Run: python -m spacy download en_core_web_md\")\n",
        "    nlp_md = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Basic POS Tagging with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if nlp_sm:\n",
        "    print(\"spaCy POS Tagging\")\n",
        "    \n",
        "    for sentence in test_sentences:\n",
        "        doc = nlp_sm(sentence)\n",
        "        \n",
        "        print(f\"\\n'{sentence}'\")\n",
        "        print(f\"  {'Word':<15} {'POS':<10} {'Tag':<10} {'Dep':<10}\")\n",
        "        \n",
        "        for token in doc:\n",
        "            print(f\"  {token.text:<15} {token.pos_:<10} {token.tag_:<10} {token.dep_:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. spaCy Tag Explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if nlp_sm:\n",
        "    # Explain tags\n",
        "    sample_text = \"The cats were running quickly through the beautiful garden\"\n",
        "    doc = nlp_sm(sample_text)\n",
        "    \n",
        "    print(\"\\nDetailed Tag Information:\")\n",
        "    print(f\"{'Word':<15} {'POS':<10} {'Tag':<10} {'Explanation':<30}\")\n",
        "    \n",
        "    for token in doc:\n",
        "        # Get explanation\n",
        "        explanation = spacy.explain(token.tag_)\n",
        "        print(f\"{token.text:<15} {token.pos_:<10} {token.tag_:<10} {explanation if explanation else 'N/A':<30}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4. Advanced spaCy Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if nlp_sm:\n",
        "    # Morphological features\n",
        "    text = \"He was running quickly, but she runs faster.\"\n",
        "    doc = nlp_sm(text)\n",
        "    \n",
        "    print(\"\\nMorphological Analysis:\")\n",
        "    print(f\"{'Word':<15} {'Lemma':<15} {'POS':<10} {'Morph Features':<30}\")\n",
        "    \n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {str(token.morph):<30}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comparison and Benchmarking\n",
        "\n",
        "- Accuracy\n",
        "- Speed\n",
        "- Ease of use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1. Speed Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare test data\n",
        "test_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Natural language processing is fascinating.\",\n",
        "    \"Machine learning models can understand human language.\",\n",
        "    \"Python is a versatile programming language.\",\n",
        "    \"Deep learning has revolutionized artificial intelligence.\"\n",
        "] * 100  # 500 sentences\n",
        "\n",
        "print(f\"Testing on {len(test_texts)} sentences...\\n\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "# NLTK Default\n",
        "print(\"[1/3] Testing NLTK default tagger...\")\n",
        "start = time.time()\n",
        "for text in test_texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    _ = pos_tag(tokens)\n",
        "nltk_time = time.time() - start\n",
        "results['NLTK Default'] = nltk_time\n",
        "print(f\"  Time: {nltk_time:.3f}s ({len(test_texts)/nltk_time:.1f} sent/s)\")\n",
        "\n",
        "# spaCy Small\n",
        "if nlp_sm:\n",
        "    print(\"\\n[2/3] Testing spaCy (small model)...\")\n",
        "    start = time.time()\n",
        "    for text in test_texts:\n",
        "        _ = nlp_sm(text)\n",
        "    spacy_sm_time = time.time() - start\n",
        "    results['spaCy Small'] = spacy_sm_time\n",
        "    print(f\"  Time: {spacy_sm_time:.3f}s ({len(test_texts)/spacy_sm_time:.1f} sent/s)\")\n",
        "\n",
        "# spaCy Medium\n",
        "if nlp_md:\n",
        "    print(\"\\n[3/3] Testing spaCy (medium model)...\")\n",
        "    start = time.time()\n",
        "    for text in test_texts:\n",
        "        _ = nlp_md(text)\n",
        "    spacy_md_time = time.time() - start\n",
        "    results['spaCy Medium'] = spacy_md_time\n",
        "    print(f\"  Time: {spacy_md_time:.3f}s ({len(test_texts)/spacy_md_time:.1f} sent/s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize speed comparison\n",
        "if results:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    models = list(results.keys())\n",
        "    times = list(results.values())\n",
        "    \n",
        "    plt.bar(models, times, color=['#3498db', '#2ecc71', '#f39c12'][:len(models)])\n",
        "    plt.ylabel('Time (seconds)', fontsize=12)\n",
        "    plt.title(f'Speed Comparison ({len(test_texts)} sentences)', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=15)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, (model, time_val) in enumerate(zip(models, times)):\n",
        "        plt.text(i, time_val + 0.1, f'{time_val:.2f}s', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Side-by-Side Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different taggers on the same sentences\n",
        "comparison_sentences = [\n",
        "    \"The stock market crashed yesterday.\",\n",
        "    \"She can run faster than anyone.\",\n",
        "    \"I will book a flight to Paris.\",\n",
        "    \"The book on the table is mine.\"\n",
        "]\n",
        "\n",
        "for sent in comparison_sentences:\n",
        "    print(f\"Sentence: {sent}\")\n",
        "    \n",
        "    tokens = word_tokenize(sent)\n",
        "    \n",
        "    # NLTK\n",
        "    nltk_tags = pos_tag(tokens, tagset='universal')\n",
        "    print(f\"\\nNLTK:    \", end=\"\")\n",
        "    for word, tag in nltk_tags:\n",
        "        print(f\"{word}/{tag}\", end=\" \")\n",
        "    \n",
        "    # Unigram\n",
        "    unigram_tags = unigram_tagger.tag(tokens)\n",
        "    print(f\"\\nUnigram: \", end=\"\")\n",
        "    for word, tag in unigram_tags:\n",
        "        tag = tag if tag else 'NOUN'\n",
        "        print(f\"{word}/{tag}\", end=\" \")\n",
        "    \n",
        "    # Trigram\n",
        "    trigram_tags = trigram_tagger.tag(tokens)\n",
        "    print(f\"\\nTrigram: \", end=\"\")\n",
        "    for word, tag in trigram_tags:\n",
        "        tag = tag if tag else 'NOUN'\n",
        "        print(f\"{word}/{tag}\", end=\" \")\n",
        "    \n",
        "    # spaCy\n",
        "    if nlp_sm:\n",
        "        doc = nlp_sm(sent)\n",
        "        print(f\"\\nspaCy:   \", end=\"\")\n",
        "        for token in doc:\n",
        "            print(f\"{token.text}/{token.pos_}\", end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1. Extract Noun Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract noun phrases using POS patterns\n",
        "def extract_noun_phrases_nltk(text):\n",
        "    \"\"\"\n",
        "    Extract noun phrases (ADJ* NOUN+) using NLTK.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens, tagset='universal')\n",
        "    \n",
        "    noun_phrases = []\n",
        "    current_phrase = []\n",
        "    \n",
        "    for word, tag in pos_tags:\n",
        "        if tag in ['ADJ', 'NOUN']:\n",
        "            current_phrase.append(word)\n",
        "        else:\n",
        "            if current_phrase and any(w for w, t in pos_tags if t == 'NOUN' and w in current_phrase):\n",
        "                noun_phrases.append(' '.join(current_phrase))\n",
        "            current_phrase = []\n",
        "    \n",
        "    if current_phrase:\n",
        "        noun_phrases.append(' '.join(current_phrase))\n",
        "    \n",
        "    return noun_phrases\n",
        "\n",
        "sample_text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. \n",
        "Natural language processing is a fascinating field of artificial intelligence.\n",
        "Modern machine learning algorithms can process large amounts of textual data.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Extracting noun phrases with NLTK:\")\n",
        "nltk_phrases = extract_noun_phrases_nltk(sample_text)\n",
        "for phrase in nltk_phrases:\n",
        "    if len(phrase.split()) > 1:  # Only multi-word phrases\n",
        "        print(f\"  - {phrase}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract noun phrases using spaCy\n",
        "if nlp_sm:\n",
        "    print(\"\\nExtracting noun phrases with spaCy:\")\n",
        "    doc = nlp_sm(sample_text)\n",
        "    \n",
        "    for chunk in doc.noun_chunks:\n",
        "        print(f\"  - {chunk.text} (root: {chunk.root.text}, dep: {chunk.root.dep_})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2. Extract Verbs and Their Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if nlp_sm:\n",
        "    # Extract verb-object pairs using dependency parsing\n",
        "    text = \"\"\"I love programming. She studies natural language processing. \n",
        "    They built a sophisticated machine learning model.\"\"\"\n",
        "    \n",
        "    doc = nlp_sm(text)\n",
        "    \n",
        "    print(\"Verb-Object pairs:\")\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"VERB\":\n",
        "            # Find direct objects\n",
        "            objects = [child for child in token.children if child.dep_ in ('dobj', 'obj')]\n",
        "            for obj in objects:\n",
        "                # Get the full noun phrase\n",
        "                obj_phrase = \" \".join([child.text for child in obj.subtree])\n",
        "                print(f\"  {token.text} → {obj_phrase}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3. Text Simplification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove adjectives and adverbs for text simplification\n",
        "def simplify_text(text, use_spacy=True):\n",
        "    \"\"\"\n",
        "    Remove adjectives and adverbs to simplify text.\n",
        "    \"\"\"\n",
        "    if use_spacy and nlp_sm:\n",
        "        doc = nlp_sm(text)\n",
        "        simplified = \" \".join([token.text for token in doc \n",
        "                              if token.pos_ not in ['ADJ', 'ADV']])\n",
        "    else:\n",
        "        tokens = word_tokenize(text)\n",
        "        pos_tags = pos_tag(tokens, tagset='universal')\n",
        "        simplified = \" \".join([word for word, tag in pos_tags \n",
        "                              if tag not in ['ADJ', 'ADV']])\n",
        "    \n",
        "    return simplified\n",
        "\n",
        "complex_text = \"The extremely talented young programmer quickly developed a highly sophisticated algorithm.\"\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(f\"  {complex_text}\")\n",
        "print(\"\\nSimplified (NLTK):\")\n",
        "print(f\"  {simplify_text(complex_text, use_spacy=False)}\")\n",
        "\n",
        "if nlp_sm:\n",
        "    print(\"\\nSimplified (spaCy):\")\n",
        "    print(f\"  {simplify_text(complex_text, use_spacy=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "### When to Use:\n",
        "\n",
        "**Use NLTK when:**\n",
        "- Learning NLP concepts\n",
        "- Need fine-grained control\n",
        "- Want to implement custom algorithms\n",
        "- Working with smaller datasets\n",
        "\n",
        "**Use spaCy when:**\n",
        "- Building production systems\n",
        "- Need high performance\n",
        "- Want additional features (NER, dependency parsing)\n",
        "- Working with large-scale data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
