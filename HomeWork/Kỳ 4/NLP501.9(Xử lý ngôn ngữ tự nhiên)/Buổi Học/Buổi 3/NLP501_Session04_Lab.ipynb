{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP501 - NATURAL LANGUAGE PROCESSING\n",
        "\n",
        "# LAB 04: Document Search System with TF-IDF and Locality Sensitive Hashing\n",
        "\n",
        "- **Dataset:** AG News (120,000 articles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset: AG News\n",
        "\n",
        "AG News is a corpus containing over 1 million news articles from over 2000 sources. The most popular version contains 120,000 training samples and 7,600 test samples, categorized into 4 classes: World, Sports, Business, Sci/Tech\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# !pip install datasets nltk scikit-learn numpy matplotlib tqdm\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict, Counter\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple, Set\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Tuple, Set\n",
        "\n",
        "from datasets import load_dataset\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Data preprocessing\n",
        "\n",
        "### 1.1. Load dataset\n",
        "\n",
        "We use Hugging Face's `datasets` library to load AG News but just use the first 10,000 documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 2046001.95 examples/s]\n",
            "Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 951884.57 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 10000 documents\n",
            "\n",
            "Label distribution:\n",
            "label\n",
            "3    2662\n",
            "0    2523\n",
            "2    2477\n",
            "1    2338\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample document:\n",
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load first 10,000 samples for faster processing\n",
        "dataset = load_dataset('ag_news', split='train[:10000]')\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': dataset['text'],\n",
        "    'label': dataset['label']\n",
        "})\n",
        "\n",
        "print(f\"Dataset size: {len(df)} documents\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nSample document:\")\n",
        "print(df['text'].iloc[0][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Text processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: The quick brown fox jumps over a lazy dogs!\n",
            "Processed: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dogs']\n"
          ]
        }
      ],
      "source": [
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in STOP_WORDS and len(w) > 2]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "sample_text = \"The quick brown fox jumps over a lazy dogs!\"\n",
        "print(f\"Original: {sample_text}\")\n",
        "print(f\"Processed: {preprocess_text(sample_text)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Preprocess documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average tokens per document: 24.6\n",
            "Sample processed text: wall bears claw back black reuters reuters shortsellers wall streets dwindlingband ultracynics seein...\n"
          ]
        }
      ],
      "source": [
        "# Process all documents\n",
        "df['tokens'] = df['text'].apply(preprocess_text)\n",
        "df['processed_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Statistics\n",
        "avg_tokens = df['tokens'].apply(len).mean()\n",
        "print(f\"Average tokens per document: {avg_tokens:.1f}\")\n",
        "print(f\"Sample processed text: {df['processed_text'].iloc[0][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: TF-IDF Vectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating TF-IDF matrix...\n",
            "TF-IDF matrix shape: (10000, 5000)\n",
            "Vocabulary size: 5000\n",
            "Sample feature names: ['aaron' 'abandon' 'abandoned' 'abbey' 'abducted' 'ability' 'able'\n",
            " 'abroad' 'absence' 'abu']\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,      # Limit vocabulary size\n",
        "    min_df=2,               # Ignore rare terms\n",
        "    max_df=0.95,            # Ignore very common terms\n",
        "    sublinear_tf=True       # Use log(1 + tf) instead of tf\n",
        ")\n",
        "\n",
        "print(\"Creating TF-IDF matrix...\")\n",
        "tfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n",
        "\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Normalize vectors for cosine similarity\n",
        "tfidf_normalized = normalize(tfidf_dense, norm='l2')\n",
        "\n",
        "print(f\"TF-IDF matrix shape: {tfidf_normalized.shape}\")\n",
        "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "print(f\"Sample feature names: {vectorizer.get_feature_names_out()[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: LSH implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_sim(a, b):\n",
        "    \"\"\"Compute cosine similarity\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
        "\n",
        "\n",
        "class LSHIndex:\n",
        "    \"\"\"\n",
        "    LSH with random hyperplanes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_planes: int, dim: int):\n",
        "        \"\"\"\n",
        "        Initialize LSH index.\n",
        "        Args:\n",
        "            n_planes: Number of random hyperplanes\n",
        "            dim: Dimension of input vectors\n",
        "        \"\"\"\n",
        "        self.n_planes = n_planes\n",
        "        self.dim = dim\n",
        "\n",
        "        # Generate random hyperplanes (normal vectors)\n",
        "        self.planes = np.random.randn(n_planes, dim)\n",
        "\n",
        "        # Hash table: bucket_id -> list of vector indices\n",
        "        self.hash_table = defaultdict(list)\n",
        "\n",
        "        # Store indexed vectors for similarity computation\n",
        "        self.vectors = None\n",
        "\n",
        "    def _hash(self, vector: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Compute hash (bucket ID) for a vector.\n",
        "\n",
        "        Args:\n",
        "            vector: Input vector of shape (dim,)\n",
        "        Returns:\n",
        "            bucket_id: Integer hash value\n",
        "        \"\"\"\n",
        "        # Compute dot product with all planes\n",
        "        projections = self.planes @ vector\n",
        "\n",
        "        # Convert to binary: 1 if positive, 0 otherwise\n",
        "        binary_hash = (projections >= 0).astype(int)\n",
        "\n",
        "        # Convert binary to integer (bucket ID)\n",
        "        bucket_id = sum(2**i * b for i, b in enumerate(binary_hash))\n",
        "\n",
        "        return bucket_id\n",
        "\n",
        "    def index(self, vectors: np.ndarray):\n",
        "        \"\"\"\n",
        "        Index all vectors into hash table.\n",
        "\n",
        "        Args:\n",
        "            vectors: Matrix of shape (n_vectors, dim)\n",
        "        \"\"\"\n",
        "        self.vectors = vectors\n",
        "\n",
        "        for idx in range(len(vectors)):\n",
        "            bucket_id = self._hash(vectors[idx])\n",
        "            self.hash_table[bucket_id].append(idx)\n",
        "\n",
        "    def query(self, query_vector: np.ndarray, k: int) -> List[Tuple[int, float]]:\n",
        "        \"\"\"\n",
        "        Find k approximate nearest neighbors.\n",
        "\n",
        "        Args:\n",
        "            query_vector: Query vector of shape (dim,)\n",
        "            k: Number of neighbors to return\n",
        "        Returns:\n",
        "            List of (index, similarity)\n",
        "        \"\"\"\n",
        "        # Find bucket for query\n",
        "        bucket_id = self._hash(query_vector)\n",
        "\n",
        "        # Get candidates from bucket\n",
        "        candidates = self.hash_table[bucket_id]\n",
        "\n",
        "        if len(candidates) == 0:\n",
        "            return []\n",
        "\n",
        "        # Compute exact similarity for candidates\n",
        "        similarities = []\n",
        "        for idx in candidates:\n",
        "            sim = cosine_sim(query_vector, self.vectors[idx])\n",
        "            similarities.append((idx, sim))\n",
        "\n",
        "        # Sort by similarity and return top k\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        return similarities[:k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. Multiple tables LSH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiTableLSH:\n",
        "    \"\"\"\n",
        "    LSH with multiple hash tables for improved recall.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_planes: int, n_tables: int, dim: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_planes: Number of planes per table\n",
        "            n_tables: Number of hash tables\n",
        "            dim: Vector dimension\n",
        "        \"\"\"\n",
        "        self.n_tables = n_tables\n",
        "        self.tables = [LSHIndex(n_planes, dim) for _ in range(n_tables)]\n",
        "        self.vectors = None\n",
        "\n",
        "    def index(self, vectors: np.ndarray):\n",
        "        \"\"\"Index vectors into all tables.\"\"\"\n",
        "        self.vectors = vectors\n",
        "        for table in self.tables:\n",
        "            table.index(vectors)\n",
        "\n",
        "    def query(self, query_vector: np.ndarray, k: int) -> List[Tuple[int, float]]:\n",
        "        \"\"\"\n",
        "        Query all tables and merge candidates.\n",
        "        \"\"\"\n",
        "        # Collect unique candidates from all tables\n",
        "        all_candidates = set()\n",
        "\n",
        "        for table in self.tables:\n",
        "            bucket_id = table._hash(query_vector)\n",
        "            candidates = table.hash_table[bucket_id]\n",
        "            all_candidates.update(candidates)\n",
        "\n",
        "        if len(all_candidates) == 0:\n",
        "            return []\n",
        "\n",
        "        # Compute exact similarity for all candidates\n",
        "        similarities = []\n",
        "        for idx in all_candidates:\n",
        "            sim = cosine_sim(query_vector, self.vectors[idx])\n",
        "            similarities.append((idx, sim))\n",
        "\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        return similarities[:k]\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get statistics about the index.\"\"\"\n",
        "        total_buckets = sum(len(t.hash_table) for t in self.tables)\n",
        "        avg_bucket_size = np.mean(\n",
        "            [len(b) for t in self.tables for b in t.hash_table.values()])\n",
        "        return {\n",
        "            'n_tables': self.n_tables,\n",
        "            'total_buckets': total_buckets,\n",
        "            'avg_bucket_size': avg_bucket_size\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4. Build LSH index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building LSH index with 10 planes and 5 tables...\n",
            "Vector dimension: 5000\n",
            "Number of documents: 10000\n",
            "\n",
            "Index built in 1.20 seconds\n",
            "Index stats: {'n_tables': 5, 'total_buckets': 5119, 'avg_bucket_size': np.float64(9.767532721234616)}\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "N_PLANES = 10      # Number of hyperplanes per table\n",
        "N_TABLES = 5       # Number of hash tables\n",
        "DIM = tfidf_normalized.shape[1]  # Vector dimension\n",
        "\n",
        "print(f\"Building LSH index with {N_PLANES} planes and {N_TABLES} tables...\")\n",
        "print(f\"Vector dimension: {DIM}\")\n",
        "print(f\"Number of documents: {len(tfidf_normalized)}\")\n",
        "\n",
        "# Create and build index\n",
        "start_time = time.time()\n",
        "lsh_index = MultiTableLSH(n_planes=N_PLANES, n_tables=N_TABLES, dim=DIM)\n",
        "lsh_index.index(tfidf_normalized)\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nIndex built in {build_time:.2f} seconds\")\n",
        "print(f\"Index stats: {lsh_index.get_stats()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Search & Evaluation\n",
        "\n",
        "### 4.1. Brute-Force search baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def brute_force_knn(query_vector: np.ndarray, vectors: np.ndarray, k: int) -> List[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    Exact k-NN search by computing similarity with all vectors.\n",
        "\n",
        "    Returns:\n",
        "        List of (index, similarity)\n",
        "    \"\"\"\n",
        "    similarities = []\n",
        "    for idx in range(len(vectors)):\n",
        "        sim = cosine_sim(query_vector, vectors[idx])\n",
        "        similarities.append((idx, sim))\n",
        "\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Evaluation metrics\n",
        "\n",
        "Metrics:\n",
        "\n",
        "- **Recall@K:** Proportion of true neighbors found\n",
        "- **Query Time:** Average time per query (ms)\n",
        "- **Speedup:** Brute_force_time / LSH_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_search(lsh_index, vectors, n_queries=100, k=10):\n",
        "    \"\"\"\n",
        "    Evaluate LSH search quality and speed.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    # Randomly sample query indices\n",
        "    query_indices = np.random.choice(len(vectors), n_queries, replace=False)\n",
        "\n",
        "    recalls = []\n",
        "    lsh_times = []\n",
        "    bf_times = []\n",
        "\n",
        "    for query_idx in tqdm(query_indices, desc=\"Evaluating\"):\n",
        "        query_vec = vectors[query_idx]\n",
        "\n",
        "        # Brute-force search\n",
        "        start = time.time()\n",
        "        bf_results = brute_force_knn(query_vec, vectors, k)\n",
        "        bf_times.append(time.time() - start)\n",
        "        bf_indices = set([idx for idx, _ in bf_results])\n",
        "\n",
        "        # LSH search\n",
        "        start = time.time()\n",
        "        lsh_results = lsh_index.query(query_vec, k)\n",
        "        lsh_times.append(time.time() - start)\n",
        "        lsh_indices = set([idx for idx, _ in lsh_results])\n",
        "\n",
        "        # Compute recall\n",
        "        if len(bf_indices) > 0:\n",
        "            recall = len(lsh_indices & bf_indices) / len(bf_indices)\n",
        "            recalls.append(recall)\n",
        "\n",
        "    return {\n",
        "        'avg_recall': np.mean(recalls) * 100,\n",
        "        'avg_lsh_time_ms': np.mean(lsh_times) * 1000,\n",
        "        'avg_bf_time_ms': np.mean(bf_times) * 1000,\n",
        "        'speedup': np.mean(bf_times) / np.mean(lsh_times)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3. Evaluate:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation on 100 random queries...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 100/100 [00:04<00:00, 20.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RESULTS:\n",
            "Average Recall@10: 17.4%\n",
            "Average LSH Query Time: 0.45 ms\n",
            "Average Brute-Force Time: 49.26 ms\n",
            "Speedup: 110.7x\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running evaluation on 100 random queries...\\n\")\n",
        "\n",
        "results = evaluate_search(lsh_index, tfidf_normalized, n_queries=100, k=10)\n",
        "\n",
        "print(\"RESULTS:\")\n",
        "\n",
        "print(f\"Average Recall@10: {results['avg_recall']:.1f}%\")\n",
        "print(f\"Average LSH Query Time: {results['avg_lsh_time_ms']:.2f} ms\")\n",
        "print(f\"Average Brute-Force Time: {results['avg_bf_time_ms']:.2f} ms\")\n",
        "print(f\"Speedup: {results['speedup']:.1f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4. Build a function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 'Apple iPhone new technology'\n",
            "\n",
            "Top 5 results:\n",
            "1. [Sci/Tech] (sim=0.129)\n",
            "   Bankrupt Commerce One Patents Fetch \\$15.5M Bankrupt Internet software maker Commerce One Inc. aucti...\n",
            "\n",
            "2. [World] (sim=0.061)\n",
            "   China Tests New Guided Missile Amid Taiwan Tensions  BEIJING (Reuters) - China has successfully test...\n",
            "\n",
            "3. [Sports] (sim=0.057)\n",
            "   Yao #39;s 39 carries China past New Zealand ATHENS, Greece - Yao Ming bounced back from a rough open...\n",
            "\n",
            "4. [Business] (sim=0.049)\n",
            "   Before-the-Bell: Biocryst Shares Higher  NEW YORK (Reuters) - Shares of Biocryst Pharmaceuticals  In...\n",
            "\n",
            "5. [Sports] (sim=0.039)\n",
            "   Northern N.E. reaches peak With some late heroics, Northern New England knocked off undefeated North...\n",
            "\n",
            "Query: 'stock market investment'\n",
            "\n",
            "Top 5 results:\n",
            "1. [Business] (sim=0.156)\n",
            "   Nortel to Cut 3,500 Jobs to Boost Profits  OTTAWA (Reuters) - Nortel Networks Corp. &lt;A HREF=\"http...\n",
            "\n",
            "2. [Business] (sim=0.107)\n",
            "   UPDATE 1-SEC, NASD probing Jefferies trader #39;s gifts -WSJ US market regulators are looking into w...\n",
            "\n",
            "3. [Business] (sim=0.089)\n",
            "   Google shares rise in debut Shares in Google rose \\$15.01 or 17.7 percent, to open at \\$100.01 its p...\n",
            "\n",
            "4. [Business] (sim=0.089)\n",
            "   Dollar Shrugs Off Overvalued U.S. Housing  NEW YORK (Reuters) - The dollar so far has brushed off  s...\n",
            "\n",
            "5. [Business] (sim=0.079)\n",
            "   Boom in house prices is  #39;well and truly over #39; The tide has  quot;definitely quot; turned aga...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def search_documents(query_text: str, k: int = 5):\n",
        "    \"\"\"\n",
        "    Search for documents similar to query text.\n",
        "    \"\"\"\n",
        "    # Vectorize query\n",
        "    query_vec = vectorizer.transform([query_text]).toarray()[0]\n",
        "    query_vec = normalize(query_vec.reshape(1, -1))[0]\n",
        "\n",
        "    # Search\n",
        "    results = lsh_index.query(query_vec, k)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Query: '{query_text}'\\n\")\n",
        "    print(f\"Top {k} results:\")\n",
        "\n",
        "    categories = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "    for i, (idx, sim) in enumerate(results):\n",
        "        cat = categories[df['label'].iloc[idx]]\n",
        "        text = df['text'].iloc[idx][:100]\n",
        "        print(f\"{i+1}. [{cat}] (sim={sim:.3f})\")\n",
        "        print(f\"   {text}...\")\n",
        "        print()\n",
        "\n",
        "\n",
        "search_documents(\"Apple iPhone new technology\")\n",
        "search_documents(\"stock market investment\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenvironment",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
