{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 06: N-gram Language Models & Word Embeddings\n",
        "\n",
        "- **Libraries:** NLTK, NumPy, Gensim, Matplotlib, scikit-learn\n",
        "- **Datasets:** Reuters Corpus, Brown Corpus, Text8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: N-gram Language Model\n",
        "\n",
        "### 1.1. Introduction to N-gram Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/60/90/81ac364ef94209c100e12579629dc92bf7a709a84af32f8c551b02c07e94/nltk-3.9.2-py3-none-any.whl.metadata\n",
            "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting numpy\n",
            "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/8e/ba/80fc0b1e3cb2fd5c6143f00f42eb67762aa043eaa05ca924ecc3222a7849/numpy-2.4.1-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
            "  Using cached numpy-2.4.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting gensim\n",
            "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/38/7c/18d40f341276a7461962512ca1fb716d5982db57615dfa272f651ecb96d7/gensim-4.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading gensim-4.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
            "Collecting matplotlib\n",
            "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/fd/14/baad3222f424b19ce6ad243c71de1ad9ec6b2e4eb1e458a48fdc6d120401/matplotlib-3.10.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached matplotlib-3.10.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (52 kB)\n",
            "Collecting scikit-learn\n",
            "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/01/18/d154dc1638803adf987910cdd07097d9c526663a55666a97c124d09fb96a/scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
            "  Using cached scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
            "Collecting click (from nltk)\n",
            "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/98/78/01c019cdb5d6498122777c1a43056ebb3ebfeef2076d9d026bfe15583b2b/click-8.3.1-py3-none-any.whl.metadata\n",
            "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/7b/91/984aca2ec129e2757d1e4e3c81c3fcda9d0f85b74670a094cc443d9ee949/joblib-1.5.3-py3-none-any.whl.metadata\n",
            "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/a0/f9/5f1fd077d106ca5655a0f9ff8f25a1ab55b92128b5713a91ed7134ff688e/regex-2026.1.15-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading regex-2026.1.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m854.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk)\n",
            "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting scipy>=1.7.0 (from gensim)\n",
            "  Obtaining dependency information for scipy>=1.7.0 from https://files.pythonhosted.org/packages/c1/20/095ad24e031ee8ed3c5975954d816b8e7e2abd731e04f8be573de8740885/scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
            "  Using cached scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Collecting smart_open>=1.8.1 (from gensim)\n",
            "  Obtaining dependency information for smart_open>=1.8.1 from https://files.pythonhosted.org/packages/ad/95/bc978be7ea0babf2fb48a414b6afaad414c6a9e8b1eafc5b8a53c030381a/smart_open-7.5.0-py3-none-any.whl.metadata\n",
            "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/0d/44/c4b0b6095fef4dc9c420e041799591e3b63e9619e3044f7f4f6c21c0ab24/contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/69/12/bf9f4eaa2fad039356cc627587e30ed008c03f1cebd3034376b5ee8d1d44/fonttools-4.61.1-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
            "  Using cached fonttools-4.61.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (114 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Obtaining dependency information for kiwisolver>=1.3.1 from https://files.pythonhosted.org/packages/31/a2/a12a503ac1fd4943c50f9822678e8015a790a13b5490354c68afb8489814/kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/default/Library/Mobile Documents/com~apple~CloudDocs/FSB_Courses/NLP501/.venv/lib/python3.11/site-packages (from matplotlib) (26.0)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Obtaining dependency information for pillow>=8 from https://files.pythonhosted.org/packages/43/06/7264c0597e676104cc22ca73ee48f752767cd4b1fe084662620b17e10120/pillow-12.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached pillow-12.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib)\n",
            "  Obtaining dependency information for pyparsing>=3 from https://files.pythonhosted.org/packages/10/bd/c038d7cc38edc1aa5bf91ab8068b63d4308c66c4c8bb3cbba7dfbc049f9c/pyparsing-3.3.2-py3-none-any.whl.metadata\n",
            "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/default/Library/Mobile Documents/com~apple~CloudDocs/FSB_Courses/NLP501/.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
            "  Obtaining dependency information for threadpoolctl>=3.2.0 from https://files.pythonhosted.org/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl.metadata\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/default/Library/Mobile Documents/com~apple~CloudDocs/FSB_Courses/NLP501/.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
            "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/f0/40/660b2898703e5cbbb43db10cdefcc294274458c3ca4c68637c2b99371507/wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
            "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "Using cached numpy-2.4.1-cp311-cp311-macosx_14_0_arm64.whl (5.5 MB)\n",
            "Downloading gensim-4.4.0-cp311-cp311-macosx_11_0_arm64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m966.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached matplotlib-3.10.8-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
            "Using cached scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl (8.1 MB)\n",
            "Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (270 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.61.1-cp311-cp311-macosx_10_9_universal2.whl (2.9 MB)\n",
            "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
            "Using cached pillow-12.1.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
            "Downloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading regex-2026.1.15-cp311-cp311-macosx_11_0_arm64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.5/288.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl (20.1 MB)\n",
            "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tqdm, threadpoolctl, regex, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, click, smart_open, scipy, nltk, contourpy, scikit-learn, matplotlib, gensim\n",
            "Successfully installed click-8.3.1 contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 gensim-4.4.0 joblib-1.5.3 kiwisolver-1.4.9 matplotlib-3.10.8 nltk-3.9.2 numpy-2.4.1 pillow-12.1.0 pyparsing-3.3.2 regex-2026.1.15 scikit-learn-1.8.0 scipy-1.17.0 smart_open-7.5.0 threadpoolctl-3.6.0 tqdm-4.67.1 wrapt-2.0.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk numpy gensim matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown, reuters\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('reuters')\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: N-gram Language Model\n",
        "\n",
        "### 1.2 Exercise 1: Build N-gram Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NgramCounter:\n",
        "    \"\"\"N-gram frequency counter\"\"\"\n",
        "    \n",
        "    def __init__(self, n=2):\n",
        "        self.n = n\n",
        "        self.ngram_counts = Counter()\n",
        "        self.context_counts = Counter()\n",
        "        self.vocab = set()\n",
        "    \n",
        "    def get_ngrams(self, tokens):\n",
        "        \"\"\"\n",
        "        Generate n-grams from token list\n",
        "        Add <s> at start and </s> at end\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # 1. Add (n-1) <s> tokens at start\n",
        "        padded = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
        "        \n",
        "        # 2. Generate all n-grams as tuples\n",
        "        ngrams = []\n",
        "        for i in range(len(padded) - self.n + 1):\n",
        "            ngram = tuple(padded[i:i + self.n])\n",
        "            ngrams.append(ngram)\n",
        "        \n",
        "        return ngrams\n",
        "    \n",
        "    def train(self, sentences):\n",
        "        \"\"\"Train on list of tokenized sentences\"\"\"\n",
        "        # TODO: Implement this method\n",
        "        for sentence in sentences:\n",
        "            # 1. Get ngrams from sentence\n",
        "            ngrams = self.get_ngrams(sentence)\n",
        "            \n",
        "            # 2. Count ngrams and contexts\n",
        "            for ngram in ngrams:\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                # Context is all except last word\n",
        "                context = ngram[:-1]\n",
        "                self.context_counts[context] += 1\n",
        "                # Add word to vocabulary\n",
        "                self.vocab.add(ngram[-1])\n",
        "        \n",
        "        print(f\"Trained on {len(sentences)} sentences\")\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "        print(f\"Total {self.n}-grams: {sum(self.ngram_counts.values())}\")\n",
        "\n",
        "counter = NgramCounter(n=2)\n",
        "counter.train([['i', 'am', 'sam'], ['sam', 'i', 'am']])\n",
        "\n",
        "print(f\"\\nngram_counts[('i', 'am')]: {counter.ngram_counts[('i', 'am')]}\")\n",
        "print(f\"context_counts[('i',)]: {counter.context_counts[('i',)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Exercise 2: Calculate N-gram Probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example: Test NgramLanguageModel with different smoothing methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NgramLanguageModel(NgramCounter):\n",
        "    \"\"\"N-gram Language Model with smoothing\"\"\"\n",
        "    \n",
        "    def probability(self, word, context, smoothing='none', k=1):\n",
        "        \"\"\"\n",
        "        Calculate P(word | context) with optional smoothing\n",
        "        smoothing: 'none', 'add-k', 'laplace'\n",
        "        \"\"\"\n",
        "        ngram = context + (word,)\n",
        "        \n",
        "        if smoothing == 'none':\n",
        "            # MLE: P(word|context) = C(context, word) / C(context)\n",
        "            ngram_count = self.ngram_counts.get(ngram, 0)\n",
        "            context_count = self.context_counts.get(context, 0)\n",
        "            \n",
        "            if context_count == 0:\n",
        "                return 1e-10  # Small probability for unseen context\n",
        "            \n",
        "            return ngram_count / context_count\n",
        "        \n",
        "        elif smoothing in ['add-k', 'laplace']:\n",
        "            # Add-k smoothing: P(word|context) = (C(context, word) + k) / (C(context) + k*V)\n",
        "            if smoothing == 'laplace':\n",
        "                k = 1\n",
        "            \n",
        "            ngram_count = self.ngram_counts.get(ngram, 0)\n",
        "            context_count = self.context_counts.get(context, 0)\n",
        "            V = len(self.vocab)\n",
        "            \n",
        "            return (ngram_count + k) / (context_count + k * V)\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown smoothing method: {smoothing}\")\n",
        "    \n",
        "    def sentence_probability(self, sentence, log=True, smoothing='add-k', k=1):\n",
        "        \"\"\"Calculate probability of entire sentence\"\"\"\n",
        "        # Get n-grams from sentence\n",
        "        ngrams = self.get_ngrams(sentence)\n",
        "        \n",
        "        if log:\n",
        "            # Use log probability\n",
        "            log_prob = 0\n",
        "            for ngram in ngrams:\n",
        "                context = ngram[:-1]\n",
        "                word = ngram[-1]\n",
        "                prob = self.probability(word, context, smoothing, k)\n",
        "                log_prob += math.log(prob + 1e-10)  # Add small epsilon\n",
        "            return log_prob\n",
        "        else:\n",
        "            # Regular probability\n",
        "            prob = 1.0\n",
        "            for ngram in ngrams:\n",
        "                context = ngram[:-1]\n",
        "                word = ngram[-1]\n",
        "                prob *= self.probability(word, context, smoothing, k)\n",
        "            return prob\n",
        "    \n",
        "    def perplexity(self, test_sentences, smoothing='add-k', k=1):\n",
        "        \"\"\"Calculate perplexity on test set\"\"\"\n",
        "        total_log_prob = 0\n",
        "        total_words = 0\n",
        "        \n",
        "        for sentence in test_sentences:\n",
        "            log_prob = self.sentence_probability(sentence, log=True, smoothing=smoothing, k=k)\n",
        "            total_log_prob += log_prob\n",
        "            total_words += len(sentence) + 1  # +1 for </s>\n",
        "        \n",
        "        # Perplexity = exp(-1/N * sum(log P))\n",
        "        avg_log_prob = total_log_prob / total_words\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "        \n",
        "        return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train a bigram language model\n",
        "lm = NgramLanguageModel(n=2)\n",
        "train_sents = [\n",
        "    ['i', 'am', 'sam'],\n",
        "    ['sam', 'i', 'am'],\n",
        "    ['i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']\n",
        "]\n",
        "lm.train(train_sents)\n",
        "\n",
        "# Test probability calculations\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Probability Calculations\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "context = ('i',)\n",
        "word = 'am'\n",
        "\n",
        "# No smoothing\n",
        "prob_none = lm.probability(word, context, smoothing='none')\n",
        "print(f\"\\nP('{word}' | '{context[0]}') [no smoothing] = {prob_none:.4f}\")\n",
        "\n",
        "# Laplace smoothing\n",
        "prob_laplace = lm.probability(word, context, smoothing='laplace')\n",
        "print(f\"P('{word}' | '{context[0]}') [Laplace] = {prob_laplace:.4f}\")\n",
        "\n",
        "# Add-k smoothing\n",
        "prob_addk = lm.probability(word, context, smoothing='add-k', k=0.5)\n",
        "print(f\"P('{word}' | '{context[0]}') [Add-0.5] = {prob_addk:.4f}\")\n",
        "\n",
        "# Test on unseen word\n",
        "unseen_word = 'python'\n",
        "prob_unseen = lm.probability(unseen_word, context, smoothing='laplace')\n",
        "print(f\"\\nP('{unseen_word}' | '{context[0]}') [Laplace] = {prob_unseen:.6f}\")\n",
        "\n",
        "# Test sentence probability\n",
        "test_sent = ['i', 'am', 'sam']\n",
        "log_prob = lm.sentence_probability(test_sent, log=True, smoothing='laplace')\n",
        "print(f\"\\nLog probability of '{' '.join(test_sent)}': {log_prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Training on Real Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess Brown corpus\n",
        "brown_sents = brown.sents()\n",
        "\n",
        "# Lowercase and limit corpus size for faster training\n",
        "def preprocess_sentences(sents, max_sents=10000):\n",
        "    \"\"\"Preprocess sentences: lowercase and filter\"\"\"\n",
        "    processed = []\n",
        "    for sent in sents[:max_sents]:\n",
        "        # Lowercase all words\n",
        "        sent_lower = [word.lower() for word in sent]\n",
        "        processed.append(sent_lower)\n",
        "    return processed\n",
        "\n",
        "corpus = preprocess_sentences(brown_sents, max_sents=10000)\n",
        "\n",
        "# Split into train and test\n",
        "split_idx = int(len(corpus) * 0.9)\n",
        "train_corpus = corpus[:split_idx]\n",
        "test_corpus = corpus[split_idx:]\n",
        "\n",
        "print(f\"✓ Loaded {len(corpus):,} sentences\")\n",
        "print(f\"  Training: {len(train_corpus):,} sentences\")\n",
        "print(f\"  Testing: {len(test_corpus):,} sentences\")\n",
        "\n",
        "# Show sample sentences\n",
        "print(\"\\nSample sentences:\")\n",
        "for i, sent in enumerate(train_corpus[:3]):\n",
        "    print(f\"  [{i+1}] {' '.join(sent[:10])}{'...' if len(sent) > 10 else ''}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train models with different n values\n",
        "print(\"Training N-gram Language Models\")\n",
        "\n",
        "models = {}\n",
        "\n",
        "for n in [1, 2, 3]:\n",
        "    print(f\"\\nTraining {n}-gram model...\")\n",
        "    model = NgramLanguageModel(n=n)\n",
        "    model.train(train_corpus)\n",
        "    models[n] = model\n",
        "    \n",
        "    # Show top 10 most common n-grams\n",
        "    top_ngrams = model.ngram_counts.most_common(10)\n",
        "    print(f\"\\n  Top 10 {n}-grams:\")\n",
        "    for ngram, count in top_ngrams:\n",
        "        ngram_str = ' '.join(ngram)\n",
        "        print(f\"    {ngram_str:30} : {count:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5. Text Generation & Autocomplete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextGenerator(NgramLanguageModel):\n",
        "    \"\"\"Text generation using N-gram Language Model\"\"\"\n",
        "    \n",
        "    def predict_next_word(self, context, top_k=5, smoothing='add-k', k=1):\n",
        "        \"\"\"\n",
        "        Predict top-k most likely next words given context.\n",
        "        \n",
        "        Args:\n",
        "            context: tuple of (n-1) previous words\n",
        "            top_k: number of predictions to return\n",
        "            smoothing: smoothing method\n",
        "            k: smoothing parameter\n",
        "            \n",
        "        Returns:\n",
        "            list of (word, probability) tuples\n",
        "        \"\"\"\n",
        "        # Calculate probability for all words in vocabulary\n",
        "        word_probs = []\n",
        "        \n",
        "        for word in self.vocab:\n",
        "            if word not in ['<s>', '</s>']:  # Skip special tokens\n",
        "                prob = self.probability(word, context, smoothing, k)\n",
        "                word_probs.append((word, prob))\n",
        "        \n",
        "        # Sort by probability and return top-k\n",
        "        word_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "        return word_probs[:top_k]\n",
        "    \n",
        "    def generate_text(self, start_words=None, max_length=20, smoothing='add-k', k=1):\n",
        "        \"\"\"\n",
        "        Generate text using the language model.\n",
        "        \n",
        "        Args:\n",
        "            start_words: list of starting words (optional)\n",
        "            max_length: maximum number of words to generate\n",
        "            smoothing: smoothing method\n",
        "            k: smoothing parameter\n",
        "            \n",
        "        Returns:\n",
        "            generated text as string\n",
        "        \"\"\"\n",
        "        if start_words is None:\n",
        "            start_words = ['<s>'] * (self.n - 1)\n",
        "        else:\n",
        "            start_words = ['<s>'] * (self.n - 1) + start_words\n",
        "        \n",
        "        generated = list(start_words)\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            # Get context (last n-1 words)\n",
        "            context = tuple(generated[-(self.n-1):])\n",
        "            \n",
        "            # Predict next word\n",
        "            predictions = self.predict_next_word(context, top_k=10, smoothing=smoothing, k=k)\n",
        "            \n",
        "            if not predictions:\n",
        "                break\n",
        "            \n",
        "            # Sample from predictions (weighted by probability)\n",
        "            words, probs = zip(*predictions)\n",
        "            probs = np.array(probs)\n",
        "            probs = probs / probs.sum()  # Normalize\n",
        "            \n",
        "            next_word = np.random.choice(words, p=probs)\n",
        "            \n",
        "            # Stop if we generate end token\n",
        "            if next_word == '</s>':\n",
        "                break\n",
        "            \n",
        "            generated.append(next_word)\n",
        "        \n",
        "        # Remove start tokens and return\n",
        "        result = [w for w in generated if w != '<s>']\n",
        "        return ' '.join(result)\n",
        "    \n",
        "    def autocomplete(self, partial_text, top_k=5, smoothing='add-k', k=1):\n",
        "        \"\"\"\n",
        "        Autocomplete: suggest next words given partial text.\n",
        "        \n",
        "        Args:\n",
        "            partial_text: string of partial text\n",
        "            top_k: number of suggestions\n",
        "            smoothing: smoothing method\n",
        "            k: smoothing parameter\n",
        "            \n",
        "        Returns:\n",
        "            list of (word, probability) tuples\n",
        "        \"\"\"\n",
        "        # Tokenize\n",
        "        tokens = partial_text.lower().split()\n",
        "        \n",
        "        # Get context (last n-1 words)\n",
        "        if len(tokens) >= self.n - 1:\n",
        "            context = tuple(tokens[-(self.n-1):])\n",
        "        else:\n",
        "            # Pad with <s> if not enough words\n",
        "            padding = ['<s>'] * (self.n - 1 - len(tokens))\n",
        "            context = tuple(padding + tokens)\n",
        "        \n",
        "        # Predict next word\n",
        "        predictions = self.predict_next_word(context, top_k=top_k, smoothing=smoothing, k=k)\n",
        "        \n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example: Text Generation and Autocomplete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create text generator from trained bigram model\n",
        "generator = TextGenerator(n=2)\n",
        "generator.train(train_corpus)\n",
        "\n",
        "print(\"TEXT GENERATION\")\n",
        "\n",
        "# Generate text from scratch\n",
        "print(\"\\n1. Generate text from scratch:\")\n",
        "for i in range(3):\n",
        "    text = generator.generate_text(max_length=15)\n",
        "    print(f\"  [{i+1}] {text}\")\n",
        "\n",
        "# Generate text with starting words\n",
        "print(\"\\n2. Generate text starting with 'the':\")\n",
        "for i in range(3):\n",
        "    text = generator.generate_text(start_words=['the'], max_length=12)\n",
        "    print(f\"  [{i+1}] {text}\")\n",
        "\n",
        "print(\"\\n3. Generate text starting with 'he was':\")\n",
        "for i in range(3):\n",
        "    text = generator.generate_text(start_words=['he', 'was'], max_length=12)\n",
        "    print(f\"  [{i+1}] {text}\")\n",
        "\n",
        "print(\"AUTOCOMPLETE / NEXT WORD PREDICTION\")\n",
        "\n",
        "# Test autocomplete\n",
        "test_phrases = [\n",
        "    \"the\",\n",
        "    \"he was\",\n",
        "    \"in the\",\n",
        "    \"to be\",\n",
        "    \"it is\"\n",
        "]\n",
        "\n",
        "for phrase in test_phrases:\n",
        "    predictions = generator.autocomplete(phrase, top_k=5)\n",
        "    print(f\"\\n'{phrase}' ->\")\n",
        "    for word, prob in predictions:\n",
        "        print(f\"  {word:<15} (p = {prob:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6. Model Evaluation with Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate perplexity on test set\n",
        "print(\"PERPLEXITY EVALUATION\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for n in [1, 2, 3]:\n",
        "    model = models[n]\n",
        "    \n",
        "    print(f\"\\n{n}-gram Model:\")\n",
        "    \n",
        "    # Try different smoothing methods\n",
        "    for smoothing in ['laplace', 'add-k']:\n",
        "        k_value = 1 if smoothing == 'laplace' else 0.5\n",
        "        \n",
        "        perplexity = model.perplexity(test_corpus, smoothing=smoothing, k=k_value)\n",
        "        \n",
        "        print(f\"  {smoothing:<15} (k={k_value}): Perplexity = {perplexity:.2f}\")\n",
        "        \n",
        "        results.append({\n",
        "            'n': n,\n",
        "            'smoothing': smoothing,\n",
        "            'k': k_value,\n",
        "            'perplexity': perplexity\n",
        "        })\n",
        "\n",
        "print(\"SUMMARY\")\n",
        "print(f\"\\n{'Model':<20} {'Smoothing':<15} {'Perplexity':<15}\")\n",
        "for r in results:\n",
        "    model_name = f\"{r['n']}-gram\"\n",
        "    print(f\"{model_name:<20} {r['smoothing']:<15} {r['perplexity']:<15.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize perplexity comparison\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Perplexity by n-gram size\n",
        "plt.subplot(1, 2, 1)\n",
        "for smoothing in df['smoothing'].unique():\n",
        "    subset = df[df['smoothing'] == smoothing]\n",
        "    plt.plot(subset['n'], subset['perplexity'], marker='o', label=smoothing, linewidth=2)\n",
        "\n",
        "plt.xlabel('N-gram Size', fontsize=12)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.title('Perplexity vs N-gram Size', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks([1, 2, 3])\n",
        "\n",
        "# Plot 2: Bar chart comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "x = np.arange(len(df))\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "bars = plt.bar(x, df['perplexity'], color=colors[:len(df)])\n",
        "\n",
        "plt.xlabel('Model Configuration', fontsize=12)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.title('Perplexity Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, [f\"{r['n']}-gram\\n{r['smoothing']}\" for r in results], rotation=0, fontsize=9)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, df['perplexity']):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{val:.1f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show best model\n",
        "best_idx = df['perplexity'].idxmin()\n",
        "best_model = df.loc[best_idx]\n",
        "print(f\"\\n Best Model: {best_model['n']}-gram with {best_model['smoothing']} smoothing\")\n",
        "print(f\"   Perplexity: {best_model['perplexity']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Word Embeddings (Word2Vec)\n",
        "\n",
        "### 2.1. Introduction to Word Embeddings\n",
        "\n",
        "**Word2Vec Architecture:**\n",
        "1. **CBOW (Continuous Bag of Words):** Predict target word from context\n",
        "2. **Skip-gram:** Predict context words from target word\n",
        "\n",
        "**Applications:**\n",
        "- Word similarity\n",
        "- Analogies (king - man + woman ≈ queen)\n",
        "- Document classification\n",
        "- Semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Training Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare corpus for Word2Vec\n",
        "print(\"Preparing corpus for Word2Vec training...\")\n",
        "w2v_corpus = preprocess_sentences(brown_sents, max_sents=20000)\n",
        "\n",
        "print(f\"Corpus size: {len(w2v_corpus):,} sentences\")\n",
        "print(f\"Sample: {' '.join(w2v_corpus[0][:15])}...\\n\")\n",
        "\n",
        "# Train Word2Vec model\n",
        "print(\"TRAINING WORD2VEC MODEL\")\n",
        "\n",
        "# Skip-gram model\n",
        "print(\"\\n[1/2] Training Skip-gram model...\")\n",
        "sg_model = Word2Vec(\n",
        "    sentences=w2v_corpus,\n",
        "    vector_size=100,      # Dimensionality of embeddings\n",
        "    window=5,             # Context window size\n",
        "    min_count=5,          # Ignore words with freq < 5\n",
        "    workers=4,            # Number of CPU cores\n",
        "    sg=1,                 # 1 = skip-gram, 0 = CBOW\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "print(f\" Trained on {len(w2v_corpus):,} sentences\")\n",
        "print(f\"  Vocabulary size: {len(sg_model.wv):,} words\")\n",
        "print(f\"  Vector dimensionality: {sg_model.wv.vector_size}\")\n",
        "\n",
        "# CBOW model\n",
        "print(\"\\n[2/2] Training CBOW model...\")\n",
        "cbow_model = Word2Vec(\n",
        "    sentences=w2v_corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        "    sg=0,                 # CBOW\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "print(f\" Trained on {len(w2v_corpus):,} sentences\")\n",
        "print(f\" Vocabulary size: {len(cbow_model.wv):,} words\")\n",
        "print(f\" Vector dimensionality: {cbow_model.wv.vector_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Loading Pre-trained Embeddings\n",
        "\n",
        "Use pre-trained Word2Vec model (Google News) or GloVe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained embeddings (this may take a while on first run)\n",
        "print(\"LOADING PRE-TRAINED EMBEDDINGS\")\n",
        "\n",
        "print(\"\\nAvailable pre-trained models:\")\n",
        "print(\"  - glove-wiki-gigaword-50 (66 MB)\")\n",
        "print(\"  - glove-wiki-gigaword-100 (128 MB)\")\n",
        "print(\"  - word2vec-google-news-300 (1.6 GB)\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nLoading GloVe embeddings (50-dim)...\")\n",
        "    print(\"(This may take a minute on first download...)\")\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
        "    \n",
        "    print(f\"  Loaded GloVe model\")\n",
        "    print(f\"  Vocabulary size: {len(glove_model):,} words\")\n",
        "    print(f\"  Vector dimensionality: {glove_model.vector_size}\")\n",
        "    \n",
        "    # Show sample vectors\n",
        "    print(\"\\nSample word vectors:\")\n",
        "    for word in ['king', 'queen', 'computer', 'python']:\n",
        "        if word in glove_model:\n",
        "            vector = glove_model[word]\n",
        "            print(f\"  '{word}': [{vector[0]:.3f}, {vector[1]:.3f}, {vector[2]:.3f}, ...]\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Could not load pre-trained model: {e}\")\n",
        "    print(\"Will use trained model instead.\")\n",
        "    glove_model = sg_model.wv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Word Similarity and Analogies\n",
        "\n",
        "Explore semantic relationships captured by word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word Similarity\n",
        "print(\"WORD SIMILARITY\")\n",
        "\n",
        "# Use GloVe or trained model\n",
        "model = glove_model\n",
        "\n",
        "test_words = ['king', 'queen', 'man', 'woman', 'computer', 'dog', 'cat']\n",
        "\n",
        "print(\"\\nFinding similar words:\\n\")\n",
        "for word in test_words:\n",
        "    if word in model:\n",
        "        similar = model.most_similar(word, topn=5)\n",
        "        print(f\"'{word}' → \", end=\"\")\n",
        "        print(\", \".join([f\"{w} ({s:.3f})\" for w, s in similar]))\n",
        "    else:\n",
        "        print(f\"'{word}' → (not in vocabulary)\")\n",
        "\n",
        "# Pairwise similarity\n",
        "print(\"PAIRWISE SIMILARITY (Cosine)\")\n",
        "\n",
        "word_pairs = [\n",
        "    ('king', 'queen'),\n",
        "    ('man', 'woman'),\n",
        "    ('dog', 'cat'),\n",
        "    ('computer', 'keyboard'),\n",
        "    ('happy', 'sad'),\n",
        "    ('good', 'bad'),\n",
        "    ('king', 'computer')  # Unrelated\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Word 1':<15} {'Word 2':<15} {'Similarity':<15}\")\n",
        "for w1, w2 in word_pairs:\n",
        "    if w1 in model and w2 in model:\n",
        "        similarity = model.similarity(w1, w2)\n",
        "        print(f\"{w1:<15} {w2:<15} {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"{w1:<15} {w2:<15} (not in vocab)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word Analogies\n",
        "print(\"WORD ANALOGIES\")\n",
        "print(\"\\nFormat: A is to B as C is to ?\")\n",
        "print(\"Computed as: vec(B) - vec(A) + vec(C) ≈ vec(?)\\n\")\n",
        "\n",
        "analogies = [\n",
        "    ('king', 'man', 'queen'),        # king - man + queen ≈ woman\n",
        "    ('man', 'woman', 'king'),        # man - woman + king ≈ ?\n",
        "    ('paris', 'france', 'london'),   # paris - france + london ≈ england\n",
        "    ('good', 'better', 'bad'),       # good - better + bad ≈ worse\n",
        "    ('walk', 'walked', 'go'),        # walk - walked + go ≈ went\n",
        "]\n",
        "\n",
        "for a, b, c in analogies:\n",
        "    if all(w in model for w in [a, b, c]):\n",
        "        try:\n",
        "            # Compute: b - a + c\n",
        "            result = model.most_similar(positive=[b, c], negative=[a], topn=3)\n",
        "            \n",
        "            print(f\"{a} - {b} + {c} ≈\")\n",
        "            for word, score in result:\n",
        "                print(f\"  {word:<15} (similarity: {score:.4f})\")\n",
        "            print()\n",
        "        except:\n",
        "            print(f\"{a} - {b} + {c} ≈ (error computing)\")\n",
        "            print()\n",
        "    else:\n",
        "        print(f\"{a} - {b} + {c} ≈ (words not in vocabulary)\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5. Visualization with t-SNE\n",
        "\n",
        "Visualize word embeddings in 2D space using t-SNE dimensionality reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize word embeddings with t-SNE\n",
        "print(\"Creating t-SNE visualization...\")\n",
        "\n",
        "# Select words to visualize\n",
        "words_to_plot = [\n",
        "    # Animals\n",
        "    'dog', 'cat', 'lion', 'tiger', 'elephant',\n",
        "    # Countries\n",
        "    'america', 'china', 'france', 'germany', 'japan',\n",
        "    # Royalty\n",
        "    'king', 'queen', 'prince', 'princess',\n",
        "    # Family\n",
        "    'man', 'woman', 'boy', 'girl', 'father', 'mother',\n",
        "    # Colors\n",
        "    'red', 'blue', 'green', 'yellow',\n",
        "    # Tech\n",
        "    'computer', 'software', 'internet', 'program'\n",
        "]\n",
        "\n",
        "# Filter words that exist in model\n",
        "words_in_vocab = [w for w in words_to_plot if w in model]\n",
        "print(f\"Plotting {len(words_in_vocab)} words...\")\n",
        "\n",
        "# Get word vectors\n",
        "word_vectors = np.array([model[w] for w in words_in_vocab])\n",
        "\n",
        "# Apply t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(words_in_vocab)-1))\n",
        "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# Create plot\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], alpha=0.6, s=100)\n",
        "\n",
        "# Add labels\n",
        "for i, word in enumerate(words_in_vocab):\n",
        "    plt.annotate(word, \n",
        "                xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]),\n",
        "                xytext=(5, 2),\n",
        "                textcoords='offset points',\n",
        "                ha='left',\n",
        "                fontsize=10,\n",
        "                weight='bold')\n",
        "\n",
        "plt.title('Word Embeddings Visualization (t-SNE)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
        "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6. Applications\n",
        "\n",
        "Practical applications of word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Application 1: Sentence Embeddings (Simple Averaging)\n",
        "print(\"APPLICATION 1: SENTENCE SIMILARITY\")\n",
        "\n",
        "def get_sentence_vector(sentence, model):\n",
        "    \"\"\"\n",
        "    Get sentence embedding by averaging word vectors.\n",
        "    \"\"\"\n",
        "    words = sentence.lower().split()\n",
        "    vectors = []\n",
        "    \n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            vectors.append(model[word])\n",
        "    \n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "    \n",
        "    if norm_v1 == 0 or norm_v2 == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    return dot_product / (norm_v1 * norm_v2)\n",
        "\n",
        "# Test sentences\n",
        "sentences = [\n",
        "    \"The cat sits on the mat\",\n",
        "    \"A dog lies on the carpet\",\n",
        "    \"I love programming in Python\",\n",
        "    \"She enjoys coding with Python\",\n",
        "    \"The weather is sunny today\"\n",
        "]\n",
        "\n",
        "print(\"\\nCalculating sentence similarities...\\n\")\n",
        "\n",
        "# Get sentence vectors\n",
        "sent_vectors = [get_sentence_vector(s, model) for s in sentences]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "print(f\"{'Sentence 1':<40} {'Sentence 2':<40} {'Similarity':<10}\")\n",
        "print(\"-\"*90)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i+1, len(sentences)):\n",
        "        sim = cosine_similarity(sent_vectors[i], sent_vectors[j])\n",
        "        s1_short = sentences[i][:37] + \"...\" if len(sentences[i]) > 40 else sentences[i]\n",
        "        s2_short = sentences[j][:37] + \"...\" if len(sentences[j]) > 40 else sentences[j]\n",
        "        print(f\"{s1_short:<40} {s2_short:<40} {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Application 2: Word Clustering\n",
        "print(\"APPLICATION 2: WORD CLUSTERING\")\n",
        "\n",
        "# Select words from different semantic categories\n",
        "cluster_words = [\n",
        "    # Animals\n",
        "    'dog', 'cat', 'lion', 'tiger', 'elephant', 'bird',\n",
        "    # Food\n",
        "    'apple', 'banana', 'orange', 'bread', 'cheese',\n",
        "    # Colors\n",
        "    'red', 'blue', 'green', 'yellow', 'black',\n",
        "    # Numbers\n",
        "    'one', 'two', 'three', 'four', 'five'\n",
        "]\n",
        "\n",
        "# Filter words in vocabulary\n",
        "cluster_words = [w for w in cluster_words if w in model]\n",
        "\n",
        "if len(cluster_words) > 10:\n",
        "    print(f\"\\nClustering {len(cluster_words)} words into semantic groups...\")\n",
        "    \n",
        "    # Get word vectors\n",
        "    word_vecs = np.array([model[w] for w in cluster_words])\n",
        "    \n",
        "    # Apply t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(cluster_words)-1))\n",
        "    word_vecs_2d = tsne.fit_transform(word_vecs)\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Define categories for coloring (if word contains certain patterns)\n",
        "    colors_map = []\n",
        "    for word in cluster_words:\n",
        "        if word in ['dog', 'cat', 'lion', 'tiger', 'elephant', 'bird']:\n",
        "            colors_map.append('red')\n",
        "        elif word in ['apple', 'banana', 'orange', 'bread', 'cheese']:\n",
        "            colors_map.append('green')\n",
        "        elif word in ['red', 'blue', 'green', 'yellow', 'black']:\n",
        "            colors_map.append('blue')\n",
        "        elif word in ['one', 'two', 'three', 'four', 'five']:\n",
        "            colors_map.append('orange')\n",
        "        else:\n",
        "            colors_map.append('gray')\n",
        "    \n",
        "    scatter = plt.scatter(word_vecs_2d[:, 0], word_vecs_2d[:, 1], \n",
        "                         c=colors_map, alpha=0.6, s=150)\n",
        "    \n",
        "    # Add labels\n",
        "    for i, word in enumerate(cluster_words):\n",
        "        plt.annotate(word,\n",
        "                    xy=(word_vecs_2d[i, 0], word_vecs_2d[i, 1]),\n",
        "                    xytext=(5, 2),\n",
        "                    textcoords='offset points',\n",
        "                    ha='left',\n",
        "                    fontsize=11,\n",
        "                    weight='bold')\n",
        "    \n",
        "    # Add legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='red', label='Animals'),\n",
        "        Patch(facecolor='green', label='Food'),\n",
        "        Patch(facecolor='blue', label='Colors'),\n",
        "        Patch(facecolor='orange', label='Numbers')\n",
        "    ]\n",
        "    plt.legend(handles=legend_elements, loc='best')\n",
        "    \n",
        "    plt.title('Word Clustering by Semantic Category', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
        "    plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"Not enough words in vocabulary for clustering\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
