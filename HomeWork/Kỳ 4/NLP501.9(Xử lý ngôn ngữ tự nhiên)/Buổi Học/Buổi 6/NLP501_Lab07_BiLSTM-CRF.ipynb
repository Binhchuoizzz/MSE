{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4076ee",
   "metadata": {},
   "source": [
    "# Lab 07: Named Entity Recognition (NER) with BiLSTM-CRF\n",
    "\n",
    "**Task:** Named Entity Recognition (NER)  \n",
    "**Dataset:** CoNLL-2003 (or custom NER dataset)  \n",
    "**Tags:** PER (Person), ORG (Organization), LOC (Location), MISC  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781228fc",
   "metadata": {},
   "source": [
    "## Part 0: Setup và Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acab7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report as seq_report\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Install seqeval if needed: pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539e88b",
   "metadata": {},
   "source": [
    "## Part 1: BiLSTM-CRF\n",
    "\n",
    "### 1.1. What is CRF (Conditional Random Field)?\n",
    "\n",
    "- Enforces valid tag transitions\n",
    "- Considers the entire sequence globally\n",
    "- Finds the best tag sequence, not individual tags\n",
    "\n",
    "### 1.2. BiLSTM-CRF Architecture\n",
    "\n",
    "```\n",
    "Input:      [Apple, is, a, company]\n",
    "            ↓\n",
    "Embedding:  [e₁, e₂, e₃, e₄]\n",
    "            ↓\n",
    "BiLSTM:     [h₁, h₂, h₃, h₄] ← contextual features\n",
    "            ↓\n",
    "Emission:   [scores for each tag at each position]\n",
    "            ↓\n",
    "CRF Layer:  Transition matrix + Viterbi decoding\n",
    "            ↓\n",
    "Output:     [B-ORG, O, O, O] ← best valid sequence\n",
    "```\n",
    "\n",
    "### 1.3. Components\n",
    "\n",
    "1. **Emission Scores**: P(tag | word) from BiLSTM\n",
    "2. **Transition Scores**: P(tagᵢ₊₁ | tagᵢ) learned by CRF\n",
    "3. **Viterbi Decoding**: Find best path considering both scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139996e",
   "metadata": {},
   "source": [
    "## Part 2: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NER dataset (BIO format)\n",
    "# B-X: Beginning of entity type X\n",
    "# I-X: Inside entity type X  \n",
    "# O: Outside any entity\n",
    "\n",
    "sample_data = [\n",
    "    ([\"Apple\", \"is\", \"looking\", \"at\", \"buying\", \"U.K.\", \"startup\", \"for\", \"$1\", \"billion\"],\n",
    "     [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    \n",
    "    ([\"Tim\", \"Cook\", \"is\", \"the\", \"CEO\", \"of\", \"Apple\", \"Inc.\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\"]),\n",
    "    \n",
    "    ([\"Google\", \"was\", \"founded\", \"in\", \"California\"],\n",
    "     [\"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"John\", \"Smith\", \"works\", \"at\", \"Microsoft\", \"in\", \"Seattle\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"The\", \"meeting\", \"will\", \"be\", \"in\", \"New\", \"York\", \"City\"],\n",
    "     [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\"]),\n",
    "]\n",
    "\n",
    "# Create more synthetic data\n",
    "additional_data = [\n",
    "    ([\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"Amazon\", \"delivers\", \"packages\", \"worldwide\"],\n",
    "     [\"B-ORG\", \"O\", \"O\", \"O\"]),\n",
    "    \n",
    "    ([\"Paris\", \"is\", \"the\", \"capital\", \"of\", \"France\"],\n",
    "     [\"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"Elon\", \"Musk\", \"founded\", \"Tesla\", \"and\", \"SpaceX\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"O\", \"B-ORG\"]),\n",
    "    \n",
    "    ([\"The\", \"United\", \"Nations\", \"is\", \"based\", \"in\", \"Geneva\"],\n",
    "     [\"O\", \"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "]\n",
    "\n",
    "# Combine and split\n",
    "all_data = sample_data + additional_data * 10  # Duplicate for more training data\n",
    "train_size = int(0.8 * len(all_data))\n",
    "train_data = all_data[:train_size]\n",
    "test_data = all_data[train_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\\\nSample:\")\n",
    "for i in range(2):\n",
    "    words, tags = train_data[i]\n",
    "    print(f\"  Words: {' '.join(words)}\")\n",
    "    print(f\"  Tags:  {' '.join(tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.tag2idx = {\"<PAD>\": 0}\n",
    "        self.idx2tag = {0: \"<PAD>\"}\n",
    "        \n",
    "    def build(self, data):\n",
    "        # Build word vocab\n",
    "        words = set()\n",
    "        tags = set()\n",
    "        for sent_words, sent_tags in data:\n",
    "            words.update(sent_words)\n",
    "            tags.update(sent_tags)\n",
    "        \n",
    "        for idx, word in enumerate(sorted(words), start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "        for idx, tag in enumerate(sorted(tags), start=1):\n",
    "            self.tag2idx[tag] = idx\n",
    "            self.idx2tag[idx] = tag\n",
    "            \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        print(f\"Tag set size: {len(self.tag2idx)}\")\n",
    "        print(f\"Tags: {list(self.tag2idx.keys())}\")\n",
    "\n",
    "vocab = Vocab()\n",
    "vocab.build(train_data)\n",
    "\n",
    "# Add special tags for CRF\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "vocab.tag2idx[START_TAG] = len(vocab.tag2idx)\n",
    "vocab.tag2idx[STOP_TAG] = len(vocab.tag2idx)\n",
    "vocab.idx2tag[vocab.tag2idx[START_TAG]] = START_TAG\n",
    "vocab.idx2tag[vocab.tag2idx[STOP_TAG]] = STOP_TAG\n",
    "\n",
    "print(f\"\\\\nFinal tag set (with START/STOP): {list(vocab.tag2idx.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.data[idx]\n",
    "        \n",
    "        word_ids = [self.vocab.word2idx.get(w, self.vocab.word2idx[\"<UNK>\"]) for w in words]\n",
    "        tag_ids = [self.vocab.tag2idx[t] for t in tags]\n",
    "        \n",
    "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for variable length sequences\"\"\"\n",
    "    word_seqs, tag_seqs = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in word_seqs])\n",
    "    \n",
    "    # Pad sequences\n",
    "    word_seqs_padded = pad_sequence(word_seqs, batch_first=True, padding_value=0)\n",
    "    tag_seqs_padded = pad_sequence(tag_seqs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return word_seqs_padded, tag_seqs_padded, lengths\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = NERDataset(train_data, vocab)\n",
    "test_dataset = NERDataset(test_data, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6086dcfa",
   "metadata": {},
   "source": [
    "## Part 3: CRF Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da266cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional Random Field layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_tags, start_tag_idx, stop_tag_idx):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.start_tag_idx = start_tag_idx\n",
    "        self.stop_tag_idx = stop_tag_idx\n",
    "        \n",
    "        # Transition parameters: transitions[i, j] = score of transitioning from tag i to tag j\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        \n",
    "        # Initialize transitions: no transition to START tag, no transition from STOP tag\n",
    "        self.transitions.data[start_tag_idx, :] = -10000\n",
    "        self.transitions.data[:, stop_tag_idx] = -10000\n",
    "    \n",
    "    def forward(self, emissions, tags, mask):\n",
    "        \"\"\"\n",
    "        Compute negative log likelihood loss\n",
    "        \n",
    "        Args:\n",
    "            emissions: [batch_size, seq_len, num_tags] - emission scores from BiLSTM\n",
    "            tags: [batch_size, seq_len] - true tag indices\n",
    "            mask: [batch_size, seq_len] - mask for padding\n",
    "        \n",
    "        Returns:\n",
    "            loss: negative log likelihood\n",
    "        \"\"\"\n",
    "        # Compute score of the true path\n",
    "        gold_score = self._score_sentence(emissions, tags, mask)\n",
    "        \n",
    "        # Compute log partition function (sum over all possible paths)\n",
    "        forward_score = self._forward_algorithm(emissions, mask)\n",
    "        \n",
    "        # Negative log likelihood\n",
    "        nll = forward_score - gold_score\n",
    "        return nll.mean()\n",
    "    \n",
    "    def _score_sentence(self, emissions, tags, mask):\n",
    "        \"\"\"\n",
    "        Compute score of the gold tag sequence\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tags.shape\n",
    "        \n",
    "        score = torch.zeros(batch_size).to(emissions.device)\n",
    "        \n",
    "        # Add START transition\n",
    "        first_tags = tags[:, 0]\n",
    "        score += self.transitions[self.start_tag_idx, first_tags]\n",
    "        \n",
    "        # Add emission and transition scores\n",
    "        for i in range(seq_len):\n",
    "            current_tags = tags[:, i]\n",
    "            emission_score = emissions[:, i].gather(1, current_tags.unsqueeze(1)).squeeze(1)\n",
    "            score += emission_score * mask[:, i]\n",
    "            \n",
    "            if i < seq_len - 1:\n",
    "                next_tags = tags[:, i + 1]\n",
    "                transition_score = self.transitions[current_tags, next_tags]\n",
    "                score += transition_score * mask[:, i + 1]\n",
    "        \n",
    "        # Add STOP transition\n",
    "        last_tag_indices = mask.sum(1).long() - 1\n",
    "        last_tags = tags.gather(1, last_tag_indices.unsqueeze(1)).squeeze(1)\n",
    "        score += self.transitions[last_tags, self.stop_tag_idx]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _forward_algorithm(self, emissions, mask):\n",
    "        \"\"\"\n",
    "        Forward algorithm to compute log partition function\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_tags = emissions.shape\n",
    "        \n",
    "        # Initialize forward variables: alpha[batch, tag]\n",
    "        alpha = torch.full((batch_size, num_tags), -10000.0).to(emissions.device)\n",
    "        alpha[:, self.start_tag_idx] = 0.0\n",
    "        \n",
    "        # Iterate through the sequence\n",
    "        for i in range(seq_len):\n",
    "            emit_score = emissions[:, i]  # [batch, num_tags]\n",
    "            \n",
    "            # Broadcast for all possible transitions\n",
    "            # alpha: [batch, from_tag] -> [batch, from_tag, 1]\n",
    "            # transitions: [from_tag, to_tag] -> [1, from_tag, to_tag]\n",
    "            # emit_score: [batch, to_tag] -> [batch, 1, to_tag]\n",
    "            \n",
    "            alpha_broadcast = alpha.unsqueeze(2)  # [batch, from_tag, 1]\n",
    "            emit_broadcast = emit_score.unsqueeze(1)  # [batch, 1, to_tag]\n",
    "            trans_broadcast = self.transitions.unsqueeze(0)  # [1, from_tag, to_tag]\n",
    "            \n",
    "            # Compute scores for all transitions\n",
    "            next_alpha = alpha_broadcast + trans_broadcast + emit_broadcast  # [batch, from_tag, to_tag]\n",
    "            \n",
    "            # Log-sum-exp over from_tag dimension\n",
    "            next_alpha = torch.logsumexp(next_alpha, dim=1)  # [batch, to_tag]\n",
    "            \n",
    "            # Apply mask\n",
    "            alpha = next_alpha * mask[:, i].unsqueeze(1) + alpha * (1 - mask[:, i].unsqueeze(1))\n",
    "        \n",
    "        # Add transition to STOP tag\n",
    "        alpha = alpha + self.transitions[:, self.stop_tag_idx].unsqueeze(0)\n",
    "        \n",
    "        # Log-sum-exp over all tags\n",
    "        return torch.logsumexp(alpha, dim=1)\n",
    "    \n",
    "    def decode(self, emissions, mask):\n",
    "        \"\"\"\n",
    "        Viterbi decoding to find the best tag sequence\n",
    "        \n",
    "        Args:\n",
    "            emissions: [batch_size, seq_len, num_tags]\n",
    "            mask: [batch_size, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            best_paths: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_tags = emissions.shape\n",
    "        \n",
    "        # Initialize viterbi variables\n",
    "        viterbi = torch.full((batch_size, num_tags), -10000.0).to(emissions.device)\n",
    "        viterbi[:, self.start_tag_idx] = 0.0\n",
    "        \n",
    "        # Backpointers\n",
    "        backpointers = []\n",
    "        \n",
    "        # Forward pass\n",
    "        for i in range(seq_len):\n",
    "            emit_score = emissions[:, i]\n",
    "            \n",
    "            # Broadcast for all transitions\n",
    "            viterbi_broadcast = viterbi.unsqueeze(2)  # [batch, from_tag, 1]\n",
    "            trans_broadcast = self.transitions.unsqueeze(0)  # [1, from_tag, to_tag]\n",
    "            emit_broadcast = emit_score.unsqueeze(1)  # [batch, 1, to_tag]\n",
    "            \n",
    "            # Compute scores\n",
    "            next_scores = viterbi_broadcast + trans_broadcast + emit_broadcast  # [batch, from_tag, to_tag]\n",
    "            \n",
    "            # Find best previous tag\n",
    "            next_viterbi, best_tags = next_scores.max(dim=1)  # [batch, to_tag]\n",
    "            \n",
    "            backpointers.append(best_tags)\n",
    "            \n",
    "            # Apply mask\n",
    "            viterbi = next_viterbi * mask[:, i].unsqueeze(1) + viterbi * (1 - mask[:, i].unsqueeze(1))\n",
    "        \n",
    "        # Add transition to STOP\n",
    "        viterbi = viterbi + self.transitions[:, self.stop_tag_idx].unsqueeze(0)\n",
    "        \n",
    "        # Find best last tag\n",
    "        _, best_last_tags = viterbi.max(dim=1)  # [batch]\n",
    "        \n",
    "        # Backward pass to reconstruct paths\n",
    "        best_paths = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            path = [best_last_tags[batch_idx].item()]\n",
    "            \n",
    "            # Backtrack\n",
    "            for i in range(seq_len - 1, 0, -1):\n",
    "                if mask[batch_idx, i] == 0:\n",
    "                    continue\n",
    "                prev_tag = backpointers[i][batch_idx, path[-1]].item()\n",
    "                path.append(prev_tag)\n",
    "            \n",
    "            path.reverse()\n",
    "            \n",
    "            # Pad to seq_len\n",
    "            while len(path) < seq_len:\n",
    "                path.append(0)  # PAD tag\n",
    "            \n",
    "            best_paths.append(path)\n",
    "        \n",
    "        return torch.tensor(best_paths).to(emissions.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c2eee",
   "metadata": {},
   "source": [
    "## Part 4: BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1842d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    \"\"\"\n",
    "    BiLSTM-CRF model for sequence labeling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim=100, hidden_dim=128, \n",
    "                 num_layers=1, dropout=0.5, start_tag_idx=None, stop_tag_idx=None):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Emission layer: BiLSTM output -> tag scores\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tag_size)\n",
    "        \n",
    "        # CRF layer\n",
    "        self.crf = CRF(tag_size, start_tag_idx, stop_tag_idx)\n",
    "    \n",
    "    def forward(self, sentences, tags, lengths):\n",
    "        \"\"\"\n",
    "        Forward pass for training\n",
    "        \n",
    "        Args:\n",
    "            sentences: [batch_size, seq_len]\n",
    "            tags: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            loss: negative log likelihood\n",
    "        \"\"\"\n",
    "        # Get emission scores from BiLSTM\n",
    "        emissions = self._get_emissions(sentences, lengths)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = self._create_mask(sentences, lengths)\n",
    "        \n",
    "        # Compute CRF loss\n",
    "        loss = self.crf(emissions, tags, mask)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _get_emissions(self, sentences, lengths):\n",
    "        \"\"\"Get emission scores from BiLSTM\"\"\"\n",
    "        # Embedding\n",
    "        embeds = self.embedding(sentences)  # [batch, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed_embeds = pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # BiLSTM\n",
    "        packed_lstm_out, _ = self.bilstm(packed_embeds)\n",
    "        \n",
    "        # Unpack\n",
    "        lstm_out, _ = pad_packed_sequence(packed_lstm_out, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Emission scores\n",
    "        emissions = self.hidden2tag(lstm_out)  # [batch, seq_len, num_tags]\n",
    "        \n",
    "        return emissions\n",
    "    \n",
    "    def _create_mask(self, sentences, lengths):\n",
    "        \"\"\"Create mask for padded positions\"\"\"\n",
    "        batch_size, seq_len = sentences.shape\n",
    "        mask = torch.zeros(batch_size, seq_len).to(sentences.device)\n",
    "        \n",
    "        for i, length in enumerate(lengths):\n",
    "            mask[i, :length] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def predict(self, sentences, lengths):\n",
    "        \"\"\"\n",
    "        Predict tags using Viterbi decoding\n",
    "        \n",
    "        Args:\n",
    "            sentences: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            predicted_tags: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # Get emission scores\n",
    "        emissions = self._get_emissions(sentences, lengths)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = self._create_mask(sentences, lengths)\n",
    "        \n",
    "        # Viterbi decoding\n",
    "        best_paths = self.crf.decode(emissions, mask)\n",
    "        \n",
    "        return best_paths\n",
    "\n",
    "# Create model\n",
    "VOCAB_SIZE = len(vocab.word2idx)\n",
    "TAG_SIZE = len(vocab.tag2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.3\n",
    "\n",
    "START_TAG_IDX = vocab.tag2idx[START_TAG]\n",
    "STOP_TAG_IDX = vocab.tag2idx[STOP_TAG]\n",
    "\n",
    "model = BiLSTM_CRF(\n",
    "    VOCAB_SIZE, TAG_SIZE, EMBEDDING_DIM, HIDDEN_DIM,\n",
    "    NUM_LAYERS, DROPOUT, START_TAG_IDX, STOP_TAG_IDX\n",
    ").to(device)\n",
    "\n",
    "print(\"BiLSTM-CRF Model\")\n",
    "print(model)\n",
    "print(f\"\\\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb8ee5",
   "metadata": {},
   "source": [
    "## Part 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for sentences, tags, lengths in tqdm(dataloader, desc=\"Training\"):\n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(sentences, tags, lengths)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, vocab, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentences, tags, lengths in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            sentences = sentences.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            # Predict\n",
    "            pred_tags = model.predict(sentences, lengths)\n",
    "            \n",
    "            # Convert to tag names for seqeval\n",
    "            for i, length in enumerate(lengths):\n",
    "                true_tags = [vocab.idx2tag[t.item()] for t in tags[i, :length]]\n",
    "                pred_tag_ids = [t.item() for t in pred_tags[i, :length]]\n",
    "                pred_tag_names = [vocab.idx2tag[t] for t in pred_tag_ids]\n",
    "                \n",
    "                # Filter out PAD, START, STOP tags\n",
    "                true_tags_filtered = [t for t in true_tags if t not in ['<PAD>', START_TAG, STOP_TAG]]\n",
    "                pred_tags_filtered = [t for t in pred_tag_names if t not in ['<PAD>', START_TAG, STOP_TAG]]\n",
    "                \n",
    "                if len(true_tags_filtered) > 0:\n",
    "                    all_true.append(true_tags_filtered)\n",
    "                    all_preds.append(pred_tags_filtered[:len(true_tags_filtered)])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(all_true, all_preds)\n",
    "    recall = recall_score(all_true, all_preds)\n",
    "    f1 = f1_score(all_true, all_preds)\n",
    "    \n",
    "    return precision, recall, f1, all_true, all_preds\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "history = {'train_loss': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "best_f1 = 0\n",
    "\n",
    "print(\"Training BiLSTM-CRF\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"\\\\nEpoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    precision, recall, f1, _, _ = evaluate(model, test_loader, vocab, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['precision'].append(precision)\n",
    "    history['recall'].append(recall)\n",
    "    history['f1'].append(f1)\n",
    "    \n",
    "    print(f\"\\\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), 'bilstm_crf_best.pt')\n",
    "        print(f\" Saved best model (F1: {best_f1:.4f})\")\n",
    "\n",
    "print(f\"Training Complete! Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41790c",
   "metadata": {},
   "source": [
    "## Part 6: Visualization và Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(1, N_EPOCHS + 1)\n",
    "\n",
    "# Loss plot\n",
    "ax = axes[0]\n",
    "ax.plot(epochs, history['train_loss'], 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics plot\n",
    "ax = axes[1]\n",
    "ax.plot(epochs, history['precision'], 'r-', label='Precision', linewidth=2)\n",
    "ax.plot(epochs, history['recall'], 'g-', label='Recall', linewidth=2)\n",
    "ax.plot(epochs, history['f1'], 'b-', label='F1-Score', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Evaluation Metrics', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('bilstm_crf_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c675a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "model.load_state_dict(torch.load('bilstm_crf_best.pt'))\n",
    "_, _, _, all_true, all_preds = evaluate(model, test_loader, vocab, device)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(seq_report(all_true, all_preds))\n",
    "\n",
    "# Sample predictions\n",
    "def predict_sentence(model, sentence, vocab, device):\n",
    "    \"\"\"Predict NER tags for a sentence\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize (simple space split)\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Convert to indices\n",
    "    word_ids = [vocab.word2idx.get(w, vocab.word2idx[\"<UNK>\"]) for w in words]\n",
    "    \n",
    "    # Create tensors\n",
    "    sentence_tensor = torch.tensor([word_ids]).to(device)\n",
    "    length_tensor = torch.tensor([len(word_ids)]).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred_tags = model.predict(sentence_tensor, length_tensor)\n",
    "    \n",
    "    # Convert to tag names\n",
    "    pred_tag_names = [vocab.idx2tag[t.item()] for t in pred_tags[0, :len(words)]]\n",
    "    \n",
    "    return list(zip(words, pred_tag_names))\n",
    "\n",
    "# Test on sample sentences\n",
    "print(\"Sample Predictions\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"Apple is looking at buying U.K. startup\",\n",
    "    \"Tim Cook is the CEO of Apple Inc.\",\n",
    "    \"Google was founded in California\",\n",
    "    \"John Smith works at Microsoft in Seattle\"\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    result = predict_sentence(model, sent, vocab, device)\n",
    "    print(f\"\\\\nSentence: {sent}\")\n",
    "    print(\"Predictions:\")\n",
    "    for word, tag in result:\n",
    "        if tag != 'O':\n",
    "            print(f\"  {word:<15} -> {tag}\")\n",
    "    if not any(tag != 'O' for _, tag in result):\n",
    "        print(\"  (No entities detected)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
