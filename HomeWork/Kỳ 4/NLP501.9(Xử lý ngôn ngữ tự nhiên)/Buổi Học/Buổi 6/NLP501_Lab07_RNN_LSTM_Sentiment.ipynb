{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP501 - Lab 07: Sentiment Analysis with RNN và LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Setup và Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_imdb_dataset(data_dir='./data'):\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    filepath = os.path.join(data_dir, \"aclImdb_v1.tar.gz\")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir, 'aclImdb')):\n",
    "        print(\"Downloading IMDB dataset...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(filepath, 'r:gz') as tar:\n",
    "            tar.extractall(data_dir)\n",
    "        os.remove(filepath)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists.\")\n",
    "    \n",
    "    return os.path.join(data_dir, 'aclImdb')\n",
    "\n",
    "data_path = download_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_path, split='train'):\n",
    "    \"\"\"Load IMDB data from directory\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for label_type in ['pos', 'neg']:\n",
    "        dir_path = os.path.join(data_path, split, label_type)\n",
    "        label = 1 if label_type == 'pos' else 0\n",
    "        \n",
    "        for filename in os.listdir(dir_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "                    labels.append(label)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_texts, train_labels = load_imdb_data(data_path, 'train')\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_texts, test_labels = load_imdb_data(data_path, 'test')\n",
    "print(f\"Test samples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: lowercase, remove HTML tags, tokenize\n",
    "    \n",
    "    Args:\n",
    "        text: raw text string\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # 3. Keep only alphanumeric characters and spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # 4. Remove extra whitespace and split\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test\n",
    "sample = \"This is a <br />GREAT movie! I loved it... 10/10\"\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Preprocessed: {preprocess_text(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Vocabulary class to map words to indices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=25000, min_freq=2):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_freq = Counter()\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        Build vocabulary from list of texts\n",
    "        \"\"\"\n",
    "        # Count word frequencies\n",
    "        print(\"Counting word frequencies...\")\n",
    "        for text in tqdm(texts):\n",
    "            tokens = preprocess_text(text)\n",
    "            self.word_freq.update(tokens)\n",
    "        \n",
    "        # Filter by min_freq and get most common\n",
    "        filtered_words = [\n",
    "            word for word, freq in self.word_freq.items() \n",
    "            if freq >= self.min_freq\n",
    "        ]\n",
    "        \n",
    "        # Sort by frequency (descending)\n",
    "        sorted_words = sorted(\n",
    "            filtered_words, \n",
    "            key=lambda w: self.word_freq[w], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Limit vocab size (subtract 2 for PAD and UNK)\n",
    "        vocab_words = sorted_words[:self.max_vocab_size - 2]\n",
    "        \n",
    "        # Build mappings\n",
    "        for idx, word in enumerate(vocab_words, start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        \n",
    "        print(f\"Vocabulary built: {len(self.word2idx)} words\")\n",
    "    \n",
    "    def text_to_indices(self, text):\n",
    "        \"\"\"\n",
    "        Convert text to list of indices\n",
    "        \"\"\"\n",
    "        tokens = preprocess_text(text)\n",
    "        indices = [\n",
    "            self.word2idx.get(token, self.word2idx['<UNK>'])\n",
    "            for token in tokens\n",
    "        ]\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "vocab = Vocabulary(max_vocab_size=25000, min_freq=2)\n",
    "vocab.build_vocab(train_texts)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Test\n",
    "sample_text = \"This movie is great!\"\n",
    "print(f\"\\nSample: '{sample_text}'\")\n",
    "print(f\"Indices: {vocab.text_to_indices(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset cho IMDB\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab, max_length=256):\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.sequences = []\n",
    "        for text in tqdm(texts, desc=\"Converting texts\"):\n",
    "            indices = vocab.text_to_indices(text)\n",
    "            if len(indices) > max_length:\n",
    "                indices = indices[:max_length]\n",
    "            self.sequences.append(indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.sequences[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to pad sequences in batch\n",
    "    \"\"\"\n",
    "    # Separate sequences and labels\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Get actual lengths\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    \n",
    "    # Pad sequences (pad_sequence expects list of tensors)\n",
    "    padded_sequences = pad_sequence(\n",
    "        sequences, \n",
    "        batch_first=True, \n",
    "        padding_value=0  # PAD index\n",
    "    )\n",
    "    \n",
    "    # Stack labels\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return padded_sequences, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = IMDBDataset(train_texts, train_labels, vocab, MAX_LENGTH)\n",
    "\n",
    "print(\"Creating test dataset...\")\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, vocab, MAX_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-based Sentiment Classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.5, pad_idx=0):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # FC layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            text: [batch_size, seq_len]\n",
    "            text_lengths: [batch_size]\n",
    "        \"\"\"\n",
    "        # Embedding: [batch_size, seq_len, embedding_dim]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, \n",
    "            text_lengths.cpu(), \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Get final hidden state from last layer\n",
    "        final_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Dropout + FC + Sigmoid\n",
    "        output = self.dropout(final_hidden)\n",
    "        output = self.fc(output)  # [batch_size, 1]\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output.squeeze(1)  # [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based Sentiment Classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers=1, bidirectional=False, dropout=0.5, pad_idx=0):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # FC layer (hidden_dim * 2 if bidirectional)\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            text: [batch_size, seq_len]\n",
    "            text_lengths: [batch_size]\n",
    "        \"\"\"\n",
    "        # Embedding: [batch_size, seq_len, embedding_dim]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, \n",
    "            text_lengths.cpu(), \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden: [n_layers * num_directions, batch_size, hidden_dim]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward hidden states from last layer\n",
    "            # Forward: hidden[-2], Backward: hidden[-1]\n",
    "            final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "            # final_hidden: [batch_size, hidden_dim * 2]\n",
    "        else:\n",
    "            # Get final hidden state from last layer\n",
    "            final_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Dropout + FC + Sigmoid\n",
    "        output = self.dropout(final_hidden)\n",
    "        output = self.fc(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = vocab.word2idx['<PAD>']\n",
    "\n",
    "# Create models\n",
    "rnn_model = RNNClassifier(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "    N_LAYERS, DROPOUT, PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "lstm_model = LSTMClassifier(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "    N_LAYERS, bidirectional=False, dropout=DROPOUT, pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "bilstm_model = LSTMClassifier(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "    N_LAYERS, bidirectional=True, dropout=DROPOUT, pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(f\"RNN parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\")\n",
    "print(f\"LSTM parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n",
    "print(f\"BiLSTM parameters: {sum(p.numel() for p in bilstm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Training và Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Get data\n",
    "        sequences, labels, lengths = batch\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(sequences, lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (important for RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        predicted_labels = (predictions >= 0.5).float()\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return epoch_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Get data\n",
    "            sequences, labels, lengths = batch\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(sequences, lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Track predictions\n",
    "            predicted_labels = (predictions >= 0.5).float()\n",
    "            all_preds.extend(predicted_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss / len(dataloader), accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, n_epochs, lr, device, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Complete training loop\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pt')\n",
    "            print(f\"✓ Saved best model with accuracy: {best_acc*100:.2f}%\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"Training RNN Model\")\n",
    "rnn_history = train_model(\n",
    "    rnn_model, train_loader, test_loader,\n",
    "    N_EPOCHS, LEARNING_RATE, device, \"rnn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize and train LSTM\n",
    "lstm_model = LSTMClassifier(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "    N_LAYERS, bidirectional=False, dropout=DROPOUT, pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(\"Training LSTM Model\")\n",
    "lstm_history = train_model(\n",
    "    lstm_model, train_loader, test_loader,\n",
    "    N_EPOCHS, LEARNING_RATE, device, \"lstm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize and train BiLSTM\n",
    "bilstm_model = LSTMClassifier(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "    N_LAYERS, bidirectional=True, dropout=DROPOUT, pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(\"Training Bidirectional LSTM Model\")\n",
    "bilstm_history = train_model(\n",
    "    bilstm_model, train_loader, test_loader,\n",
    "    N_EPOCHS, LEARNING_RATE, device, \"bilstm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Visualization và Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories, labels):\n",
    "    \"\"\"Plot training history comparison\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "    \n",
    "    # Loss\n",
    "    ax = axes[0]\n",
    "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
    "        epochs = range(1, len(hist['train_loss']) + 1)\n",
    "        ax.plot(epochs, hist['train_loss'], '--', color=colors[i], label=f'{label} Train')\n",
    "        ax.plot(epochs, hist['test_loss'], '-', color=colors[i], label=f'{label} Test')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training & Test Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax = axes[1]\n",
    "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
    "        epochs = range(1, len(hist['train_acc']) + 1)\n",
    "        ax.plot(epochs, [acc*100 for acc in hist['train_acc']], '--', color=colors[i], label=f'{label} Train')\n",
    "        ax.plot(epochs, [acc*100 for acc in hist['test_acc']], '-', color=colors[i], label=f'{label} Test')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Training & Test Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('training_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(\n",
    "    [rnn_history, lstm_history, bilstm_history],\n",
    "    ['RNN', 'LSTM', 'BiLSTM']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(f\"{'Model':<15} {'Parameters':>15} {'Best Test Acc':>15}\")\n",
    "print(f\"{'RNN':<15} {sum(p.numel() for p in rnn_model.parameters()):>15,} {max(rnn_history['test_acc'])*100:>14.2f}%\")\n",
    "print(f\"{'LSTM':<15} {sum(p.numel() for p in lstm_model.parameters()):>15,} {max(lstm_history['test_acc'])*100:>14.2f}%\")\n",
    "print(f\"{'BiLSTM':<15} {sum(p.numel() for p in bilstm_model.parameters()):>15,} {max(bilstm_history['test_acc'])*100:>14.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based Sentiment Classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers=1, bidirectional=False, dropout=0.5, pad_idx=0):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # FC layer\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Pack\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, \n",
    "            text_lengths.cpu(), \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # GRU (only hidden, no cell state unlike LSTM)\n",
    "        packed_output, hidden = self.gru(packed_embedded)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        else:\n",
    "            final_hidden = hidden[-1]\n",
    "        \n",
    "        # Output\n",
    "        output = self.dropout(final_hidden)\n",
    "        output = self.fc(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output.squeeze(1)\n",
    "\n",
    "# Test GRU\n",
    "gru_model = GRUClassifier(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "    N_LAYERS, bidirectional=True, dropout=DROPOUT, pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(f\"GRU parameters: {sum(p.numel() for p in gru_model.parameters()):,}\")\n",
    "print(\"Note: GRU has ~25% fewer parameters than LSTM due to having 2 gates instead of 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, vocab, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Load pretrained GloVe embeddings\n",
    "    \n",
    "    Args:\n",
    "        glove_path: path to glove.6B.100d.txt\n",
    "        vocab: Vocabulary object\n",
    "        embedding_dim: embedding dimension\n",
    "    \n",
    "    Returns:\n",
    "        embedding_matrix: numpy array [vocab_size, embedding_dim]\n",
    "    \"\"\"\n",
    "    # Initialize with random values\n",
    "    embedding_matrix = np.random.randn(len(vocab), embedding_dim) * 0.01\n",
    "    \n",
    "    # Set PAD to zeros\n",
    "    embedding_matrix[0] = np.zeros(embedding_dim)\n",
    "    \n",
    "    # Load GloVe\n",
    "    glove_dict = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            glove_dict[word] = vector\n",
    "    \n",
    "    # Fill embedding matrix\n",
    "    found = 0\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        if word in glove_dict:\n",
    "            embedding_matrix[idx] = glove_dict[word]\n",
    "            found += 1\n",
    "    \n",
    "    print(f\"Found {found}/{len(vocab)} words in GloVe\")\n",
    "    return embedding_matrix\n",
    "\n",
    "# Usage (uncomment after downloading GloVe):\n",
    "# embeddings = load_glove_embeddings('glove.6B.100d.txt', vocab)\n",
    "# model.embedding.weight.data.copy_(torch.from_numpy(embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
