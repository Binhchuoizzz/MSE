{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edaf5a8c",
   "metadata": {},
   "source": [
    "# Lab 070-01: Word2Vec Embeddings with Deep Learning Models\n",
    "## Sentiment Analysis với MLP, RNN, LSTM, GRU\n",
    "\n",
    "**Task:** Binary Sentiment Classification (Positive/Negative)  \n",
    "**Dataset:** IMDB Movie Reviews  \n",
    "**Embeddings:** Word2Vec (Skip-gram & CBOW)  \n",
    "**Models:** MLP, RNN, LSTM, GRU  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd50d2",
   "metadata": {},
   "source": [
    "## Part 0: Setup và Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe927b",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = [\n",
    "    \"this movie is absolutely amazing and wonderful\",\n",
    "    \"i loved every minute of this film it was fantastic\",\n",
    "    \"brilliant performance by all actors highly recommend\",\n",
    "    \"one of the best movies i have ever seen\",\n",
    "    \"outstanding cinematography and great story\",\n",
    "    \"superb acting and excellent direction\",\n",
    "    \"thoroughly enjoyed this masterpiece\",\n",
    "    \"fantastic movie with incredible visuals\",\n",
    "    \"amazing storyline and perfect execution\",\n",
    "    \"loved it absolutely brilliant work\",\n",
    "    \"excellent movie highly entertaining\",\n",
    "    \"wonderful film with great performances\",\n",
    "    \"best movie of the year must watch\",\n",
    "    \"incredible acting and beautiful scenes\",\n",
    "    \"perfect movie loved every second\",\n",
    "    \"amazing experience highly recommended\",\n",
    "    \"brilliant film with excellent cast\",\n",
    "    \"fantastic story and great acting\",\n",
    "    \"wonderful movie enjoyed it thoroughly\",\n",
    "    \"outstanding film one of the best\",\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"this movie is terrible waste of time\",\n",
    "    \"horrible acting and boring plot\",\n",
    "    \"worst film i have ever watched\",\n",
    "    \"completely disappointed terrible movie\",\n",
    "    \"awful storyline and bad direction\",\n",
    "    \"boring movie fell asleep halfway\",\n",
    "    \"terrible acting worst experience\",\n",
    "    \"horrible film total disaster\",\n",
    "    \"awful movie do not watch\",\n",
    "    \"worst performance terrible script\",\n",
    "    \"boring and poorly executed film\",\n",
    "    \"terrible movie complete waste\",\n",
    "    \"awful acting and bad story\",\n",
    "    \"horrible film very disappointing\",\n",
    "    \"worst movie terrible experience\",\n",
    "    \"boring plot and bad acting\",\n",
    "    \"awful film not recommended\",\n",
    "    \"terrible storyline horrible movie\",\n",
    "    \"worst acting disappointing film\",\n",
    "    \"boring waste of time awful\",\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "texts = positive_reviews * 10 + negative_reviews * 10  # 200 positive, 200 negative\n",
    "labels = [1] * len(positive_reviews) * 10 + [0] * len(negative_reviews) * 10\n",
    "\n",
    "# Shuffle\n",
    "from sklearn.utils import shuffle\n",
    "texts, labels = shuffle(texts, labels, random_state=SEED)\n",
    "\n",
    "# Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=SEED)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED)\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Val samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nSample reviews:\")\n",
    "for i in range(3):\n",
    "    label = \"Positive\" if y_train[i] == 1 else \"Negative\"\n",
    "    print(f\"  [{label}] {X_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Simple text preprocessing\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize all texts\n",
    "train_tokens = [preprocess_text(text) for text in X_train]\n",
    "val_tokens = [preprocess_text(text) for text in X_val]\n",
    "test_tokens = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "print(\"Tokenized examples:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {train_tokens[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91698b6e",
   "metadata": {},
   "source": [
    "## Part 2: Training Word2Vec Embeddings\n",
    "\n",
    "We'll train our own Word2Vec model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffefd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Word2Vec model...\")\n",
    "\n",
    "# Combine all tokens for training\n",
    "all_tokens = train_tokens + val_tokens + test_tokens\n",
    "\n",
    "# Skip-gram model\n",
    "w2v_sg = Word2Vec(\n",
    "    sentences=all_tokens,\n",
    "    vector_size=100,      # Embedding dimension\n",
    "    window=5,             # Context window\n",
    "    min_count=1,          # Minimum word frequency\n",
    "    sg=1,                 # 1 = Skip-gram, 0 = CBOW\n",
    "    workers=4,\n",
    "    epochs=20,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# CBOW model (for comparison)\n",
    "w2v_cbow = Word2Vec(\n",
    "    sentences=all_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=0,                 # CBOW\n",
    "    workers=4,\n",
    "    epochs=20,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Skip-gram vocabulary size: {len(w2v_sg.wv)}\")\n",
    "print(f\"CBOW vocabulary size: {len(w2v_cbow.wv)}\")\n",
    "print(f\"Embedding dimension: {w2v_sg.vector_size}\")\n",
    "\n",
    "# Test word similarities\n",
    "print(\"\\nWord Similarities (Skip-gram):\")\n",
    "test_words = ['great', 'excellent', 'terrible', 'awful']\n",
    "for word in test_words:\n",
    "    if word in w2v_sg.wv:\n",
    "        similar = w2v_sg.wv.most_similar(word, topn=3)\n",
    "        print(f\"  {word}: {[w for w, _ in similar]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary and create embedding matrix\n",
    "class Vocabulary:\n",
    "    def __init__(self, w2v_model):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        \n",
    "        # Add words from Word2Vec vocabulary\n",
    "        for idx, word in enumerate(w2v_model.wv.index_to_key, start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        \n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.embedding_dim = w2v_model.vector_size\n",
    "        \n",
    "        # Create embedding matrix\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "        \n",
    "        # Fill with Word2Vec vectors\n",
    "        for word, idx in self.word2idx.items():\n",
    "            if word in w2v_model.wv:\n",
    "                self.embedding_matrix[idx] = w2v_model.wv[word]\n",
    "            elif word == \"<UNK>\":\n",
    "                # Random vector for unknown words\n",
    "                self.embedding_matrix[idx] = np.random.randn(self.embedding_dim) * 0.01\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Embedding dimension: {self.embedding_dim}\")\n",
    "    \n",
    "    def encode(self, tokens):\n",
    "        \"\"\"Convert tokens to indices\"\"\"\n",
    "        return [self.word2idx.get(token, self.word2idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "# Create vocabulary with Skip-gram embeddings\n",
    "vocab = Vocabulary(w2v_sg)\n",
    "print(f\"\\nEmbedding matrix shape: {vocab.embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, tokens_list, labels, vocab):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Encode tokens to indices\n",
    "        indices = self.vocab.encode(tokens)\n",
    "        \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for variable length sequences\"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    \n",
    "    # Pad sequences\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return sequences_padded, labels, lengths\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SentimentDataset(train_tokens, y_train, vocab)\n",
    "val_dataset = SentimentDataset(val_tokens, y_val, vocab)\n",
    "test_dataset = SentimentDataset(test_tokens, y_test, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dcdb56",
   "metadata": {},
   "source": [
    "## Part 3: Model Implementations\n",
    "\n",
    "### 3.1. Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for Sentiment Classification\n",
    "    \n",
    "    Architecture:\n",
    "    Word2Vec Embedding → Average Pooling → MLP → Sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, dropout=0.5, freeze_embeddings=True):\n",
    "        super(MLP_Classifier, self).__init__()\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Initialize embedding layer with Word2Vec weights\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        \n",
    "        # Freeze or fine-tune embeddings\n",
    "        self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # Average pooling (ignore padding)\n",
    "        mask = (x != 0).float().unsqueeze(-1)  # [batch, seq_len, 1]\n",
    "        embedded_masked = embedded * mask\n",
    "        pooled = embedded_masked.sum(dim=1) / lengths.unsqueeze(1).float()  # [batch, embed_dim]\n",
    "        \n",
    "        # MLP\n",
    "        out = self.relu(self.fc1(pooled))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out).squeeze(1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e6fa3",
   "metadata": {},
   "source": [
    "### 3.2. Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla RNN for Sentiment Classification\n",
    "    \n",
    "    Architecture:\n",
    "    Word2Vec Embedding → RNN → Last hidden state → FC → Sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_layers=2, \n",
    "                 dropout=0.5, freeze_embeddings=True):\n",
    "        super(RNN_Classifier, self).__init__()\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # RNN\n",
    "        packed_out, hidden = self.rnn(packed)\n",
    "        \n",
    "        # Use last hidden state\n",
    "        final_hidden = hidden[-1]  # [batch, hidden_dim]\n",
    "        \n",
    "        # FC layer\n",
    "        out = self.dropout(final_hidden)\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out).squeeze(1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a3801",
   "metadata": {},
   "source": [
    "### 3.3. LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM for Sentiment Classification\n",
    "    \n",
    "    Architecture:\n",
    "    Word2Vec Embedding → LSTM → Last hidden state → FC → Sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_layers=2, \n",
    "                 dropout=0.5, freeze_embeddings=True):\n",
    "        super(LSTM_Classifier, self).__init__()\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM\n",
    "        packed_out, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Use last hidden state\n",
    "        final_hidden = hidden[-1]  # [batch, hidden_dim]\n",
    "        \n",
    "        # FC layer\n",
    "        out = self.dropout(final_hidden)\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out).squeeze(1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7e7b7",
   "metadata": {},
   "source": [
    "### 3.4. GRU Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU for Sentiment Classification\n",
    "    \n",
    "    Architecture:\n",
    "    Word2Vec Embedding → GRU → Last hidden state → FC → Sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_layers=2, \n",
    "                 dropout=0.5, freeze_embeddings=True):\n",
    "        super(GRU_Classifier, self).__init__()\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # GRU\n",
    "        packed_out, hidden = self.gru(packed)\n",
    "        \n",
    "        # Use last hidden state\n",
    "        final_hidden = hidden[-1]  # [batch, hidden_dim]\n",
    "        \n",
    "        # FC layer\n",
    "        out = self.dropout(final_hidden)\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out).squeeze(1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110551a",
   "metadata": {},
   "source": [
    "## Part 4: Training và Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels, lengths in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = (outputs >= 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels, lengths in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store predictions\n",
    "            predictions = (outputs >= 0.5).float()\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547eac9",
   "metadata": {},
   "source": [
    "## Part 5: Training All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfdd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "N_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Models to train\n",
    "embedding_matrix = vocab.embedding_matrix\n",
    "\n",
    "models_to_train = {\n",
    "    'MLP': MLP_Classifier(embedding_matrix, HIDDEN_DIM, DROPOUT, freeze_embeddings=True),\n",
    "    'RNN': RNN_Classifier(embedding_matrix, HIDDEN_DIM, NUM_LAYERS, DROPOUT, freeze_embeddings=True),\n",
    "    'LSTM': LSTM_Classifier(embedding_matrix, HIDDEN_DIM, NUM_LAYERS, DROPOUT, freeze_embeddings=True),\n",
    "    'GRU': GRU_Classifier(embedding_matrix, HIDDEN_DIM, NUM_LAYERS, DROPOUT, freeze_embeddings=True)\n",
    "}\n",
    "\n",
    "print(\"Model Comparison (with Word2Vec Embeddings)\")\n",
    "for name, model in models_to_train.items():\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{name:<10} - Trainable Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models_to_train.items():\n",
    "    print(f\"Training {model_name} Model\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    training_time = 0\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        training_time += epoch_time\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'{model_name.lower()}_w2v_best.pt')\n",
    "            print(f\"  Saved best model (Val Acc: {best_val_acc:.4f})\")\n",
    "    \n",
    "    # Test on best model\n",
    "    model.load_state_dict(torch.load(f'{model_name.lower()}_w2v_best.pt'))\n",
    "    test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'history': history,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'training_time': training_time,\n",
    "        'predictions': test_preds,\n",
    "        'labels': test_labels,\n",
    "        'n_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Training Complete!\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.4f}\")\n",
    "    print(f\"  Test Acc: {test_acc:.4f}\")\n",
    "    print(f\"  Total Training Time: {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70824902",
   "metadata": {},
   "source": [
    "## Part 7: Visualization và Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "epochs = range(1, N_EPOCHS + 1)\n",
    "model_names = ['MLP', 'RNN', 'LSTM', 'GRU']\n",
    "\n",
    "# Training Loss\n",
    "ax = axes[0, 0]\n",
    "for name in model_names:\n",
    "    ax.plot(epochs, results[name]['history']['train_loss'], \n",
    "            label=name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "ax = axes[0, 1]\n",
    "for name in model_names:\n",
    "    ax.plot(epochs, results[name]['history']['val_loss'], \n",
    "            label=name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy\n",
    "ax = axes[1, 0]\n",
    "for name in model_names:\n",
    "    ax.plot(epochs, results[name]['history']['train_acc'], \n",
    "            label=name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "ax = axes[1, 1]\n",
    "for name in model_names:\n",
    "    ax.plot(epochs, results[name]['history']['val_acc'], \n",
    "            label=name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('w2v_models_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3324ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison table\n",
    "print(\"MODEL COMPARISON SUMMARY (Word2Vec Embeddings)\")\n",
    "print(f\"{'Model':<10} {'Params':<15} {'Train Time':<15} {'Best Val Acc':<15} {'Test Acc':<12}\")\n",
    "\n",
    "for name in model_names:\n",
    "    res = results[name]\n",
    "    print(f\"{name:<10} {res['n_params']:>13,}  \"\n",
    "          f\"{res['training_time']:>12.2f}s  \"\n",
    "          f\"{res['best_val_acc']:>12.4f}  \"\n",
    "          f\"{res['test_acc']:>10.4f}\")\n",
    "\n",
    "\n",
    "# Bar plots for metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "# Test Accuracy\n",
    "ax = axes[0]\n",
    "test_accs = [results[name]['test_acc'] for name in model_names]\n",
    "bars = ax.bar(model_names, test_accs, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1.0])\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Training Time\n",
    "ax = axes[1]\n",
    "train_times = [results[name]['training_time'] for name in model_names]\n",
    "bars = ax.bar(model_names, train_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "for bar, time_val in zip(bars, train_times):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Parameters\n",
    "ax = axes[2]\n",
    "n_params = [results[name]['n_params'] for name in model_names]\n",
    "bars = ax.bar(model_names, n_params, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Number of Parameters', fontsize=12)\n",
    "ax.set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "for bar, params in zip(bars, n_params):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 200,\n",
    "            f'{params:,}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('w2v_metrics_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2835c",
   "metadata": {},
   "source": [
    "## Part 8: Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e94b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports and confusion matrices\n",
    "for name in model_names:\n",
    "    print(f\"Classification Report - {name}\")\n",
    "    \n",
    "    preds = np.array(results[name]['predictions'])\n",
    "    labels = np.array(results[name]['labels'])\n",
    "    \n",
    "    print(classification_report(labels, preds, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(f'Confusion Matrix - {name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'confusion_matrix_w2v_{name.lower()}.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc672d8",
   "metadata": {},
   "source": [
    "## Part 9: Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, vocab, device):\n",
    "    \"\"\"Predict sentiment for a given text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    tokens = preprocess_text(text)\n",
    "    indices = vocab.encode(tokens)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    sequence = torch.tensor([indices]).to(device)\n",
    "    length = torch.tensor([len(indices)]).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence, length)\n",
    "        prediction = (output >= 0.5).float().item()\n",
    "        confidence = output.item()\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    return sentiment, confidence\n",
    "\n",
    "# Load best models\n",
    "best_models = {}\n",
    "model_classes = {\n",
    "    'MLP': MLP_Classifier,\n",
    "    'RNN': RNN_Classifier,\n",
    "    'LSTM': LSTM_Classifier,\n",
    "    'GRU': GRU_Classifier\n",
    "}\n",
    "\n",
    "for name, model_class in model_classes.items():\n",
    "    if name == 'MLP':\n",
    "        model = model_class(embedding_matrix, HIDDEN_DIM, DROPOUT)\n",
    "    else:\n",
    "        model = model_class(embedding_matrix, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'{name.lower()}_w2v_best.pt'))\n",
    "    model = model.to(device)\n",
    "    best_models[name] = model\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"this movie is absolutely amazing and wonderful\",\n",
    "    \"terrible film complete waste of time\",\n",
    "    \"loved every minute of this fantastic movie\",\n",
    "    \"horrible acting and boring storyline\",\n",
    "    \"excellent performance highly recommended\",\n",
    "    \"worst experience ever very disappointing\"\n",
    "]\n",
    "\n",
    "print(\"SAMPLE PREDICTIONS COMPARISON\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(f\"\\n Review: {sent}\")\n",
    "    print(f\"{'Model':<10} {'Prediction':<15} {'Confidence':<15}\")\n",
    "    \n",
    "    for name, model in best_models.items():\n",
    "        sentiment, confidence = predict_sentiment(model, sent, vocab, device)\n",
    "        indicator = \"Pos\" if sentiment == \"Positive\" else \"Neg\"\n",
    "        print(f\"{name:<10} {indicator} {sentiment:<12} {confidence:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b277eef",
   "metadata": {},
   "source": [
    "## Part 10: Word2Vec Embedding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3622b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word similarity analysis\n",
    "print(\"Word2Vec Embedding Analysis\")\n",
    "\n",
    "# Test semantic relationships\n",
    "print(\"\\nSemantic Similarities:\")\n",
    "word_pairs = [\n",
    "    ('excellent', 'fantastic'),\n",
    "    ('terrible', 'awful'),\n",
    "    ('love', 'enjoy'),\n",
    "    ('hate', 'dislike'),\n",
    "    ('amazing', 'wonderful'),\n",
    "    ('boring', 'dull')\n",
    "]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    if word1 in w2v_sg.wv and word2 in w2v_sg.wv:\n",
    "        similarity = w2v_sg.wv.similarity(word1, word2)\n",
    "        print(f\"  {word1:15} <-> {word2:15}: {similarity:.4f}\")\n",
    "\n",
    "# Most similar words\n",
    "print(\"\\nMost Similar Words:\")\n",
    "sentiment_words = ['excellent', 'terrible', 'great', 'awful', 'fantastic', 'boring']\n",
    "\n",
    "for word in sentiment_words:\n",
    "    if word in w2v_sg.wv:\n",
    "        similar = w2v_sg.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\n  {word}:\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"    - {sim_word:15} ({score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings with t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"\\nVisualizing Word Embeddings with t-SNE...\")\n",
    "\n",
    "# Select important sentiment words\n",
    "sentiment_vocab = ['excellent', 'fantastic', 'amazing', 'wonderful', 'great', 'brilliant',\n",
    "                   'terrible', 'awful', 'horrible', 'boring', 'worst', 'bad',\n",
    "                   'movie', 'film', 'acting', 'story', 'performance']\n",
    "\n",
    "# Get vectors\n",
    "vectors = []\n",
    "labels = []\n",
    "for word in sentiment_vocab:\n",
    "    if word in w2v_sg.wv:\n",
    "        vectors.append(w2v_sg.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=min(5, len(vectors)-1))\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "positive_words = ['excellent', 'fantastic', 'amazing', 'wonderful', 'great', 'brilliant']\n",
    "negative_words = ['terrible', 'awful', 'horrible', 'boring', 'worst', 'bad']\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    x, y = vectors_2d[i]\n",
    "    if label in positive_words:\n",
    "        color = 'green'\n",
    "        marker = 'o'\n",
    "    elif label in negative_words:\n",
    "        color = 'red'\n",
    "        marker = 'x'\n",
    "    else:\n",
    "        color = 'blue'\n",
    "        marker = 's'\n",
    "    \n",
    "    plt.scatter(x, y, c=color, marker=marker, s=200, alpha=0.6, edgecolors='black')\n",
    "    plt.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title('Word2Vec Embeddings Visualization (t-SNE)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dimension 1', fontsize=12)\n",
    "plt.ylabel('Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Positive'),\n",
    "    Line2D([0], [0], marker='x', color='w', markerfacecolor='red', markersize=10, label='Negative'),\n",
    "    Line2D([0], [0], marker='s', color='w', markerfacecolor='blue', markersize=10, label='Neutral')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='best', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('w2v_embeddings_tsne.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
