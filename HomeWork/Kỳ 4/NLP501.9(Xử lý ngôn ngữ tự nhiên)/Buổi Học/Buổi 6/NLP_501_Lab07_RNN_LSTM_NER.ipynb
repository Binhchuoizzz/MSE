{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6778f5",
   "metadata": {},
   "source": [
    "# Lab 07-04: Named Entity Recognition with RNN, LSTM và GRU\n",
    "##  Comparing Recurrent Architectures for Sequence Labeling\n",
    "\n",
    "**Task:** Named Entity Recognition (NER)  \n",
    "**Dataset:** Synthetic NER dataset với tags: PER, ORG, LOC  \n",
    "**Models:** Vanilla RNN, LSTM, GRU  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2f05a",
   "metadata": {},
   "source": [
    "## Part 1: Setup và Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c03e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d205edf",
   "metadata": {},
   "source": [
    "## Part 2: Data Preparation\n",
    "\n",
    "We'll use a synthetic NER dataset with BIO tagging scheme:\n",
    "- **B-X**: Beginning of entity type X\n",
    "- **I-X**: Inside entity type X\n",
    "- **O**: Outside any entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fde76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NER dataset (BIO format)\n",
    "sample_data = [\n",
    "    ([\"Apple\", \"is\", \"looking\", \"at\", \"buying\", \"U.K.\", \"startup\", \"for\", \"$1\", \"billion\"],\n",
    "     [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    \n",
    "    ([\"Tim\", \"Cook\", \"is\", \"the\", \"CEO\", \"of\", \"Apple\", \"Inc.\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\"]),\n",
    "    \n",
    "    ([\"Google\", \"was\", \"founded\", \"in\", \"California\"],\n",
    "     [\"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"John\", \"Smith\", \"works\", \"at\", \"Microsoft\", \"in\", \"Seattle\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"The\", \"meeting\", \"will\", \"be\", \"in\", \"New\", \"York\", \"City\"],\n",
    "     [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\"]),\n",
    "    \n",
    "    ([\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"Amazon\", \"delivers\", \"packages\", \"worldwide\"],\n",
    "     [\"B-ORG\", \"O\", \"O\", \"O\"]),\n",
    "    \n",
    "    ([\"Paris\", \"is\", \"the\", \"capital\", \"of\", \"France\"],\n",
    "     [\"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"Elon\", \"Musk\", \"founded\", \"Tesla\", \"and\", \"SpaceX\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"O\", \"B-ORG\"]),\n",
    "    \n",
    "    ([\"The\", \"United\", \"Nations\", \"is\", \"based\", \"in\", \"Geneva\"],\n",
    "     [\"O\", \"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"Mark\", \"Zuckerberg\", \"created\", \"Facebook\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\"]),\n",
    "    \n",
    "    ([\"London\", \"is\", \"in\", \"England\"],\n",
    "     [\"B-LOC\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"IBM\", \"has\", \"offices\", \"in\", \"Tokyo\"],\n",
    "     [\"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    \n",
    "    ([\"Bill\", \"Gates\", \"works\", \"with\", \"Microsoft\"],\n",
    "     [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-ORG\"]),\n",
    "    \n",
    "    ([\"The\", \"company\", \"is\", \"based\", \"in\", \"Silicon\", \"Valley\"],\n",
    "     [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\"]),\n",
    "]\n",
    "\n",
    "# Create more data by duplication\n",
    "all_data = sample_data * 20  # 300 samples\n",
    "np.random.shuffle(all_data)\n",
    "\n",
    "train_size = int(0.7 * len(all_data))\n",
    "val_size = int(0.15 * len(all_data))\n",
    "\n",
    "train_data = all_data[:train_size]\n",
    "val_data = all_data[train_size:train_size + val_size]\n",
    "test_data = all_data[train_size + val_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Val samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nSample:\")\n",
    "for i in range(2):\n",
    "    words, tags = train_data[i]\n",
    "    print(f\"  Words: {' '.join(words)}\")\n",
    "    print(f\"  Tags:  {' '.join(tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.tag2idx = {\"<PAD>\": 0}\n",
    "        self.idx2tag = {0: \"<PAD>\"}\n",
    "        \n",
    "    def build(self, data):\n",
    "        # Build word vocab\n",
    "        words = set()\n",
    "        tags = set()\n",
    "        for sent_words, sent_tags in data:\n",
    "            words.update(sent_words)\n",
    "            tags.update(sent_tags)\n",
    "        \n",
    "        for idx, word in enumerate(sorted(words), start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "        for idx, tag in enumerate(sorted(tags), start=1):\n",
    "            self.tag2idx[tag] = idx\n",
    "            self.idx2tag[idx] = tag\n",
    "            \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        print(f\"Tag set size: {len(self.tag2idx)}\")\n",
    "        print(f\"Tags: {list(self.tag2idx.keys())}\")\n",
    "\n",
    "vocab = Vocab()\n",
    "vocab.build(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe52fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.data[idx]\n",
    "        \n",
    "        word_ids = [self.vocab.word2idx.get(w, self.vocab.word2idx[\"<UNK>\"]) for w in words]\n",
    "        tag_ids = [self.vocab.tag2idx[t] for t in tags]\n",
    "        \n",
    "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for variable length sequences\"\"\"\n",
    "    word_seqs, tag_seqs = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in word_seqs])\n",
    "    \n",
    "    # Pad sequences\n",
    "    word_seqs_padded = pad_sequence(word_seqs, batch_first=True, padding_value=0)\n",
    "    tag_seqs_padded = pad_sequence(tag_seqs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return word_seqs_padded, tag_seqs_padded, lengths\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = NERDataset(train_data, vocab)\n",
    "val_dataset = NERDataset(val_data, vocab)\n",
    "test_dataset = NERDataset(test_data, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b4943",
   "metadata": {},
   "source": [
    "## Part 3: Model Implementations\n",
    "\n",
    "### 3.1. Vanilla RNN for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_NER(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla RNN for Named Entity Recognition\n",
    "    \n",
    "    Architecture:\n",
    "    Embedding → RNN → Linear → Output tags\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim=100, hidden_dim=128, \n",
    "                 num_layers=1, dropout=0.3):\n",
    "        super(RNN_NER, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Vanilla RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
    "    \n",
    "    def forward(self, sentences, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            outputs: [batch_size, seq_len, tag_size]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embeds = self.embedding(sentences)  # [batch, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed_embeds = pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # RNN\n",
    "        packed_rnn_out, hidden = self.rnn(packed_embeds)\n",
    "        \n",
    "        # Unpack\n",
    "        rnn_out, _ = pad_packed_sequence(packed_rnn_out, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = self.fc(rnn_out)  # [batch, seq_len, tag_size]\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0be3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_NER(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM for Named Entity Recognition\n",
    "    \n",
    "    Architecture:\n",
    "    Embedding → LSTM → Linear → Output tags\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim=100, hidden_dim=128, \n",
    "                 num_layers=1, dropout=0.3):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
    "    \n",
    "    def forward(self, sentences, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            outputs: [batch_size, seq_len, tag_size]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embeds = self.embedding(sentences)  # [batch, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed_embeds = pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # LSTM\n",
    "        packed_lstm_out, (hidden, cell) = self.lstm(packed_embeds)\n",
    "        \n",
    "        # Unpack\n",
    "        lstm_out, _ = pad_packed_sequence(packed_lstm_out, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = self.fc(lstm_out)  # [batch, seq_len, tag_size]\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9175286",
   "metadata": {},
   "source": [
    "### 3.3. GRU for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_NER(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU for Named Entity Recognition\n",
    "    \n",
    "    Architecture:\n",
    "    Embedding → GRU → Linear → Output tags\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim=100, hidden_dim=128, \n",
    "                 num_layers=1, dropout=0.3):\n",
    "        super(GRU_NER, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
    "    \n",
    "    def forward(self, sentences, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: [batch_size, seq_len]\n",
    "            lengths: [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            outputs: [batch_size, seq_len, tag_size]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embeds = self.embedding(sentences)  # [batch, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed_embeds = pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # GRU\n",
    "        packed_gru_out, hidden = self.gru(packed_embeds)\n",
    "        \n",
    "        # Unpack\n",
    "        gru_out, _ = pad_packed_sequence(packed_gru_out, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = self.fc(gru_out)  # [batch, seq_len, tag_size]\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4604f",
   "metadata": {},
   "source": [
    "## Part 4: Training và Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sentences, tags, lengths in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sentences, lengths)  # [batch, seq_len, tag_size]\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        outputs_flat = outputs.view(-1, outputs.shape[-1])  # [batch*seq_len, tag_size]\n",
    "        tags_flat = tags.view(-1)  # [batch*seq_len]\n",
    "        \n",
    "        # Calculate loss (ignore padding)\n",
    "        loss = criterion(outputs_flat, tags_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy (excluding padding)\n",
    "        predictions = outputs.argmax(dim=-1)  # [batch, seq_len]\n",
    "        mask = tags != 0  # Non-padding positions\n",
    "        correct += ((predictions == tags) & mask).sum().item()\n",
    "        total += mask.sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, vocab):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentences, tags, lengths in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            sentences = sentences.to(device)\n",
    "            tags = tags.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sentences, lengths)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
    "            tags_flat = tags.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs_flat, tags_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = outputs.argmax(dim=-1)\n",
    "            mask = tags != 0\n",
    "            correct += ((predictions == tags) & mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "            \n",
    "            # Store predictions for detailed metrics\n",
    "            for i, length in enumerate(lengths):\n",
    "                pred_tags = predictions[i, :length].cpu().numpy()\n",
    "                true_tags = tags[i, :length].cpu().numpy()\n",
    "                all_predictions.extend(pred_tags)\n",
    "                all_targets.extend(true_tags)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a53c0",
   "metadata": {},
   "source": [
    "## Part 5: Training All Models\n",
    "\n",
    "Let's train all three models and compare their performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "VOCAB_SIZE = len(vocab.word2idx)\n",
    "TAG_SIZE = len(vocab.tag2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Training hyperparameters\n",
    "N_EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Loss function (ignore padding index 0)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Models to train\n",
    "models_to_train = {\n",
    "    'RNN': RNN_NER(VOCAB_SIZE, TAG_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT),\n",
    "    'LSTM': LSTM_NER(VOCAB_SIZE, TAG_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT),\n",
    "    'GRU': GRU_NER(VOCAB_SIZE, TAG_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "}\n",
    "\n",
    "print(\"Model Comparison\")\n",
    "for name, model in models_to_train.items():\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:10} - Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model\n",
    "for model_name, model in models_to_train.items():\n",
    "    print(f\"Training {model_name} Model\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    training_time = 0\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device, vocab)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        training_time += epoch_time\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'{model_name.lower()}_ner_best.pt')\n",
    "            print(f\"  Saved best model (Val Acc: {best_val_acc:.4f})\")\n",
    "    \n",
    "    # Test on best model\n",
    "    model.load_state_dict(torch.load(f'{model_name.lower()}_ner_best.pt'))\n",
    "    test_loss, test_acc, test_preds, test_targets = evaluate(model, test_loader, criterion, device, vocab)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'history': history,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'training_time': training_time,\n",
    "        'predictions': test_preds,\n",
    "        'targets': test_targets,\n",
    "        'n_params': sum(p.numel() for p in model.parameters())\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Training Complete!\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.4f}\")\n",
    "    print(f\"  Test Acc: {test_acc:.4f}\")\n",
    "    print(f\"  Total Training Time: {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368229e",
   "metadata": {},
   "source": [
    "## Part 6: Visualization và Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "epochs = range(1, N_EPOCHS + 1)\n",
    "\n",
    "# Training Loss\n",
    "ax = axes[0, 0]\n",
    "for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "    ax.plot(epochs, results[model_name]['history']['train_loss'], \n",
    "            label=model_name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "ax = axes[0, 1]\n",
    "for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "    ax.plot(epochs, results[model_name]['history']['val_loss'], \n",
    "            label=model_name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy\n",
    "ax = axes[1, 0]\n",
    "for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "    ax.plot(epochs, results[model_name]['history']['train_acc'], \n",
    "            label=model_name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "ax = axes[1, 1]\n",
    "for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "    ax.plot(epochs, results[model_name]['history']['val_acc'], \n",
    "            label=model_name, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('rnn_lstm_gru_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc01e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison table\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(f\"{'Model':<10} {'Params':<12} {'Train Time':<15} {'Best Val Acc':<15} {'Test Acc':<12}\")\n",
    "\n",
    "for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "    res = results[model_name]\n",
    "    print(f\"{model_name:<10} {res['n_params']:>10,}  \"\n",
    "          f\"{res['training_time']:>12.2f}s  \"\n",
    "          f\"{res['best_val_acc']:>12.4f}  \"\n",
    "          f\"{res['test_acc']:>10.4f}\")\n",
    "\n",
    "# Bar plots for final comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "model_names = ['RNN', 'LSTM', 'GRU']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# Test Accuracy\n",
    "ax = axes[0]\n",
    "test_accs = [results[name]['test_acc'] for name in model_names]\n",
    "bars = ax.bar(model_names, test_accs, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1.0])\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Training Time\n",
    "ax = axes[1]\n",
    "train_times = [results[name]['training_time'] for name in model_names]\n",
    "bars = ax.bar(model_names, train_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "for bar, time_val in zip(bars, train_times):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Parameters\n",
    "ax = axes[2]\n",
    "n_params = [results[name]['n_params'] for name in model_names]\n",
    "bars = ax.bar(model_names, n_params, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Number of Parameters', fontsize=12)\n",
    "ax.set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "for bar, params in zip(bars, n_params):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 500,\n",
    "            f'{params:,}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('model_metrics_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f958e26",
   "metadata": {},
   "source": [
    "## Part 7: Detailed Analysis\n",
    "\n",
    "Let's analyze the predictions from each model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ff304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for each model\n",
    "for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "    print(f\"Classification Report - {model_name}\")\n",
    "    \n",
    "    preds = results[model_name]['predictions']\n",
    "    targets = results[model_name]['targets']\n",
    "    \n",
    "    # Convert indices to tag names\n",
    "    pred_tags = [vocab.idx2tag[p] for p in preds]\n",
    "    true_tags = [vocab.idx2tag[t] for t in targets]\n",
    "    \n",
    "    print(classification_report(true_tags, pred_tags, zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tags_list = sorted([tag for tag in vocab.tag2idx.keys() if tag != '<PAD>'])\n",
    "    cm = confusion_matrix(true_tags, pred_tags, labels=tags_list)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=tags_list, yticklabels=tags_list)\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'confusion_matrix_{model_name.lower()}.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af56195",
   "metadata": {},
   "source": [
    "## Part 8: Sample Predictions\n",
    "\n",
    "Let's test all models on some example sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e29167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(model, sentence, vocab, device):\n",
    "    \"\"\"Predict NER tags for a sentence\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize (simple space split)\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Convert to indices\n",
    "    word_ids = [vocab.word2idx.get(w, vocab.word2idx[\"<UNK>\"]) for w in words]\n",
    "    \n",
    "    # Create tensors\n",
    "    sentence_tensor = torch.tensor([word_ids]).to(device)\n",
    "    length_tensor = torch.tensor([len(word_ids)]).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sentence_tensor, length_tensor)\n",
    "        predictions = outputs.argmax(dim=-1)[0, :len(words)]\n",
    "    \n",
    "    # Convert to tag names\n",
    "    pred_tags = [vocab.idx2tag[p.item()] for p in predictions]\n",
    "    \n",
    "    return list(zip(words, pred_tags))\n",
    "\n",
    "# Load best models\n",
    "best_models = {}\n",
    "for model_name, model_class in [('RNN', RNN_NER), ('LSTM', LSTM_NER), ('GRU', GRU_NER)]:\n",
    "    model = model_class(VOCAB_SIZE, TAG_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "    model.load_state_dict(torch.load(f'{model_name.lower()}_ner_best.pt'))\n",
    "    model = model.to(device)\n",
    "    best_models[model_name] = model\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"Apple is looking at buying U.K. startup\",\n",
    "    \"Tim Cook is the CEO of Apple Inc.\",\n",
    "    \"Google was founded in California\",\n",
    "    \"John Smith works at Microsoft in Seattle\",\n",
    "    \"Barack Obama was born in Hawaii\"\n",
    "]\n",
    "\n",
    "print(\"SAMPLE PREDICTIONS COMPARISON\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(f\"\\n Sentence: {sent}\")\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    all_predictions = {}\n",
    "    for model_name, model in best_models.items():\n",
    "        predictions = predict_sentence(model, sent, vocab, device)\n",
    "        all_predictions[model_name] = predictions\n",
    "    \n",
    "    # Display in table format\n",
    "    words = sent.split()\n",
    "    print(f\"{'Word':<15} {'RNN':<15} {'LSTM':<15} {'GRU':<15}\")\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        rnn_tag = all_predictions['RNN'][i][1]\n",
    "        lstm_tag = all_predictions['LSTM'][i][1]\n",
    "        gru_tag = all_predictions['GRU'][i][1]\n",
    "        \n",
    "        # Highlight entities\n",
    "        rnn_display = f\"➤ {rnn_tag}\" if rnn_tag != 'O' else rnn_tag\n",
    "        lstm_display = f\"➤ {lstm_tag}\" if lstm_tag != 'O' else lstm_tag\n",
    "        gru_display = f\"➤ {gru_tag}\" if gru_tag != 'O' else gru_tag\n",
    "        \n",
    "        print(f\"{word:<15} {rnn_display:<15} {lstm_display:<15} {gru_display:<15}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
