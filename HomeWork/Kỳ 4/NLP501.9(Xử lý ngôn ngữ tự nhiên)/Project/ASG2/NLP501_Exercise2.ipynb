{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "292f5c36",
      "metadata": {
        "id": "292f5c36"
      },
      "source": [
        "# NLP501 – Exercise 2: Probabilistic Models & Language Modeling\n",
        "\n",
        "**Student:** Nguyễn Đức Bình  \n",
        "**Course:** NLP501 – Natural Language Processing  \n",
        "**Exercise:** 2 (Autocorrect System • POS Tagging with HMM • N-gram Language Model)\n",
        "\n",
        "This notebook implements all 3 parts required in the assignment:\n",
        "\n",
        "1. **Autocorrect System** using **Minimum Edit Distance (Levenshtein)**\n",
        "2. **POS Tagging** using **Hidden Markov Model (HMM)** + **Viterbi decoding**\n",
        "3. **N-gram Language Model** (unigram/bigram/trigram) with **Add-k smoothing**, **perplexity**, and **autocomplete**\n",
        "\n",
        ">  Everything is written to be easy to run end-to-end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "125f777c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "125f777c",
        "outputId": "890d7a27-467f-413f-a815-89d3cd23b106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# =========================\n",
        "# 0. Setup & Imports\n",
        "# =========================\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# NLTK for corpora\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK datasets (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a40ef0f",
      "metadata": {
        "id": "0a40ef0f"
      },
      "source": [
        "## Part 1 — Autocorrect System (Edit Distance)\n",
        "\n",
        "###  Goal\n",
        "\n",
        "Build an autocorrect system that:\n",
        "\n",
        "- Implements **Minimum Edit Distance (Levenshtein)**\n",
        "- Builds a **vocabulary** from an English corpus\n",
        "- Uses **word frequency** to rank suggestions\n",
        "- Combines edit distance + frequency to pick best correction\n",
        "- Supports **distance 1–2**\n",
        "- Returns **top 5 suggestions**\n",
        "- Handles **lowercase & uppercase**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a980d63b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a980d63b",
        "outputId": "048b62d0-c536-4501-a20f-1feeb30bc4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 49,815\n",
            "Top 10 words: [('the', 69971), (',', 58334), ('.', 49346), ('of', 36412), ('and', 28853), ('to', 26158), ('a', 23195), ('in', 21337), ('that', 10594), ('is', 10109)]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 1.1 — Text processing + Vocabulary\n",
        "# =========================\n",
        "\n",
        "def tokenize_words(text: str) -> List[str]:\n",
        "    \"\"\"Extract words (letters + apostrophes) and lowercase them.\"\"\"\n",
        "    return re.findall(r\"[a-zA-Z']+\", text.lower())\n",
        "\n",
        "def build_vocabulary_from_corpus_words(words: List[str], min_freq: int = 1) -> Tuple[set, Counter]:\n",
        "    \"\"\"Build vocabulary set and frequency counter.\"\"\"\n",
        "    freqs = Counter(words)\n",
        "    vocab = {w for w, c in freqs.items() if c >= min_freq}\n",
        "    return vocab, freqs\n",
        "\n",
        "# Use Brown corpus as default corpus for vocab\n",
        "from nltk.corpus import brown\n",
        "\n",
        "brown_words = [w.lower() for w in brown.words()]\n",
        "vocab, word_freqs = build_vocabulary_from_corpus_words(brown_words, min_freq=1)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab):,}\")\n",
        "print(\"Top 10 words:\", word_freqs.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cf18c2bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf18c2bf",
        "outputId": "571bb30b-0504-4685-fb61-84185d21599a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kitten sitting 3\n",
            "book back 2\n",
            "NLP nlp 0\n",
            "speling spelling 1\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 1.2 — Minimum Edit Distance (Levenshtein)\n",
        "# =========================\n",
        "\n",
        "def levenshtein_distance(a: str, b: str) -> int:\n",
        "    \"\"\"\n",
        "    Compute Minimum Edit Distance (Levenshtein):\n",
        "    insert / delete / substitute all cost 1\n",
        "    \"\"\"\n",
        "    a = a.lower()\n",
        "    b = b.lower()\n",
        "    n, m = len(a), len(b)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "\n",
        "    for i in range(n+1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(m+1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j] + 1,      # deletion\n",
        "                dp[i][j-1] + 1,      # insertion\n",
        "                dp[i-1][j-1] + cost  # substitution\n",
        "            )\n",
        "    return dp[n][m]\n",
        "\n",
        "# Quick tests\n",
        "tests = [\n",
        "    (\"kitten\", \"sitting\"),\n",
        "    (\"book\", \"back\"),\n",
        "    (\"NLP\", \"nlp\"),\n",
        "    (\"speling\", \"spelling\"),\n",
        "]\n",
        "for a, b in tests:\n",
        "    print(a, b, levenshtein_distance(a,b))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e4521c24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4521c24",
        "outputId": "da2eab4d-1863-479b-e6e0-2df974673eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 ['spelling', 'spewing']\n",
            "2 40 ['dueling', 'feeling', 'opening', 'paling', 'peeling', 'peking', 'pelting', 'piling', 'poling', 'reeling']\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 1.3 — Candidate generation (edit distance 1 & 2)\n",
        "# =========================\n",
        "\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "def edits1(word: str) -> set:\n",
        "    \"\"\"All edits that are one edit away from `word`.\"\"\"\n",
        "    word = word.lower()\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
        "\n",
        "    deletes    = [L + R[1:]           for L, R in splits if R]\n",
        "    inserts    = [L + c + R           for L, R in splits for c in alphabet]\n",
        "    replaces   = [L + c + R[1:]       for L, R in splits if R for c in alphabet]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "\n",
        "    return set(deletes + inserts + replaces + transposes)\n",
        "\n",
        "def edits2(word: str) -> set:\n",
        "    \"\"\"All edits that are two edits away from `word`.\"\"\"\n",
        "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def known(words: set, vocab: set) -> set:\n",
        "    \"\"\"Filter candidates that exist in vocabulary.\"\"\"\n",
        "    return set(w for w in words if w in vocab)\n",
        "\n",
        "def get_candidates(word: str, vocab: set, max_distance: int = 2) -> Dict[int, set]:\n",
        "    \"\"\"Return candidate words grouped by edit distance.\"\"\"\n",
        "    word_l = word.lower()\n",
        "    out = defaultdict(set)\n",
        "\n",
        "    if word_l in vocab:\n",
        "        out[0].add(word_l)\n",
        "        return out\n",
        "\n",
        "    c1 = known(edits1(word_l), vocab)\n",
        "    out[1] = c1\n",
        "\n",
        "    if max_distance >= 2:\n",
        "        c2 = known(edits2(word_l), vocab)\n",
        "        # Avoid duplicates already in distance 1\n",
        "        out[2] = c2 - c1\n",
        "\n",
        "    return out\n",
        "\n",
        "# Demo candidates\n",
        "word = \"speling\"\n",
        "cands = get_candidates(word, vocab, max_distance=2)\n",
        "for d in sorted(cands.keys()):\n",
        "    print(d, len(cands[d]), list(sorted(list(cands[d])))[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "49a9c931",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49a9c931",
        "outputId": "3b799d33-2951-4e64-b80c-13d8aa27139e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: speling\n",
            "[(1, 4, 'spelling'), (1, 1, 'spewing'), (2, 172, 'feeling'), (2, 127, 'spring'), (2, 86, 'seeing')]\n",
            "\n",
            "Input: acress\n",
            "[(1, 282, 'across'), (1, 44, 'acres'), (1, 24, 'access'), (1, 6, 'actress'), (1, 1, 'caress')]\n",
            "\n",
            "Input: Langauge\n",
            "[(1, 109, 'Language'), (2, 40, 'Languages')]\n",
            "\n",
            "Input: thier\n",
            "[(1, 2669, 'their'), (1, 8, 'thief'), (1, 5, 'ther'), (2, 69971, 'the'), (2, 5145, 'this')]\n",
            "\n",
            "Input: wrold\n",
            "[(1, 787, 'world'), (1, 1, 'wold'), (2, 2714, 'would'), (2, 661, 'old'), (2, 413, 'told')]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 1.4 — Ranking suggestions (edit distance + frequency)\n",
        "# =========================\n",
        "\n",
        "def suggest_corrections(word: str, vocab: set, freqs: Counter, top_k: int = 5, max_distance: int = 2):\n",
        "    \"\"\"\n",
        "    Return top_k suggestions sorted by:\n",
        "      1) smaller edit distance\n",
        "      2) higher frequency\n",
        "    Keeps original casing for output.\n",
        "    \"\"\"\n",
        "    original = word\n",
        "    w = word.lower()\n",
        "\n",
        "    candidates_by_d = get_candidates(w, vocab, max_distance=max_distance)\n",
        "\n",
        "    ranked = []\n",
        "    for d in sorted(candidates_by_d.keys()):\n",
        "        for cand in candidates_by_d[d]:\n",
        "            ranked.append((d, freqs[cand], cand))\n",
        "\n",
        "    ranked.sort(key=lambda x: (x[0], -x[1], x[2]))  # dist asc, freq desc\n",
        "    ranked = ranked[:top_k]\n",
        "\n",
        "    # Restore casing: if original is capitalized, capitalize suggestions\n",
        "    if original[:1].isupper():\n",
        "        return [(d, f, c.capitalize()) for d, f, c in ranked]\n",
        "    return ranked\n",
        "\n",
        "# Test suggestions\n",
        "for w in [\"speling\", \"acress\", \"Langauge\", \"thier\", \"wrold\"]:\n",
        "    print(\"\\nInput:\", w)\n",
        "    print(suggest_corrections(w, vocab, word_freqs, top_k=5, max_distance=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "46290a9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46290a9d",
        "outputId": "0652aa19-7279-4243-ed6a-a974455b418b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready. To run demo: uncomment interactive_autocorrect()\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 1.5 — Interactive CLI demo\n",
        "# =========================\n",
        "\n",
        "def autocorrect_sentence(sentence: str, vocab: set, freqs: Counter, max_distance: int = 2, top_k: int = 5):\n",
        "    \"\"\"Autocorrect each word (simple token-based).\"\"\"\n",
        "    tokens = re.findall(r\"[A-Za-z']+|[^A-Za-z']+\", sentence)\n",
        "    corrected = []\n",
        "    for tok in tokens:\n",
        "        if re.fullmatch(r\"[A-Za-z']+\", tok):\n",
        "            if tok.lower() in vocab:\n",
        "                corrected.append(tok)\n",
        "            else:\n",
        "                sug = suggest_corrections(tok, vocab, freqs, top_k=top_k, max_distance=max_distance)\n",
        "                corrected.append(sug[0][2] if sug else tok)\n",
        "        else:\n",
        "            corrected.append(tok)\n",
        "    return ''.join(corrected)\n",
        "\n",
        "def interactive_autocorrect():\n",
        "    print(\"\\n=== Autocorrect Demo (type 'exit' to stop) ===\")\n",
        "    while True:\n",
        "        s = input(\"Enter a sentence: \")\n",
        "        if s.strip().lower() == 'exit':\n",
        "            break\n",
        "        print(\"Corrected:\", autocorrect_sentence(s, vocab, word_freqs))\n",
        "        print(\"\\nSuggestions per word:\")\n",
        "        for w in re.findall(r\"[A-Za-z']+\", s):\n",
        "            if w.lower() not in vocab:\n",
        "                print(f\"  {w} -> {suggest_corrections(w, vocab, word_freqs)}\")\n",
        "\n",
        "# Uncomment to run interactively:\n",
        "# interactive_autocorrect()\n",
        "\n",
        "print(\"Ready. To run demo: uncomment interactive_autocorrect()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa92a70",
      "metadata": {
        "id": "9aa92a70"
      },
      "source": [
        "## Part 2 — POS Tagging with HMM\n",
        "\n",
        "###  Goal\n",
        "\n",
        "Build a POS tagger using:\n",
        "\n",
        "- Transition probabilities: \\(P(tag*i \\mid tag*{i-1})\\)\n",
        "- Emission probabilities: \\(P(word \\mid tag)\\)\n",
        "- **Viterbi algorithm** for decoding\n",
        "- Handle **unknown words** with smoothing\n",
        "- Evaluate **accuracy** on a test set\n",
        "\n",
        "Dataset requirement in the exercise:\n",
        "\n",
        "- WSJ corpus (NLTK Treebank) **or** Brown corpus with simplified tags.\n",
        "\n",
        "I'll use **Treebank + Universal Tagset** for a standard POS tagging task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4966e937",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4966e937",
        "outputId": "1556cc2a-2064-446e-9079-33c2259434e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 3914\n",
            "Train: 3131 | Test: 783\n",
            "Example: [('The', 'DET'), ('company', 'NOUN'), ('noted', 'VERB'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('reduced', 'VERB'), ('debt', 'NOUN'), ('by', 'ADP'), ('$', '.')]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 2.1 — Load dataset (Treebank / WSJ) and split train/test\n",
        "# =========================\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Tagged sentences from WSJ Treebank\n",
        "tagged_sents = list(treebank.tagged_sents(tagset='universal')) # Convert to list\n",
        "\n",
        "# Shuffle and split\n",
        "random.seed(42)\n",
        "random.shuffle(tagged_sents)\n",
        "\n",
        "split = int(0.8 * len(tagged_sents))\n",
        "train_sents = tagged_sents[:split]\n",
        "test_sents  = tagged_sents[split:]\n",
        "\n",
        "print(f\"Total sentences: {len(tagged_sents)}\")\n",
        "print(f\"Train: {len(train_sents)} | Test: {len(test_sents)}\")\n",
        "print(\"Example:\", train_sents[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "353461e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "353461e9",
        "outputId": "0691bcb0-1c84-43e5-9ac3-d3794215539f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Tags: 12\n",
            "Tags: ['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 2.2 — Train HMM parameters (pi, transition, emission) with smoothing\n",
        "# =========================\n",
        "\n",
        "START = \"<s>\"\n",
        "END = \"</s>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "def build_hmm_counts(tagged_sentences):\n",
        "    tag_counts = Counter()\n",
        "    word_counts = Counter()\n",
        "    transition_counts = Counter()\n",
        "    emission_counts = Counter()\n",
        "    start_counts = Counter()\n",
        "\n",
        "    for sent in tagged_sentences:\n",
        "        prev_tag = START\n",
        "        start_counts[sent[0][1]] += 1\n",
        "\n",
        "        for word, tag in sent:\n",
        "            w = word.lower()\n",
        "            tag_counts[tag] += 1\n",
        "            word_counts[w] += 1\n",
        "            transition_counts[(prev_tag, tag)] += 1\n",
        "            emission_counts[(tag, w)] += 1\n",
        "            prev_tag = tag\n",
        "\n",
        "        transition_counts[(prev_tag, END)] += 1\n",
        "\n",
        "    tags = sorted(tag_counts.keys())\n",
        "    vocab_words = set(word_counts.keys())\n",
        "    return tags, vocab_words, tag_counts, transition_counts, emission_counts, start_counts\n",
        "\n",
        "tags, train_vocab_words, tag_counts, trans_counts, emit_counts, start_counts = build_hmm_counts(train_sents)\n",
        "\n",
        "print(f\"#Tags: {len(tags)}\")\n",
        "print(\"Tags:\", tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "587bdeee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "587bdeee",
        "outputId": "2b73095d-f920-4c97-de92-a7b01ddf254f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM parameters trained.\n",
            "Example log_pi for top tags:\n",
            ". -2.480779004619803\n",
            "ADJ -3.185398586341985\n",
            "ADP -2.036775877099214\n",
            "ADV -2.9530666089733693\n",
            "CONJ -2.9715286718131044\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 2.3 — Convert counts to probabilities (log-space)\n",
        "# Using Laplace smoothing for transitions & emissions\n",
        "# =========================\n",
        "\n",
        "def train_hmm_params(tags, vocab_words, tag_counts, trans_counts, emit_counts, start_counts,\n",
        "                     alpha_trans: float = 1.0, alpha_emit: float = 1.0):\n",
        "    \"\"\"\n",
        "    Return log-prob dictionaries for:\n",
        "      - log_pi[tag]\n",
        "      - log_A[prev_tag][tag]\n",
        "      - log_B[tag][word]\n",
        "    Includes UNK in emission vocab.\n",
        "    \"\"\"\n",
        "    vocab = set(vocab_words) | {UNK}\n",
        "    num_tags = len(tags)\n",
        "    V = len(vocab)\n",
        "\n",
        "    # Initial probabilities pi\n",
        "    total_starts = sum(start_counts.values())\n",
        "    log_pi = {}\n",
        "    for t in tags:\n",
        "        # smoothing for unseen start tags\n",
        "        log_pi[t] = math.log((start_counts[t] + alpha_trans) / (total_starts + alpha_trans * num_tags))\n",
        "\n",
        "    # Transition probabilities A\n",
        "    log_A = {pt: {} for pt in [START] + tags}\n",
        "    for prev_t in [START] + tags:\n",
        "        # total outgoing transitions from prev_t\n",
        "        total_out = 0\n",
        "        for t in tags + [END]:\n",
        "            total_out += trans_counts[(prev_t, t)]\n",
        "        denom = total_out + alpha_trans * (len(tags) + 1)  # +END\n",
        "\n",
        "        for t in tags:\n",
        "            log_A[prev_t][t] = math.log((trans_counts[(prev_t, t)] + alpha_trans) / denom)\n",
        "        # END transition separately\n",
        "        log_A[prev_t][END] = math.log((trans_counts[(prev_t, END)] + alpha_trans) / denom)\n",
        "\n",
        "    # Emission probabilities B\n",
        "    log_B = {t: {} for t in tags}\n",
        "    for t in tags:\n",
        "        denom = tag_counts[t] + alpha_emit * V\n",
        "        for w in vocab:\n",
        "            log_B[t][w] = math.log((emit_counts[(t, w)] + alpha_emit) / denom)\n",
        "\n",
        "    return vocab, log_pi, log_A, log_B\n",
        "\n",
        "vocab_hmm, log_pi, log_A, log_B = train_hmm_params(\n",
        "    tags, train_vocab_words, tag_counts, trans_counts, emit_counts, start_counts,\n",
        "    alpha_trans=1.0, alpha_emit=1.0\n",
        ")\n",
        "\n",
        "print(\"HMM parameters trained.\")\n",
        "print(\"Example log_pi for top tags:\")\n",
        "for t in tags[:5]:\n",
        "    print(t, log_pi[t])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bf6c93c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf6c93c5",
        "outputId": "9cb74a90-c33b-47b8-c074-826dd0c2eebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['Terms', 'were', \"n't\", 'disclosed', '*-1', '.']\n",
            "Pred: ['NOUN', 'VERB', 'ADV', 'VERB', 'X', '.']\n",
            "Gold: ['NOUN', 'VERB', 'ADV', 'VERB', 'X', '.']\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 2.4 — Viterbi decoding\n",
        "# =========================\n",
        "\n",
        "def viterbi_decode(words: List[str], tags: List[str], vocab: set,\n",
        "                   log_pi: Dict[str, float],\n",
        "                   log_A: Dict[str, Dict[str, float]],\n",
        "                   log_B: Dict[str, Dict[str, float]]):\n",
        "    \"\"\"\n",
        "    Return best tag sequence for input words using Viterbi (log probabilities).\n",
        "    \"\"\"\n",
        "    # Convert to lowercase, map unknown to UNK\n",
        "    obs = [w.lower() if w.lower() in vocab else UNK for w in words]\n",
        "    n = len(obs)\n",
        "\n",
        "    # dp[t][i] = best log prob ending in tag t at position i\n",
        "    dp = {t: [-float('inf')] * n for t in tags}\n",
        "    back = {t: [None] * n for t in tags}\n",
        "\n",
        "    # init\n",
        "    for t in tags:\n",
        "        dp[t][0] = log_pi[t] + log_B[t][obs[0]]\n",
        "        back[t][0] = START\n",
        "\n",
        "    # recursion\n",
        "    for i in range(1, n):\n",
        "        for t in tags:\n",
        "            best_prev = None\n",
        "            best_score = -float('inf')\n",
        "            for pt in tags:\n",
        "                score = dp[pt][i-1] + log_A[pt][t] + log_B[t][obs[i]]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_prev = pt\n",
        "            dp[t][i] = best_score\n",
        "            back[t][i] = best_prev\n",
        "\n",
        "    # termination to END\n",
        "    best_last_tag = None\n",
        "    best_last_score = -float('inf')\n",
        "    for t in tags:\n",
        "        score = dp[t][n-1] + log_A[t][END]\n",
        "        if score > best_last_score:\n",
        "            best_last_score = score\n",
        "            best_last_tag = t\n",
        "\n",
        "    # backtrack\n",
        "    best_tags = [best_last_tag]\n",
        "    for i in range(n-1, 0, -1):\n",
        "        best_tags.append(back[best_tags[-1]][i])\n",
        "    best_tags.reverse()\n",
        "    return best_tags\n",
        "\n",
        "# Quick demo\n",
        "sample_words = [w for w,_ in test_sents[0]]\n",
        "pred_tags = viterbi_decode(sample_words, tags, vocab_hmm, log_pi, log_A, log_B)\n",
        "print(\"Sentence:\", sample_words[:12])\n",
        "print(\"Pred:\", pred_tags[:12])\n",
        "print(\"Gold:\", [t for _,t in test_sents[0]][:12])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0d02a0fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d02a0fb",
        "outputId": "5fdc77b0-0ea8-455c-d81a-679adae59a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM POS Tagger accuracy (first 200 test sents): 0.8923\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 2.5 — Evaluate accuracy\n",
        "# =========================\n",
        "\n",
        "def evaluate_hmm(test_tagged_sents):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for sent in test_tagged_sents:\n",
        "        words = [w for w,_ in sent]\n",
        "        gold  = [t for _,t in sent]\n",
        "        pred  = viterbi_decode(words, tags, vocab_hmm, log_pi, log_A, log_B)\n",
        "\n",
        "        for g, p in zip(gold, pred):\n",
        "            total += 1\n",
        "            if g == p:\n",
        "                correct += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "acc = evaluate_hmm(test_sents[:200])  # evaluate on subset for speed\n",
        "print(f\"HMM POS Tagger accuracy (first 200 test sents): {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd6e57b",
      "metadata": {
        "id": "dcd6e57b"
      },
      "source": [
        "## Part 3 — N-gram Language Model (Autocomplete)\n",
        "\n",
        "###  Goal\n",
        "\n",
        "Build an **N-gram Language Model** for autocomplete:\n",
        "\n",
        "- Unigram, Bigram, Trigram\n",
        "- Add-k smoothing\n",
        "- Compute perplexity on test set\n",
        "- Autocomplete next word\n",
        "- Compare unigram vs bigram vs trigram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5b07c832",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b07c832",
        "outputId": "c8ac7f93-9063-4138-d3f8-8449dc4461d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sentences: 51190\n",
            "Test sentences: 5691\n",
            "Example: ['he', 'let', 'her', 'tell', 'him', 'all', 'about', 'the', 'church']\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 3.1 — Prepare corpus text and train/test split\n",
        "# We'll use Brown corpus words for language modeling\n",
        "# =========================\n",
        "\n",
        "brown_sents = list(brown.sents()) # Convert to list for shuffling\n",
        "random.seed(42)\n",
        "random.shuffle(brown_sents)\n",
        "\n",
        "split = int(0.9 * len(brown_sents))\n",
        "lm_train_sents = brown_sents[:split]\n",
        "lm_test_sents  = brown_sents[split:]\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "    # Lowercase + keep words only\n",
        "    out=[]\n",
        "    for w in sent:\n",
        "        w = re.sub(r\"[^A-Za-z']+\", \"\", w)\n",
        "        if w:\n",
        "            out.append(w.lower())\n",
        "    return out\n",
        "\n",
        "lm_train = [preprocess_sentence(s) for s in lm_train_sents]\n",
        "lm_test  = [preprocess_sentence(s) for s in lm_test_sents]\n",
        "\n",
        "# Remove empty\n",
        "lm_train = [s for s in lm_train if len(s)>0]\n",
        "lm_test  = [s for s in lm_test if len(s)>0]\n",
        "\n",
        "print(\"Train sentences:\", len(lm_train))\n",
        "print(\"Test sentences:\", len(lm_test))\n",
        "print(\"Example:\", lm_train[0][:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5dc862a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dc862a1",
        "outputId": "04194f6a-d860-43fd-a4f7-896d4d7bf31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LM vocab size: 45,127\n",
            "Top 10 unigrams: [('the', 62986), ('of', 32765), ('and', 26002), ('to', 23572), ('a', 21078), ('in', 19236), ('that', 9500), ('is', 9115), ('was', 8861), ('he', 8592)]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 3.2 — Build N-gram counts\n",
        "# =========================\n",
        "\n",
        "BOS = \"<s>\"\n",
        "EOS = \"</s>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "def build_ngram_counts(sentences, n:int):\n",
        "    \"\"\"Return n-gram counts and (n-1)-gram context counts.\"\"\"\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "    unigram_counts = Counter()\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent = [w for w in sent if w]\n",
        "        # pad\n",
        "        padded = [BOS]*(n-1) + sent + [EOS]\n",
        "        for i in range(len(padded) - n + 1):\n",
        "            ngram = tuple(padded[i:i+n])\n",
        "            context = tuple(padded[i:i+n-1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "        for w in sent:\n",
        "            unigram_counts[w]+=1\n",
        "\n",
        "    vocab = set(unigram_counts.keys())\n",
        "    return vocab, unigram_counts, context_counts, ngram_counts\n",
        "\n",
        "vocab_lm, uni_counts, ctx2_counts, bi_counts = build_ngram_counts(lm_train, n=2)\n",
        "_, _, ctx3_counts, tri_counts = build_ngram_counts(lm_train, n=3)\n",
        "\n",
        "print(f\"LM vocab size: {len(vocab_lm):,}\")\n",
        "print(\"Top 10 unigrams:\", uni_counts.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7ac1d94a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ac1d94a",
        "outputId": "3ac04421-e282-47d1-9c01-8f6960db669d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity n=1: 2095.69\n",
            "Perplexity n=2: 5556.06\n",
            "Perplexity n=3: 22624.34\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 3.3 — Add-k smoothing probabilities\n",
        "# =========================\n",
        "\n",
        "def prob_unigram(w, uni_counts, vocab_size, k=1.0):\n",
        "    return (uni_counts[w] + k) / (sum(uni_counts.values()) + k*vocab_size)\n",
        "\n",
        "def prob_ngram(context: Tuple[str,...], word: str, ngram_counts: Counter, context_counts: Counter, vocab_size: int, k=1.0):\n",
        "    \"\"\"P(word | context) with add-k smoothing\"\"\"\n",
        "    return (ngram_counts[context + (word,)] + k) / (context_counts[context] + k*vocab_size)\n",
        "\n",
        "def sentence_log_prob(sent: List[str], n: int, k=1.0):\n",
        "    \"\"\"Compute log probability for a sentence using n-gram model.\"\"\"\n",
        "    sent = [w.lower() for w in sent if w]\n",
        "    padded = [BOS]*(n-1) + sent + [EOS]\n",
        "    V = len(vocab_lm) + 1  # plus UNK\n",
        "\n",
        "    logp = 0.0\n",
        "    for i in range(len(padded)-n+1):\n",
        "        context = tuple(padded[i:i+n-1])\n",
        "        w = padded[i+n-1]\n",
        "        if w not in vocab_lm and w not in (BOS, EOS):\n",
        "            w = UNK\n",
        "\n",
        "        if n == 1:\n",
        "            logp += math.log(prob_unigram(w, uni_counts, V, k))\n",
        "        elif n == 2:\n",
        "            logp += math.log(prob_ngram(context, w, bi_counts, ctx2_counts, V, k))\n",
        "        else:\n",
        "            logp += math.log(prob_ngram(context, w, tri_counts, ctx3_counts, V, k))\n",
        "    return logp\n",
        "\n",
        "def perplexity(sentences: List[List[str]], n: int, k=1.0, max_sentences: int = 2000):\n",
        "    \"\"\"Perplexity on a subset for speed.\"\"\"\n",
        "    sents = sentences[:max_sentences]\n",
        "    N = 0\n",
        "    total_logp = 0.0\n",
        "    for sent in sents:\n",
        "        # number of predicted tokens ~ len(sent)+1 for EOS\n",
        "        N += (len(sent) + 1)\n",
        "        total_logp += sentence_log_prob(sent, n=n, k=k)\n",
        "    return math.exp(-total_logp / N)\n",
        "\n",
        "for n in [1,2,3]:\n",
        "    pp = perplexity(lm_test, n=n, k=1.0, max_sentences=2000)\n",
        "    print(f\"Perplexity n={n}: {pp:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "31807b43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31807b43",
        "outputId": "d26abda2-ab03-4c23-ae9d-2ca34c348400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prefix: ['the', 'united']\n",
            "Bigram: [(0.007769291545957335, 'states'), (0.0009656746554297253, 'nations'), (0.00010973575629883242, \"states'\"), (8.778860503906593e-05, 'to'), (8.778860503906593e-05, 'and')]\n",
            "Trigram: [(0.006640427450031883, 'states'), (0.0008135623034807274, 'nations'), (0.00010994085182171991, \"states'\"), (4.397634072868797e-05, 'steel'), (4.397634072868797e-05, 'state')]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Part 3.4 — Autocomplete next word\n",
        "# =========================\n",
        "\n",
        "def top_next_words(prefix: List[str], n: int = 3, k: float = 1.0, top_k: int = 5):\n",
        "    \"\"\"Return top_k next-word predictions given prefix.\"\"\"\n",
        "    prefix = [w.lower() for w in prefix if w]\n",
        "    V = len(vocab_lm) + 1\n",
        "\n",
        "    if n == 1:\n",
        "        # just most frequent unigrams\n",
        "        candidates = []\n",
        "        for w, c in uni_counts.items():\n",
        "            p = prob_unigram(w, uni_counts, V, k)\n",
        "            candidates.append((p, w))\n",
        "        candidates.sort(reverse=True)\n",
        "        return candidates[:top_k]\n",
        "\n",
        "    # n>=2, build context\n",
        "    context = [BOS]*(n-1)\n",
        "    context = context + prefix\n",
        "    context = tuple(context[-(n-1):])\n",
        "\n",
        "    candidates = []\n",
        "    for w in list(vocab_lm)[:50000]:  # limit for speed\n",
        "        if n == 2:\n",
        "            p = prob_ngram(context, w, bi_counts, ctx2_counts, V, k)\n",
        "        else:\n",
        "            p = prob_ngram(context, w, tri_counts, ctx3_counts, V, k)\n",
        "        candidates.append((p, w))\n",
        "\n",
        "    candidates.sort(reverse=True)\n",
        "    return candidates[:top_k]\n",
        "\n",
        "# Demo autocomplete\n",
        "prefix = \"the united\".split()\n",
        "print(\"Prefix:\", prefix)\n",
        "print(\"Bigram:\", top_next_words(prefix, n=2))\n",
        "print(\"Trigram:\", top_next_words(prefix, n=3))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}